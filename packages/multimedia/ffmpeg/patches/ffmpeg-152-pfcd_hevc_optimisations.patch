diff -Naur ffmpeg-3.2.4/ffmpeg.c ffmpeg-3.2.4.patch/ffmpeg.c
--- ffmpeg-3.2.4/ffmpeg.c	2017-02-10 14:25:26.000000000 +0100
+++ ffmpeg-3.2.4.patch/ffmpeg.c	2017-05-28 20:42:45.712088573 +0200
@@ -23,6 +23,11 @@
  * multimedia converter based on the FFmpeg libraries
  */
 
+#ifdef RPI
+#define RPI_DISPLAY
+#define RPI_ZERO_COPY
+#endif
+
 #include "config.h"
 #include <ctype.h>
 #include <string.h>
@@ -66,6 +71,25 @@
 # include "libavfilter/buffersrc.h"
 # include "libavfilter/buffersink.h"
 
+#ifdef RPI_DISPLAY
+#pragma GCC diagnostic push
+// Many many redundant decls in the header files
+#pragma GCC diagnostic ignored "-Wredundant-decls"
+#include <bcm_host.h>
+#include <interface/mmal/mmal.h>
+#include <interface/mmal/mmal_parameters_camera.h>
+#include <interface/mmal/mmal_buffer.h>
+#include <interface/mmal/util/mmal_util.h>
+#include <interface/mmal/util/mmal_default_components.h>
+#include <interface/mmal/util/mmal_connection.h>
+#include <interface/mmal/util/mmal_util_params.h>
+#pragma GCC diagnostic pop
+#ifdef RPI_ZERO_COPY
+#include "libavcodec/rpi_qpu.h"
+#endif
+#include "libavcodec/rpi_zc.h"
+#endif
+
 #if HAVE_SYS_RESOURCE_H
 #include <sys/time.h>
 #include <sys/types.h>
@@ -160,6 +184,182 @@
 static void free_input_threads(void);
 #endif
 
+#ifdef RPI_DISPLAY
+
+#define NUM_BUFFERS 4
+
+static MMAL_COMPONENT_T* rpi_display = NULL;
+static MMAL_POOL_T *rpi_pool = NULL;
+static volatile int rpi_display_count = 0;
+
+static MMAL_POOL_T* display_alloc_pool(MMAL_PORT_T* port, size_t w, size_t h)
+{
+    MMAL_POOL_T* pool;
+    size_t i;
+    size_t size = (w*h*3)/2;
+#ifdef RPI_ZERO_COPY
+    mmal_port_parameter_set_boolean(port, MMAL_PARAMETER_ZERO_COPY, MMAL_TRUE); // Does this mark that the buffer contains a vc_handle?  Would have expected a vc_image?
+    pool = mmal_port_pool_create(port, NUM_BUFFERS, 0);
+    assert(pool);
+#else
+    pool = mmal_port_pool_create(port, NUM_BUFFERS, size);
+
+    for (i = 0; i < NUM_BUFFERS; ++i)
+    {
+       MMAL_BUFFER_HEADER_T* buffer = pool->header[i];
+       char * bufPtr = buffer->data;
+       memset(bufPtr, i*30, w*h);
+       memset(bufPtr+w*h, 128, (w*h)/2);
+    }
+#endif
+
+    return pool;
+}
+
+static void display_cb_input(MMAL_PORT_T *port, MMAL_BUFFER_HEADER_T *buffer) {
+#ifdef RPI_ZERO_COPY
+    av_rpi_zc_unref(buffer->user_data);
+    --rpi_display_count;
+#endif
+    mmal_buffer_header_release(buffer);
+}
+
+static void display_cb_control(MMAL_PORT_T *port,MMAL_BUFFER_HEADER_T *buffer) {
+  mmal_buffer_header_release(buffer);
+}
+
+static MMAL_COMPONENT_T* display_init(const enum AVPixelFormat fmt, size_t x, size_t y, size_t w, size_t h)
+{
+    MMAL_COMPONENT_T* display;
+    MMAL_DISPLAYREGION_T region =
+    {
+        .hdr = {MMAL_PARAMETER_DISPLAYREGION, sizeof(region)},
+        .set = MMAL_DISPLAY_SET_LAYER | MMAL_DISPLAY_SET_FULLSCREEN | MMAL_DISPLAY_SET_DEST_RECT,
+        .layer = 2,
+        .fullscreen = 0,
+        .dest_rect = {x, y, w, h}
+    };
+    const AVRpiZcFrameGeometry geo = av_rpi_zc_frame_geometry(fmt, w, h);
+
+    bcm_host_init();  // TODO is this needed?
+    mmal_component_create(MMAL_COMPONENT_DEFAULT_VIDEO_RENDERER, &display);
+    assert(display);
+
+    mmal_port_parameter_set(display->input[0], &region.hdr);
+
+    {
+        MMAL_ES_FORMAT_T* format = display->input[0]->format;
+        format->encoding = fmt == AV_PIX_FMT_SAND128 ? MMAL_ENCODING_YUVUV128 : MMAL_ENCODING_I420;
+        format->es->video.width = geo.stride_y;
+        format->es->video.height = geo.height_y;
+        format->es->video.crop.x = 0;
+        format->es->video.crop.y = 0;
+        format->es->video.crop.width = w;
+        format->es->video.crop.height = h;
+        mmal_port_format_commit(display->input[0]);
+    }
+
+    mmal_component_enable(display);
+
+    rpi_pool = display_alloc_pool(display->input[0], geo.stride_y, geo.height_y);
+
+    mmal_port_enable(display->input[0],display_cb_input);
+    mmal_port_enable(display->control,display_cb_control);
+
+    printf("Allocated display %dx%d in %dx%d, fmt=%d\n", w, h, geo.stride_y, geo.height_y, fmt);
+
+    return display;
+}
+
+static void display_frame(struct AVCodecContext * const s, MMAL_COMPONENT_T* const display, const AVFrame* const fr)
+{
+    MMAL_BUFFER_HEADER_T* buf;
+
+    if (!display || !rpi_pool)
+        return;
+
+    if (rpi_display_count >= 3) {
+        av_log(s, AV_LOG_VERBOSE, "Frame dropped\n");
+        return;
+    }
+
+    buf = mmal_queue_get(rpi_pool->queue);
+    if (!buf) {
+        // Running too fast so drop the frame
+        printf("Q alloc failure\n");
+        return;
+    }
+    assert(buf);
+    buf->cmd = 0;
+    buf->offset = 0; // Offset to valid data
+    buf->flags = 0;
+#ifdef RPI_ZERO_COPY
+{
+    const AVRpiZcRefPtr fr_buf = av_rpi_zc_ref(s, fr, 1);
+    if (fr_buf == NULL) {
+        mmal_buffer_header_release(buf);
+        return;
+    }
+
+    buf->user_data = fr_buf;
+    buf->data = av_rpi_zc_vc_handle(fr_buf);
+    buf->offset = av_rpi_zc_offset(fr_buf);
+    buf->length = av_rpi_zc_length(fr_buf);
+    buf->alloc_size = av_rpi_zc_numbytes(fr_buf);
+#if 0
+    {
+        unsigned int n;
+        for (n = 0; n < fr->width; n += 128) {
+            memset(fr->data[1] + n * fr->linesize[3], 0x80, 128 * fr->height / 2);
+        }
+    }
+#endif
+    ++rpi_display_count;
+}
+#else
+{
+#error YYY
+    int w = fr->width;
+    int h = fr->height;
+    int w2 = (w+31)&~31;
+    int h2 = (h+15)&~15;
+
+    buf->length = (w2 * h2 * 3)/2;
+    buf->user_data = NULL;
+
+    //mmal_buffer_header_mem_lock(buf);
+    memcpy(buf->data, fr->data[0], w2 * h);
+    memcpy(buf->data+w2*h2, fr->data[1], w2 * h / 4);
+    memcpy(buf->data+w2*h2*5/4, fr->data[2], w2 * h / 4);
+    //mmal_buffer_header_mem_unlock(buf);
+}
+#endif
+
+    while (rpi_display_count >= 3) {
+        usleep(5000);
+    }
+
+    if (mmal_port_send_buffer(display->input[0], buf) != MMAL_SUCCESS)
+    {
+        printf("** send failed: depth=%d\n", rpi_display_count);
+        display_cb_input(NULL, buf);
+    }
+}
+
+static void display_exit(MMAL_COMPONENT_T* display)
+{
+//    sleep(120);
+    if (display) {
+        mmal_component_destroy(display);
+    }
+    if (rpi_pool) {
+        mmal_port_pool_destroy(display->input[0], rpi_pool);
+    }
+}
+
+#endif
+
+
 /* sub2video hack:
    Convert subtitles to video with alpha to insert them in filter graphs.
    This is a temporary solution until libavfilter gets real subtitles support.
@@ -549,6 +749,11 @@
         avformat_close_input(&input_files[i]->ctx);
         av_freep(&input_files[i]);
     }
+
+#ifdef RPI_DISPLAY
+    display_exit(rpi_display);
+#endif
+
     for (i = 0; i < nb_input_streams; i++) {
         InputStream *ist = input_streams[i];
 
@@ -561,6 +766,9 @@
         av_freep(&ist->hwaccel_device);
         av_freep(&ist->dts_buffer);
 
+#ifdef RPI_ZERO_COPY
+        av_rpi_zc_uninit(ist->dec_ctx);
+#endif
         avcodec_free_context(&ist->dec_ctx);
 
         av_freep(&input_streams[i]);
@@ -591,6 +799,7 @@
     }
     term_exit();
     ffmpeg_exited = 1;
+
 }
 
 void remove_avoptions(AVDictionary **a, AVDictionary *b)
@@ -1019,6 +1228,15 @@
     if (ost->source_index >= 0)
         ist = input_streams[ost->source_index];
 
+#ifdef RPI_DISPLAY
+    if (next_picture && ist != NULL)
+    {
+        if (!rpi_display)
+            rpi_display = display_init(next_picture->format, 0, 0, next_picture->width, next_picture->height);
+        display_frame(ist->dec_ctx, rpi_display, next_picture);
+    }
+#endif
+
     if (filter->inputs[0]->frame_rate.num > 0 &&
         filter->inputs[0]->frame_rate.den > 0)
         duration = 1/(av_q2d(filter->inputs[0]->frame_rate) * av_q2d(enc->time_base));
@@ -2707,6 +2925,12 @@
         ist->dec_ctx->opaque                = ist;
         ist->dec_ctx->get_format            = get_format;
         ist->dec_ctx->get_buffer2           = get_buffer;
+
+#ifdef RPI_ZERO_COPY
+        // Overrides the above get_buffer2
+        av_rpi_zc_init(ist->dec_ctx);
+#endif
+
         ist->dec_ctx->thread_safe_callbacks = 1;
 
         av_opt_set_int(ist->dec_ctx, "refcounted_frames", 1, 0);
diff -Naur ffmpeg-3.2.4/libavcodec/allcodecs.c ffmpeg-3.2.4.patch/libavcodec/allcodecs.c
--- ffmpeg-3.2.4/libavcodec/allcodecs.c	2017-02-10 14:25:26.000000000 +0100
+++ ffmpeg-3.2.4.patch/libavcodec/allcodecs.c	2017-05-28 20:42:45.714088580 +0200
@@ -687,6 +687,7 @@
     REGISTER_PARSER(H261,               h261);
     REGISTER_PARSER(H263,               h263);
     REGISTER_PARSER(H264,               h264);
+    REGISTER_PARSER(H264_MVC,           h264_mvc);
     REGISTER_PARSER(HEVC,               hevc);
     REGISTER_PARSER(MJPEG,              mjpeg);
     REGISTER_PARSER(MLP,                mlp);
diff -Naur ffmpeg-3.2.4/libavcodec/arm/cabac.h ffmpeg-3.2.4.patch/libavcodec/arm/cabac.h
--- ffmpeg-3.2.4/libavcodec/arm/cabac.h	2016-03-29 04:25:10.000000000 +0200
+++ ffmpeg-3.2.4.patch/libavcodec/arm/cabac.h	2017-05-28 20:42:45.715088584 +0200
@@ -26,13 +26,34 @@
 #include "libavutil/internal.h"
 #include "libavcodec/cabac.h"
 
+
+#if UNCHECKED_BITSTREAM_READER
+#define LOAD_16BITS_BEHI\
+        "ldrh       %[tmp]        , [%[ptr]]    , #2            \n\t"\
+        "rev        %[tmp]        , %[tmp]                      \n\t"
+#elif CONFIG_THUMB
+#define LOAD_16BITS_BEHI\
+        "ldr        %[tmp]        , [%[c], %[end]]              \n\t"\
+        "cmp        %[tmp]        , %[ptr]                      \n\t"\
+        "it         cs                                          \n\t"\
+        "ldrhcs     %[tmp]        , [%[ptr]]    , #2            \n\t"\
+        "rev        %[tmp]        , %[tmp]                      \n\t"
+#else
+#define LOAD_16BITS_BEHI\
+        "ldr        %[tmp]        , [%[c], %[end]]              \n\t"\
+        "cmp        %[tmp]        , %[ptr]                      \n\t"\
+        "ldrcsh     %[tmp]        , [%[ptr]]    , #2            \n\t"\
+        "rev        %[tmp]        , %[tmp]                      \n\t"
+#endif
+
+
 #define get_cabac_inline get_cabac_inline_arm
 static av_always_inline int get_cabac_inline_arm(CABACContext *c,
                                                  uint8_t *const state)
 {
     int bit;
+#if 0
     void *reg_b, *reg_c, *tmp;
-
     __asm__ volatile(
         "ldrb       %[bit]        , [%[state]]                  \n\t"
         "add        %[r_b]        , %[tables]   , %[lps_off]    \n\t"
@@ -100,9 +121,141 @@
           [mlps_off]"I"(H264_MLPS_STATE_OFFSET + 128)
         : "memory", "cc"
         );
+#else
+   // *** Not thumb compatible yet
+   unsigned int reg_b, tmp;
+    __asm__ (
+        "ldrb       %[bit]        , [%[state]]                  \n\t"
+        "sub        %[r_b]        , %[mlps_tables], %[lps_off]  \n\t"
+        "and        %[tmp]        , %[range]    , #0xC0         \n\t"
+        "add        %[r_b]        , %[r_b]      , %[bit]        \n\t"
+        "ldrb       %[tmp]        , [%[r_b]     , %[tmp], lsl #1] \n\t"
+// %bit = *state
+// %range = range
+// %tmp = RangeLPS
+        "sub        %[range]      , %[range]    , %[tmp]        \n\t"
+
+        "cmp        %[low]        , %[range]    , lsl #17       \n\t"
+        "ittt       ge                                          \n\t"
+        "subge      %[low]        , %[low]      , %[range], lsl #17 \n\t"
+        "mvnge      %[bit]        , %[bit]                      \n\t"
+        "movge      %[range]      , %[tmp]                      \n\t"
+
+        "clz        %[tmp]        , %[range]                    \n\t"
+        "sub        %[tmp]        , #23                         \n\t"
+
+        "ldrb       %[r_b]        , [%[mlps_tables], %[bit]]    \n\t"
+        "lsl        %[low]        , %[low]      , %[tmp]        \n\t"
+        "lsl        %[range]      , %[range]    , %[tmp]        \n\t"
+
+        "strb       %[r_b]        , [%[state]]                  \n\t"
+        "lsls       %[tmp]        , %[low]      , #16           \n\t"
+
+        "bne        2f                                          \n\t"
+        LOAD_16BITS_BEHI
+        "lsr        %[tmp]        , %[tmp]      , #15           \n\t"
+        "movw       %[r_b]        , #0xFFFF                     \n\t"
+        "sub        %[tmp]        , %[tmp]      , %[r_b]        \n\t"
+
+        "rbit       %[r_b]        , %[low]                      \n\t"
+        "clz        %[r_b]        , %[r_b]                      \n\t"
+        "sub        %[r_b]        , %[r_b]      , #16           \n\t"
+#if CONFIG_THUMB
+        "lsl        %[tmp]        , %[tmp]      , %[r_b]        \n\t"
+        "add        %[low]        , %[low]      , %[tmp]        \n\t"
+#else
+        "add        %[low]        , %[low]      , %[tmp], lsl %[r_b] \n\t"
+#endif
+        "2:                                                     \n\t"
+        :    [bit]"=&r"(bit),
+             [low]"+&r"(c->low),
+           [range]"+&r"(c->range),
+             [r_b]"=&r"(reg_b),
+             [ptr]"+&r"(c->bytestream),
+             [tmp]"=&r"(tmp)
+          :  [state]"r"(state),
+            [mlps_tables]"r"(ff_h264_cabac_tables + H264_MLPS_STATE_OFFSET + 128),
+              [byte]"M"(offsetof(CABACContext, bytestream)),
+#if !UNCHECKED_BITSTREAM_READER
+                 [c]"r"(c),
+               [end]"M"(offsetof(CABACContext, bytestream_end)),
+#endif
+           [lps_off]"I"((H264_MLPS_STATE_OFFSET + 128) - H264_LPS_RANGE_OFFSET)
+        : "memory", "cc"
+        );
+#endif
 
     return bit & 1;
 }
+
+#define get_cabac_bypass get_cabac_bypass_arm
+static inline int get_cabac_bypass_arm(CABACContext * const c)
+{
+    int rv = 0;
+    unsigned int tmp;
+    __asm (
+        "lsl        %[low]        , #1                          \n\t"
+        "cmp        %[low]        , %[range]    , lsl #17       \n\t"
+        "adc        %[rv]         , %[rv]       , #0            \n\t"
+        "it         cs                                          \n\t"
+        "subcs      %[low]        , %[low]      , %[range], lsl #17 \n\t"
+        "lsls       %[tmp]        , %[low]      , #16           \n\t"
+        "bne        1f                                          \n\t"
+        LOAD_16BITS_BEHI
+        "add        %[low]        , %[low]      , %[tmp], lsr #15 \n\t"
+        "movw       %[tmp]        , #0xFFFF                     \n\t"
+        "sub        %[low]        , %[low]      , %[tmp]        \n\t"
+        "1:                                                     \n\t"
+        : // Outputs
+              [rv]"+&r"(rv),
+             [low]"+&r"(c->low),
+             [tmp]"=&r"(tmp),
+             [ptr]"+&r"(c->bytestream)
+        : // Inputs
+#if !UNCHECKED_BITSTREAM_READER
+                 [c]"r"(c),
+               [end]"M"(offsetof(CABACContext, bytestream_end)),
+#endif
+             [range]"r"(c->range)
+        : "cc"
+    );
+    return rv;
+}
+
+
+#define get_cabac_bypass_sign get_cabac_bypass_sign_arm
+static inline int get_cabac_bypass_sign_arm(CABACContext * const c, int rv)
+{
+    unsigned int tmp;
+    __asm (
+        "lsl        %[low]        , #1                          \n\t"
+        "cmp        %[low]        , %[range]    , lsl #17       \n\t"
+        "ite        cc                                          \n\t"
+        "rsbcc      %[rv]         , %[rv]       , #0            \n\t"
+        "subcs      %[low]        , %[low]      , %[range], lsl #17 \n\t"
+        "lsls       %[tmp]        , %[low]      , #16           \n\t"
+        "bne        1f                                          \n\t"
+        LOAD_16BITS_BEHI
+        "add        %[low]        , %[low]      , %[tmp], lsr #15 \n\t"
+        "movw       %[tmp]        , #0xFFFF                     \n\t"
+        "sub        %[low]        , %[low]      , %[tmp]        \n\t"
+        "1:                                                     \n\t"
+        : // Outputs
+              [rv]"+&r"(rv),
+             [low]"+&r"(c->low),
+             [tmp]"=&r"(tmp),
+             [ptr]"+&r"(c->bytestream)
+        : // Inputs
+#if !UNCHECKED_BITSTREAM_READER
+                 [c]"r"(c),
+               [end]"M"(offsetof(CABACContext, bytestream_end)),
+#endif
+             [range]"r"(c->range)
+        : "cc"
+    );
+    return rv;
+}
+
 #endif /* HAVE_ARMV6T2_INLINE */
 
 #endif /* AVCODEC_ARM_CABAC_H */
diff -Naur ffmpeg-3.2.4/libavcodec/arm/hevc_cabac.h ffmpeg-3.2.4.patch/libavcodec/arm/hevc_cabac.h
--- ffmpeg-3.2.4/libavcodec/arm/hevc_cabac.h	1970-01-01 01:00:00.000000000 +0100
+++ ffmpeg-3.2.4.patch/libavcodec/arm/hevc_cabac.h	2017-05-28 20:42:45.716088588 +0200
@@ -0,0 +1,491 @@
+/*
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#ifndef AVCODEC_ARM_HEVC_CABAC_H
+#define AVCODEC_ARM_HEVC_CABAC_H
+
+#include "config.h"
+#if HAVE_ARMV6T2_INLINE
+
+#define hevc_mem_bits32 hevc_mem_bits32_arm
+static inline uint32_t hevc_mem_bits32_arm(const void * p, const unsigned int bits)
+{
+    unsigned int n;
+    __asm__ (
+        "rev        %[n], %[x]                     \n\t"
+        : [n]"=r"(n)
+        : [x]"r"(*(const uint32_t *)((const uint8_t *)p + (bits >> 3)))
+        :
+        );
+    return n << (bits & 7);
+}
+
+
+// ---------------------------------------------------------------------------
+//
+// Helper fns - little bits of code where ARM has an instraction that the
+// compiler doesn't know about / use
+
+#define trans_scale_sat trans_scale_sat_arm
+static inline int trans_scale_sat_arm(const int level, const unsigned int scale, const unsigned int scale_m, const unsigned int shift)
+{
+    int rv;
+    int t = ((level * (int)(scale * scale_m)) >> shift) + 1;
+
+    __asm__ (
+    "ssat %[rv], #16, %[t], ASR #1 \n\t"
+    : [rv]"=r"(rv)
+    : [t]"r"(t)
+    :
+    );
+    return rv;
+}
+
+#define update_rice update_rice_arm
+static inline void update_rice_arm(uint8_t * const stat_coeff,
+    const unsigned int last_coeff_abs_level_remaining,
+    const unsigned int c_rice_param)
+{
+    int t;
+    __asm__ (
+    "lsl   %[t], %[coeff], #1               \n\t"
+    "lsrs  %[t], %[t], %[shift]             \n\t"
+    "it    eq                               \n\t"
+    "subeq %[stat], %[stat], #1             \n\t"
+    "cmp   %[t], #6                         \n\t"
+    "adc   %[stat], %[stat], #0             \n\t"
+    "usat  %[stat], #8, %[stat]             \n\t"
+    : [stat]"+&r"(*stat_coeff),
+         [t]"=&r"(t)
+    :  [coeff]"r"(last_coeff_abs_level_remaining),
+       [shift]"r"(c_rice_param)
+    : "cc"
+    );
+}
+
+// ---------------------------------------------------------------------------
+//
+// CABAC get loops
+//
+// Where the loop is simple enough we can normally do 10-30% better than the
+// compiler
+
+// Get the residual greater than 1 bits
+
+#define get_cabac_greater1_bits get_cabac_greater1_bits_arm
+static inline unsigned int get_cabac_greater1_bits_arm(CABACContext * const c, const unsigned int n,
+    uint8_t * const state0)
+{
+    unsigned int i, reg_b, st, tmp, bit, rv;
+     __asm__ (
+         "mov        %[i]          , #0                          \n\t"
+         "mov        %[rv]         , #0                          \n\t"
+         "1:                                                     \n\t"
+         "add        %[i]          , %[i]        , #1            \n\t"
+         "cmp        %[rv]         , #0                          \n\t"
+         "ite        eq                                          \n\t"
+         "usateq     %[st]         , #2          , %[i]          \n\t"
+         "movne      %[st]         , #0                          \n\t"
+
+         "ldrb       %[bit]        , [%[state0], %[st]]          \n\t"
+         "sub        %[r_b]        , %[mlps_tables], %[lps_off]  \n\t"
+         "and        %[tmp]        , %[range]    , #0xC0         \n\t"
+         "add        %[r_b]        , %[r_b]      , %[bit]        \n\t"
+         "ldrb       %[tmp]        , [%[r_b], %[tmp], lsl #1]    \n\t"
+         "sub        %[range]      , %[range]    , %[tmp]        \n\t"
+
+         "cmp        %[low]        , %[range], lsl #17           \n\t"
+         "ittt       ge                                          \n\t"
+         "subge      %[low]        , %[low]      , %[range], lsl #17 \n\t"
+         "mvnge      %[bit]        , %[bit]                      \n\t"
+         "movge      %[range]      , %[tmp]                      \n\t"
+
+         "ldrb       %[r_b]        , [%[mlps_tables], %[bit]]    \n\t"
+         "and        %[bit]        , %[bit]      , #1            \n\t"
+         "orr        %[rv]         , %[bit]      , %[rv], lsl #1 \n\t"
+
+         "clz        %[tmp]        , %[range]                    \n\t"
+         "sub        %[tmp]        , #23                         \n\t"
+
+         "lsl        %[low]        , %[low]      , %[tmp]        \n\t"
+         "lsl        %[range]      , %[range]    , %[tmp]        \n\t"
+
+         "strb       %[r_b]        , [%[state0], %[st]]          \n\t"
+// There is a small speed gain from combining both conditions, using a single
+// branch and then working out what that meant later
+         "lsls       %[tmp]        , %[low]      , #16           \n\t"
+         "it         ne                                          \n\t"
+         "cmpne      %[n]          , %[i]                        \n\t"
+         "bne        1b                                          \n\t"
+
+// If reload is not required then we must have run out of flags to decode
+         "tst        %[tmp]        , %[tmp]                      \n\t"
+         "bne        2f                                          \n\t"
+
+// Do reload
+         "ldrh       %[tmp]        , [%[bptr]]   , #2            \n\t"
+         "movw       %[r_b]        , #0xFFFF                     \n\t"
+         "rev        %[tmp]        , %[tmp]                      \n\t"
+         "rsb        %[tmp]        , %[r_b]      , %[tmp], lsr #15 \n\t"
+
+         "rbit       %[r_b]        , %[low]                      \n\t"
+         "clz        %[r_b]        , %[r_b]                      \n\t"
+         "sub        %[r_b]        , %[r_b]      , #16           \n\t"
+
+#if CONFIG_THUMB
+         "lsl        %[tmp]        , %[tmp]      , %[r_b]        \n\t"
+         "add        %[low]        , %[low]      , %[tmp]        \n\t"
+#else
+         "add        %[low]        , %[low]      , %[tmp], lsl %[r_b] \n\t"
+#endif
+
+         "cmp        %[n]          , %[i]                        \n\t"
+         "bne        1b                                          \n\t"
+         "2:                                                     \n\t"
+         :    [bit]"=&r"(bit),
+              [low]"+&r"(c->low),
+            [range]"+&r"(c->range),
+              [r_b]"=&r"(reg_b),
+             [bptr]"+&r"(c->bytestream),
+                [i]"=&r"(i),
+              [tmp]"=&r"(tmp),
+               [st]"=&r"(st),
+               [rv]"=&r"(rv)
+          :  [state0]"r"(state0),
+                  [n]"r"(n),
+        [mlps_tables]"r"(ff_h264_cabac_tables + H264_MLPS_STATE_OFFSET + 128),
+               [byte]"M"(offsetof(CABACContext, bytestream)),
+            [lps_off]"I"((H264_MLPS_STATE_OFFSET + 128) - H264_LPS_RANGE_OFFSET)
+         : "memory", "cc"
+    );
+    return rv;
+}
+
+
+// n must be > 0 on entry
+#define get_cabac_sig_coeff_flag_idxs get_cabac_sig_coeff_flag_idxs_arm
+static inline uint8_t * get_cabac_sig_coeff_flag_idxs_arm(CABACContext * const c, uint8_t * const state0,
+    unsigned int n,
+    const uint8_t const * ctx_map,
+    uint8_t * p)
+{
+    unsigned int reg_b, tmp, st, bit;
+     __asm__ (
+         "1:                                                     \n\t"
+// Get bin from map
+         "ldrb       %[st]         , [%[ctx_map], %[n]]          \n\t"
+
+// Load state & ranges
+         "sub        %[r_b]        , %[mlps_tables], %[lps_off]  \n\t"
+         "ldrb       %[bit]        , [%[state0], %[st]]          \n\t"
+         "and        %[tmp]        , %[range]    , #0xC0         \n\t"
+         "add        %[r_b]        , %[r_b]      , %[tmp], lsl #1 \n\t"
+         "ldrb       %[tmp]        , [%[r_b], %[bit]]            \n\t"
+         "sub        %[range]      , %[range]    , %[tmp]        \n\t"
+
+         "cmp        %[low]        , %[range], lsl #17           \n\t"
+         "ittt       ge                                          \n\t"
+         "subge      %[low]        , %[low]      , %[range], lsl #17 \n\t"
+         "mvnge      %[bit]        , %[bit]                      \n\t"
+         "movge      %[range]      , %[tmp]                      \n\t"
+
+         "ldrb       %[r_b]        , [%[mlps_tables], %[bit]]    \n\t"
+         "tst        %[bit]        , #1                          \n\t"
+// GCC asm seems to need strbne written differently for thumb and arm
+#if CONFIG_THUMB
+         "it         ne                                          \n\t"
+         "strbne     %[n]          , [%[idx]]    , #1            \n\t"
+#else
+         "strneb     %[n]          , [%[idx]]    , #1            \n\t"
+#endif
+
+// Renorm
+         "clz        %[tmp]        , %[range]                    \n\t"
+         "sub        %[tmp]        , #23                         \n\t"
+         "lsl        %[low]        , %[low]      , %[tmp]        \n\t"
+         "lsl        %[range]      , %[range]    , %[tmp]        \n\t"
+
+         "strb       %[r_b]        , [%[state0], %[st]]          \n\t"
+// There is a small speed gain from combining both conditions, using a single
+// branch and then working out what that meant later
+         "subs       %[n]          , %[n]        , #1            \n\t"
+#if CONFIG_THUMB
+         "itt        ne                                          \n\t"
+         "lslsne     %[tmp]        , %[low]      , #16           \n\t"
+         "bne        1b                                          \n\t"
+#else
+         "lslnes     %[tmp]        , %[low]      , #16           \n\t"
+         "bne        1b                                          \n\t"
+#endif
+
+// If we have bits left then n must be 0 so give up now
+         "lsls       %[tmp]        , %[low]      , #16           \n\t"
+         "bne        2f                                          \n\t"
+
+// Do reload
+         "ldrh       %[tmp]        , [%[bptr]]   , #2            \n\t"
+         "movw       %[r_b]        , #0xFFFF                     \n\t"
+         "rev        %[tmp]        , %[tmp]                      \n\t"
+         "rsb        %[tmp]        , %[r_b]      , %[tmp], lsr #15 \n\t"
+
+         "rbit       %[r_b]        , %[low]                      \n\t"
+         "clz        %[r_b]        , %[r_b]                      \n\t"
+         "sub        %[r_b]        , %[r_b]      , #16           \n\t"
+
+#if CONFIG_THUMB
+         "lsl        %[tmp]        , %[tmp]      , %[r_b]        \n\t"
+         "add        %[low]        , %[low]      , %[tmp]        \n\t"
+#else
+         "add        %[low]        , %[low]      , %[tmp], lsl %[r_b] \n\t"
+#endif
+
+// Check to see if we still have more to do
+         "cmp        %[n]          , #0                          \n\t"
+         "bne        1b                                          \n\t"
+         "2:                                                     \n\t"
+         :    [bit]"=&r"(bit),
+              [low]"+&r"(c->low),
+            [range]"+&r"(c->range),
+              [r_b]"=&r"(reg_b),
+             [bptr]"+&r"(c->bytestream),
+              [idx]"+&r"(p),
+                [n]"+&r"(n),
+              [tmp]"=&r"(tmp),
+               [st]"=&r"(st)
+          :  [state0]"r"(state0),
+            [ctx_map]"r"(ctx_map),
+        [mlps_tables]"r"(ff_h264_cabac_tables + H264_MLPS_STATE_OFFSET + 128),
+               [byte]"M"(offsetof(CABACContext, bytestream)),
+            [lps_off]"I"((H264_MLPS_STATE_OFFSET + 128) - H264_LPS_RANGE_OFFSET)
+         : "memory", "cc"
+    );
+
+    return p;
+}
+
+// ---------------------------------------------------------------------------
+//
+// CABAC_BY22 functions
+//
+// By and large these are (at best) no faster than their C equivalents - the
+// only one worth having is _peek where we do a slightly better job than the
+// compiler
+//
+// The others have been stashed here for reference in case larger scale asm
+// is attempted in which case they might be a useful base
+
+
+#define get_cabac_by22_peek get_cabac_by22_peek_arm
+static inline uint32_t get_cabac_by22_peek_arm(const CABACContext *const c)
+{
+    uint32_t rv, tmp;
+    __asm__ (
+        "bic      %[rv]  , %[low], #1            \n\t"
+        "cmp      %[inv] , #0                    \n\t"
+        "it       ne                             \n\t"
+        "umullne  %[tmp] , %[rv] , %[inv], %[rv] \n\t"
+        :  // Outputs
+             [rv]"=&r"(rv),
+             [tmp]"=r"(tmp)
+        :  // Inputs
+             [low]"r"(c->low),
+             [inv]"r"(c->range)
+        :  // Clobbers
+                "cc"
+    );
+    return rv << 1;
+}
+
+#if 0
+
+// ***** Slower than the C  :-(
+#define get_cabac_by22_flush get_cabac_by22_flush_arm
+static inline void get_cabac_by22_flush_arm(CABACContext *const c, const unsigned int n, const uint32_t val)
+{
+    uint32_t m, tmp;
+    __asm__ (
+    "add    %[bits], %[bits], %[n]   \n\t"
+    "ldr    %[m], [%[ptr], %[bits], lsr #3]  \n\t"
+
+    "rsb    %[tmp], %[n], #32        \n\t"
+    "lsr    %[tmp], %[val], %[tmp]   \n\t"
+    "mul    %[tmp], %[range], %[tmp] \n\t"
+
+    "rev    %[m], %[m]               \n\t"
+
+    "lsl    %[tmp], %[tmp], #23      \n\t"
+    "rsb    %[low], %[tmp], %[low], lsl %[n] \n\t"
+
+    "and    %[tmp], %[bits], #7         \n\t"
+    "lsl    %[m], %[m], %[tmp]          \n\t"
+
+    "orr    %[low], %[low], %[m], lsr #9      \n\t"
+        :  // Outputs
+             [m]"=&r"(m),
+           [tmp]"=&r"(tmp),
+          [bits]"+&r"(c->by22.bits),
+           [low]"+&r"(c->low)
+        :  // Inputs
+               [n]"r"(n),
+             [val]"r"(val),
+             [inv]"r"(c->range),
+           [range]"r"(c->by22.range),
+             [ptr]"r"(c->bytestream)
+        :  // Clobbers
+    );
+}
+
+
+// Works but slower than C
+#define coeff_abs_level_remaining_decode_by22(c,r) coeff_abs_level_remaining_decode_by22_arm(c, r)
+static int coeff_abs_level_remaining_decode_by22_arm(CABACContext * const c, const unsigned int c_rice_param)
+{
+    uint32_t n, val, tmp, level;
+
+//    PROFILE_START();
+
+    __asm__ (
+            // Peek
+            "bic    %[val],  %[low],   #1  \n\t"
+            "cmp    %[inv], #0          \n\t"
+            "umullne  %[tmp], %[val], %[inv], %[val] \n\t"
+            "lsl    %[val], %[val], #1  \n\t"
+
+            // Count bits (n = prefix)
+            "mvn    %[n], %[val] \n\t"
+            "clz    %[n], %[n]   \n\t"
+
+            "lsl    %[level], %[val], %[n] \n\t"
+            "subs   %[tmp], %[n], #3 \n\t"
+            "blo    2f \n\t"
+
+            // prefix >= 3
+            // < tmp = prefix - 3
+            // > tmp = prefix + rice - 3
+            "add    %[tmp], %[tmp], %[rice] \n\t"
+            // > n = prefix * 2 + rice - 3
+            "add    %[n], %[tmp], %[n] \n\t"
+            "cmp    %[n], #21 \n\t"
+            "bhi    3f \n\t"
+
+            "orr    %[level], %[level], #0x80000000 \n\t"
+            "rsb    %[tmp], %[tmp], #31 \n\t"
+            "lsr    %[level], %[level], %[tmp] \n\t"
+
+            "mov    %[tmp], #2 \n\t"
+            "add    %[level], %[level], %[tmp], lsl %[rice] \n\t"
+            "b      1f \n\t"
+
+            // > 22 bits used in total - need reload
+            "3:  \n\t"
+
+            // Stash prefix + rice - 3 in level (only spare reg)
+            "mov    %[level], %[tmp] \n\t"
+            // Restore n to flush value (prefix)
+            "sub    %[n], %[n], %[tmp] \n\t"
+
+            // Flush + reload
+
+//          "rsb    %[tmp], %[n], #32        \n\t"
+//          "lsr    %[tmp], %[val], %[tmp]   \n\t"
+//          "mul    %[tmp], %[range], %[tmp] \n\t"
+
+            // As it happens we know that all the bits we are flushing are 1
+            // so we can cheat slightly
+            "rsb    %[tmp], %[range], %[range], lsl %[n] \n\t"
+            "lsl    %[tmp], %[tmp], #23      \n\t"
+            "rsb    %[low], %[tmp], %[low], lsl %[n] \n\t"
+
+            "add    %[bits], %[bits], %[n]   \n\t"
+            "ldr    %[n], [%[ptr], %[bits], lsr #3]  \n\t"
+            "rev    %[n], %[n]               \n\t"
+            "and    %[tmp], %[bits], #7         \n\t"
+            "lsl    %[n], %[n], %[tmp]          \n\t"
+
+            "orr    %[low], %[low], %[n], lsr #9      \n\t"
+
+            // (reload)
+
+            "bic    %[val],  %[low],   #1  \n\t"
+            "cmp    %[inv], #0          \n\t"
+            "umullne  %[tmp], %[val], %[inv], %[val] \n\t"
+            "lsl    %[val], %[val], #1  \n\t"
+
+            // Build value
+
+            "mov    %[n], %[level] \n\t"
+
+            "orr     %[tmp], %[val], #0x80000000 \n\t"
+            "rsb     %[level], %[level], #31 \n\t"
+            "lsr     %[level], %[tmp], %[level] \n\t"
+
+            "mov    %[tmp], #2 \n\t"
+            "add    %[level], %[level], %[tmp], lsl %[rice] \n\t"
+            "b      1f \n\t"
+
+            // prefix < 3
+            "2:  \n\t"
+            "rsb    %[tmp], %[rice], #31 \n\t"
+            "lsr    %[level], %[level], %[tmp] \n\t"
+            "orr    %[level], %[level], %[n], lsl %[rice] \n\t"
+            "add    %[n], %[n], %[rice] \n\t"
+
+            "1:  \n\t"
+            // Flush
+            "add    %[n], %[n], #1 \n\t"
+
+            "rsb    %[tmp], %[n], #32        \n\t"
+            "lsr    %[tmp], %[val], %[tmp]   \n\t"
+
+            "add    %[bits], %[bits], %[n]   \n\t"
+            "ldr    %[val], [%[ptr], %[bits], lsr #3]  \n\t"
+
+            "mul    %[tmp], %[range], %[tmp] \n\t"
+            "lsl    %[tmp], %[tmp], #23      \n\t"
+            "rsb    %[low], %[tmp], %[low], lsl %[n] \n\t"
+
+            "rev    %[val], %[val]               \n\t"
+            "and    %[tmp], %[bits], #7         \n\t"
+            "lsl    %[val], %[val], %[tmp]          \n\t"
+
+            "orr    %[low], %[low], %[val], lsr #9      \n\t"
+        :  // Outputs
+         [level]"=&r"(level),
+             [n]"=&r"(n),
+           [val]"=&r"(val),
+           [tmp]"=&r"(tmp),
+          [bits]"+&r"(c->by22.bits),
+           [low]"+&r"(c->low)
+        :  // Inputs
+            [rice]"r"(c_rice_param),
+             [inv]"r"(c->range),
+           [range]"r"(c->by22.range),
+             [ptr]"r"(c->bytestream)
+        :  // Clobbers
+                "cc"
+    );
+
+//    PROFILE_ACC(residual_abs);
+
+    return level;
+}
+#endif
+
+#endif /* HAVE_ARMV6T2_INLINE */
+
+#endif /* AVCODEC_ARM_HEVC_CABAC_H */
diff -Naur ffmpeg-3.2.4/libavcodec/arm/hevcdsp_deblock_neon.S ffmpeg-3.2.4.patch/libavcodec/arm/hevcdsp_deblock_neon.S
--- ffmpeg-3.2.4/libavcodec/arm/hevcdsp_deblock_neon.S	2016-03-29 04:25:10.000000000 +0200
+++ ffmpeg-3.2.4.patch/libavcodec/arm/hevcdsp_deblock_neon.S	2017-05-28 20:42:45.718088595 +0200
@@ -15,7 +15,7 @@
  *
  * You should have received a copy of the GNU Lesser General Public
  * License along with FFmpeg; if not, write to the Free Software
- * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1
  */
 
 
@@ -31,6 +31,9 @@
         bxeq     lr
 .endm
 
+@ Uses: d2, d4, d18, d19
+@ Returns: d2, d4
+@ Modifies: d0-d7, d22-d25
 .macro hevc_loop_filter_chroma_body
         vsubl.u8  q3, d4, d2
         vsubl.u8  q11, d18, d19
@@ -49,6 +52,33 @@
         vqmovun.s16 d4, q2
 .endm
 
+
+@ Uses r2[0:7], r2[8:15]
+@ Modifies: d0-d7, d22-d25
+.macro hevc_loop_filter_uv_body P1, P0, Q0, Q1
+        vsubl.u8  q3, \Q0, \P0
+        vsubl.u8  q11, \P1, \Q1
+        vshl.i16  q3, #2
+        vadd.i16  q11, q3
+
+        @ r2[0:7] -> d0.16 (all), r2[8:15] -> d1.16(all)
+        vdup.16   d0, r2
+        vmovl.u8  q0, d0
+        vuzp.16   d0, d1
+
+        vrshr.s16 q11, q11, #3
+        vneg.s16  q12, q0
+        vmovl.u8  q2, \Q0
+        vmin.s16  q11, q11, q0
+        vmax.s16  q11, q11, q12
+        vaddw.u8  q1, q11, \P0
+        vsub.i16  q2, q11
+        vqmovun.s16 \P0, q1
+        vqmovun.s16 \Q0, q2
+.endm
+
+
+
 .macro hevc_loop_filter_luma_start
         ldr     r12, [r3]
         ldr      r3, [r3, #4]
@@ -60,15 +90,17 @@
         lsr      r3, #16
 .endm
 
-.macro hevc_loop_filter_luma_body
+@ Uses: r2, r3, r12
+@ Modifies: r5, r6, r7, r8, r9
+function hevc_loop_filter_luma_body
+        vmovl.u8  q15, d23
+        vmovl.u8  q14, d22
+        vmovl.u8  q13, d21
+        vmovl.u8  q12, d20
+        vmovl.u8  q11, d19
+        vmovl.u8  q10, d18
+        vmovl.u8  q9, d17
         vmovl.u8  q8, d16
-        vmovl.u8  q9, d18
-        vmovl.u8  q10, d20
-        vmovl.u8  q11, d22
-        vmovl.u8  q12, d24
-        vmovl.u8  q13, d26
-        vmovl.u8  q14, d28
-        vmovl.u8  q15, d30
 
         vadd.i16   q7, q9, q11
         vadd.i16   q6, q14, q12
@@ -77,7 +109,6 @@
         vabd.s16   q7, q7, q10
         vabd.s16   q6, q6, q13
 
-
         vdup.16    q0, r2
         vmov       q4, q7
         vmov       q5, q6
@@ -152,7 +183,7 @@
 
         and        r9, r8, r7
         cmp        r9, #0
-        beq        weakfilter_\@
+        beq        weakfilter_
 
         vadd.i16  q2, q11, q12
         vadd.i16  q4, q9, q8
@@ -210,11 +241,11 @@
         vbit      q13, q3, q5
         vbit      q14, q2, q5
 
-weakfilter_\@:
+weakfilter_:
         mvn       r8, r8
         and       r9, r8, r7
         cmp       r9, #0
-        beq       ready_\@
+        beq       ready_
 
         vdup.16    q4, r2
 
@@ -275,75 +306,345 @@
         vbit      q11, q0, q5
         vbit      q12, q4, q5
 
-ready_\@:
+ready_:
         vqmovun.s16 d16, q8
-        vqmovun.s16 d18, q9
-        vqmovun.s16 d20, q10
-        vqmovun.s16 d22, q11
-        vqmovun.s16 d24, q12
-        vqmovun.s16 d26, q13
-        vqmovun.s16 d28, q14
-        vqmovun.s16 d30, q15
-.endm
+        vqmovun.s16 d17, q9
+        vqmovun.s16 d18, q10
+        vqmovun.s16 d19, q11
+        vqmovun.s16 d20, q12
+        vqmovun.s16 d21, q13
+        vqmovun.s16 d22, q14
+        vqmovun.s16 d23, q15
+        mov       pc, lr
+endfunc
+
+@ ff_hevc_v_loop_filter_luma2_neon(src (r0), stride (r1), beta (r2), tc (r3), np_p (sp[0]), no_q (sp[4]), src2 (sp[8]))
+function ff_hevc_v_loop_filter_luma2_neon_8, export=1
+        hevc_loop_filter_luma_start
+        push     {r4-r10,lr}       @ 8 regs = 32 bytes
+
+        ldr      r4, [sp, #40]
+        b        v_loop_luma_common
+endfunc
+
 
 function ff_hevc_v_loop_filter_luma_neon, export=1
         hevc_loop_filter_luma_start
-        push     {r5-r11}
+        push     {r4-r10,lr}
+
+        sub      r4, r0, #4
+v_loop_luma_common:
+        @ Why this isn't a bitmask to start with I have no idea...
+        @ Beware that no_x[] seems to be loaded with 2/0 rather than 1/0
+        ldr      r5, [sp, #32]
+        ldrh     r10, [r5]
+        ldr      r5, [sp, #36]
+        ldrh     r5, [r5]
+        orr      r10, r10, r5, lsl #16  @ So should have b0:no_p[0], b8:no_p[1], b16: no_q[0], b24:no_q[1]
+
         vpush    {d8-d15}
-        sub      r0, #4
-        vld1.8   {d16}, [r0], r1
-        vld1.8   {d18}, [r0], r1
-        vld1.8   {d20}, [r0], r1
-        vld1.8   {d22}, [r0], r1
-        vld1.8   {d24}, [r0], r1
-        vld1.8   {d26}, [r0], r1
-        vld1.8   {d28}, [r0], r1
-        vld1.8   {d30}, [r0], r1
-        sub      r0, r0, r1, lsl #3
-        transpose_8x8 d16, d18, d20, d22, d24, d26, d28, d30
-        hevc_loop_filter_luma_body
-        transpose_8x8 d16, d18, d20, d22, d24, d26, d28, d30
-        vst1.8   {d16}, [r0], r1
-        vst1.8   {d18}, [r0], r1
-        vst1.8   {d20}, [r0], r1
-        vst1.8   {d22}, [r0], r1
-        vst1.8   {d24}, [r0], r1
-        vst1.8   {d26}, [r0], r1
-        vst1.8   {d28}, [r0], r1
-        vst1.8   {d30}, [r0]
+
+        @ Uses slightly fewer instructions to do laned loads than unlaned
+        @ and transpose.  This also means that we can use the same code for
+        @ both split & unsplit deblock
+        vld4.8  {d16[0],d17[0],d18[0],d19[0]}, [r4:32], r1
+        vld4.8  {d20[0],d21[0],d22[0],d23[0]}, [r0:32], r1
+
+        vld4.8  {d16[1],d17[1],d18[1],d19[1]}, [r4:32], r1
+        vld4.8  {d20[1],d21[1],d22[1],d23[1]}, [r0:32], r1
+
+        vld4.8  {d16[2],d17[2],d18[2],d19[2]}, [r4:32], r1
+        vld4.8  {d20[2],d21[2],d22[2],d23[2]}, [r0:32], r1
+
+        vld4.8  {d16[3],d17[3],d18[3],d19[3]}, [r4:32], r1
+        vld4.8  {d20[3],d21[3],d22[3],d23[3]}, [r0:32], r1
+
+        vld4.8  {d16[4],d17[4],d18[4],d19[4]}, [r4:32], r1
+        vld4.8  {d20[4],d21[4],d22[4],d23[4]}, [r0:32], r1
+
+        vld4.8  {d16[5],d17[5],d18[5],d19[5]}, [r4:32], r1
+        vld4.8  {d20[5],d21[5],d22[5],d23[5]}, [r0:32], r1
+
+        vld4.8  {d16[6],d17[6],d18[6],d19[6]}, [r4:32], r1
+        vld4.8  {d20[6],d21[6],d22[6],d23[6]}, [r0:32], r1
+
+        vld4.8  {d16[7],d17[7],d18[7],d19[7]}, [r4:32]
+        vld4.8  {d20[7],d21[7],d22[7],d23[7]}, [r0:32]
+
+        bl hevc_loop_filter_luma_body
+
+        neg     r1, r1
+
+        @ no_p[1]
+        tst     r10, #0xff00
+        itt ne
+        addne    r4, r4, r1, lsl #2
+        bne     1f
+        vst4.8  {d16[7],d17[7],d18[7],d19[7]}, [r4:32], r1
+        vst4.8  {d16[6],d17[6],d18[6],d19[6]}, [r4:32], r1
+        vst4.8  {d16[5],d17[5],d18[5],d19[5]}, [r4:32], r1
+        vst4.8  {d16[4],d17[4],d18[4],d19[4]}, [r4:32], r1
+
+1:
+        @ no_q[1]
+        tst     r10, #0xff000000
+        itt ne
+        addne    r0, r0, r1, lsl #2
+        bne     2f
+        vst4.8  {d20[7],d21[7],d22[7],d23[7]}, [r0:32], r1
+        vst4.8  {d20[6],d21[6],d22[6],d23[6]}, [r0:32], r1
+        vst4.8  {d20[5],d21[5],d22[5],d23[5]}, [r0:32], r1
+        vst4.8  {d20[4],d21[4],d22[4],d23[4]}, [r0:32], r1
+
+2:
+        @ no_p[0]
+        tst     r10, #0xff
+        bne     3f
+        vst4.8  {d16[3],d17[3],d18[3],d19[3]}, [r4:32], r1
+        vst4.8  {d16[2],d17[2],d18[2],d19[2]}, [r4:32], r1
+        vst4.8  {d16[1],d17[1],d18[1],d19[1]}, [r4:32], r1
+        vst4.8  {d16[0],d17[0],d18[0],d19[0]}, [r4:32]
+
+3:
+        @ no_q[0]
+        tst     r10, #0xff0000
+        bne     4f
+        vst4.8  {d20[3],d21[3],d22[3],d23[3]}, [r0:32], r1
+        vst4.8  {d20[2],d21[2],d22[2],d23[2]}, [r0:32], r1
+        vst4.8  {d20[1],d21[1],d22[1],d23[1]}, [r0:32], r1
+        vst4.8  {d20[0],d21[0],d22[0],d23[0]}, [r0:32]
+
+4:
+bypasswrite:
         vpop     {d8-d15}
-        pop      {r5-r11}
-        bx lr
+        pop      {r4-r10,pc}
 endfunc
 
+@ void (*hevc_h_loop_filter_luma)(uint8_t *pix,     [r0]
+@                                 ptrdiff_t stride, [r1]
+@                                 int beta,         [r2]
+@                                 int32_t *tc,      [r3]
+@                                 uint8_t *no_p,    sp[0]
+@                                 uint8_t *no_q);   sp[4]
+@
+@ Src should always be on 8 byte boundry & all in the same slice
+
 function ff_hevc_h_loop_filter_luma_neon, export=1
         hevc_loop_filter_luma_start
-        push     {r5-r11}
+        push     {r4-r10,lr}
+
         vpush    {d8-d15}
         sub      r0, r0, r1, lsl #2
+
         vld1.8  {d16}, [r0], r1
+        vld1.8  {d17}, [r0], r1
         vld1.8  {d18}, [r0], r1
+        vld1.8  {d19}, [r0], r1
         vld1.8  {d20}, [r0], r1
+        vld1.8  {d21}, [r0], r1
         vld1.8  {d22}, [r0], r1
-        vld1.8  {d24}, [r0], r1
-        vld1.8  {d26}, [r0], r1
-        vld1.8  {d28}, [r0], r1
-        vld1.8  {d30}, [r0], r1
-        sub        r0, r0, r1, lsl #3
-        add        r0, r1
-        hevc_loop_filter_luma_body
-        vst1.8   {d18}, [r0], r1
-        vst1.8   {d20}, [r0], r1
-        vst1.8   {d22}, [r0], r1
-        vst1.8   {d24}, [r0], r1
-        vst1.8   {d26}, [r0], r1
-        vst1.8   {d28}, [r0]
-bypasswrite:
+        vld1.8  {d23}, [r0]
+
+        bl hevc_loop_filter_luma_body
+
         vpop     {d8-d15}
-        pop      {r5-r11}
-        bx lr
+
+        neg     r1, r1
+        add     r0, r0, r1
+
+        @ Why this isn't a bitmask to start with I have no idea...
+        @ Beware that no_x[] seems to be loaded with 2/0 rather than 1/0
+        ldr      r5, [sp, #32]
+        ldrh     r10, [r5]
+        ldr      r5, [sp, #36]
+        ldrh     r5, [r5]
+        orrs     r10, r10, r5, lsl #16  @ So should have b1:no_p[0], b9:no_p[1], b17: no_q[0], b25:no_q[1]
+        bne      1f
+
+        vst1.8  {d22}, [r0], r1
+        vst1.8  {d21}, [r0], r1
+        vst1.8  {d20}, [r0], r1
+        vst1.8  {d19}, [r0], r1
+        vst1.8  {d18}, [r0], r1
+        vst1.8  {d17}, [r0]
+
+        pop      {r4-r10,pc}
+
+@ Partial write
+1:
+        vmov     r2, r3, d22
+        vmov     r4, r5, d21
+        vmov     r6, r7, d20
+
+        tst      r10, #0xff0000
+        ittt eq
+        streq    r2, [r0]
+        streq    r4, [r0, r1]
+        streq    r6, [r0, r1, lsl # 1]
+
+        add      r0, r0, #4
+        tst      r10, #0xff000000
+        ittt eq
+        streq    r3, [r0]
+        streq    r5, [r0, r1]
+        streq    r7, [r0, r1, lsl # 1]
+
+        vmov     r2, r3, d19
+        vmov     r4, r5, d18
+        vmov     r6, r7, d17
+        add      r0, r0, r1
+        add      r0, r0, r1, lsl # 1
+
+        tst      r10, #0xff00
+        ittt eq
+        streq    r3, [r0]
+        streq    r5, [r0, r1]
+        streq    r7, [r0, r1, lsl # 1]
+
+        tst      r10, #0xff
+        ittt eq
+        streq    r2, [r0, #-4]!
+        streq    r4, [r0, r1]
+        streq    r6, [r0, r1, lsl # 1]
+
+        pop      {r4-r10,pc}
+
 endfunc
 
+@ void ff_hevc_h_loop_filter_uv_neon(uint8_t * src_r,        // r0
+@                                     unsigned int stride,   // r1
+@                                     uint32_t tc4,          // r2
+@                                     unsigned int no_f);    // r3
+@
+@ no-F = b0:no_p[0], b1:no_p[1], b2:no_q[0], b3:no_q[1]
+function ff_hevc_h_loop_filter_uv_neon_8, export=1
+        sub      r0, r0, r1, lsl #1
+        vld2.8   {d16,d17}, [r0], r1
+        vld2.8   {d18,d19}, [r0], r1
+        vld2.8   {d26,d27}, [r0], r1
+        vld2.8   {d28,d29}, [r0]
+        sub      r0, r0, r1, lsl #1
+        hevc_loop_filter_uv_body d16, d18, d26, d28
+        lsr      r2, r2, #16
+        hevc_loop_filter_uv_body d17, d19, d27, d29
+        cmp      r3, #0
+        bne      1f
+        vst2.8   {d18,d19}, [r0], r1
+        vst2.8   {d26,d27}, [r0]
+        bx       lr
+
+        @ At least one no_f bit is set
+        @ Which means we need to break this apart in an ugly fashion
+1:      vzip.8   d18, d19
+        vzip.8   d26, d27
+        sub      r1, r1, #8
+
+        tst      r3, #1
+        bne      1f
+        vst1.8   {d18}, [r0]
+1:      add      r0, r0, #8
+        tst      r3, #2
+        bne      2f
+        vst1.8   {d19}, [r0]
+2:      add      r0, r0, r1
+
+        tst      r3, #4
+        bne      1f
+        vst1.8   {d26}, [r0]
+1:      add      r0, r0, #8
+        tst      r3, #8
+        it ne
+        bxne     lr
+        vst1.8   {d27}, [r0]
+        bx       lr
+
+endfunc
+
+
+@ void ff_hevc_v_loop_filter_uv2_neon(uint8_t * src_r,       // r0
+@                                     unsigned int stride,   // r1
+@                                     uint32_t tc4,          // r2
+@                                     uint8_t * src_l,       // r3
+@                                     unsigned int no_f);   // sp[0]
+@
+@ no-F = b0:no_p[0], b1:no_p[1], b2:no_q[0], b3:no_q[1]
+function ff_hevc_v_loop_filter_uv2_neon_8, export=1
+        vld4.8   {d16[0], d17[0], d18[0], d19[0]}, [r3], r1
+        vld4.8   {d26[0], d27[0], d28[0], d29[0]}, [r0], r1
+
+        vld4.8   {d16[1], d17[1], d18[1], d19[1]}, [r3], r1
+        vld4.8   {d26[1], d27[1], d28[1], d29[1]}, [r0], r1
+
+        vld4.8   {d16[2], d17[2], d18[2], d19[2]}, [r3], r1
+        vld4.8   {d26[2], d27[2], d28[2], d29[2]}, [r0], r1
+
+        vld4.8   {d16[3], d17[3], d18[3], d19[3]}, [r3], r1
+        vld4.8   {d26[3], d27[3], d28[3], d29[3]}, [r0], r1
+
+        vld4.8   {d16[4], d17[4], d18[4], d19[4]}, [r3], r1
+        vld4.8   {d26[4], d27[4], d28[4], d29[4]}, [r0], r1
+
+        vld4.8   {d16[5], d17[5], d18[5], d19[5]}, [r3], r1
+        vld4.8   {d26[5], d27[5], d28[5], d29[5]}, [r0], r1
+
+        vld4.8   {d16[6], d17[6], d18[6], d19[6]}, [r3], r1
+        vld4.8   {d26[6], d27[6], d28[6], d29[6]}, [r0], r1
+
+        vld4.8   {d16[7], d17[7], d18[7], d19[7]}, [r3]
+        vld4.8   {d26[7], d27[7], d28[7], d29[7]}, [r0]
+
+        hevc_loop_filter_uv_body d16, d18, d26, d28
+        lsr      r2, r2, #16
+        hevc_loop_filter_uv_body d17, d19, d27, d29
+
+        neg      r1, r1
+
+        ldr      r2, [sp, #0]
+
+        @ p[1]
+        tst      r2, #2
+        itt ne
+        addne    r3, r3, r1, lsl #2
+        bne      1f
+        vst4.8   {d16[7], d17[7], d18[7], d19[7]}, [r3], r1
+        vst4.8   {d16[6], d17[6], d18[6], d19[6]}, [r3], r1
+        vst4.8   {d16[5], d17[5], d18[5], d19[5]}, [r3], r1
+        vst4.8   {d16[4], d17[4], d18[4], d19[4]}, [r3], r1
+
+1:
+        @ q[1]
+        tst      r2, #8
+        itt ne
+        addne    r0, r0, r1, lsl #2
+        bne 2f
+        vst4.8   {d26[7], d27[7], d28[7], d29[7]}, [r0], r1
+        vst4.8   {d26[6], d27[6], d28[6], d29[6]}, [r0], r1
+        vst4.8   {d26[5], d27[5], d28[5], d29[5]}, [r0], r1
+        vst4.8   {d26[4], d27[4], d28[4], d29[4]}, [r0], r1
+
+2:
+        @ p[0]
+        tst      r2, #1
+        bne      3f
+        vst4.8   {d16[3], d17[3], d18[3], d19[3]}, [r3], r1
+        vst4.8   {d16[2], d17[2], d18[2], d19[2]}, [r3], r1
+        vst4.8   {d16[1], d17[1], d18[1], d19[1]}, [r3], r1
+        vst4.8   {d16[0], d17[0], d18[0], d19[0]}, [r3]
+
+3:
+        @ q[0]
+        tst      r2, #4
+        it ne
+        bxne     lr
+        vst4.8   {d26[3], d27[3], d28[3], d29[3]}, [r0], r1
+        vst4.8   {d26[2], d27[2], d28[2], d29[2]}, [r0], r1
+        vst4.8   {d26[1], d27[1], d28[1], d29[1]}, [r0], r1
+        vst4.8   {d26[0], d27[0], d28[0], d29[0]}, [r0]
+
+        bx       lr
+endfunc
+
+
 function ff_hevc_v_loop_filter_chroma_neon, export=1
         hevc_loop_filter_chroma_start
         sub      r0, #4
@@ -383,3 +684,128 @@
         vst1.8   {d4}, [r0]
         bx       lr
 endfunc
+
+/* ff_hevc_deblocking_boundary_strengths_neon(int pus, int dup, int in_i
+ *                                            int *curr_rpl0, int *curr_
+ *                                            MvField *curr, MvField *ne
+ */
+function ff_hevc_deblocking_boundary_strengths_neon, export=1
+        add         ip, sp, #4*4
+        push        {a2-a4,v1-v8,lr}
+        ldmia       ip, {v5-v7}
+1:      ldmdb       ip, {v1-v4}
+        ldrsb       a3, [v5, #8]    @ curr->ref_idx
+        ldrsb       v8, [v5, #9]
+        ldrsb       ip, [v6, #8]    @ neigh->ref_idx
+        ldrsb       lr, [v6, #9]
+        ldr         v1, [v1, a3, lsl #2]
+        ldrb        a3, [v5, #10]   @ curr->pred_flag
+        ldr         v2, [v2, v8, lsl #2]
+        ldrb        v8, [v6, #10]   @ neigh->pred_flag
+        ldr         v3, [v3, ip, lsl #2]
+        ldr         v4, [v4, lr, lsl #2]
+        teq         a3, #3
+        beq         20f
+        teq         v8, #3
+        beq         90f
+
+        tst         a3, #1
+        itee        ne
+        ldrne       a3, [v5, #0]    @ curr->mv[0]
+        ldreq       a3, [v5, #4]    @ curr->mv[1]
+        moveq       v1, v2
+        tst         v8, #1
+        itee        ne
+        ldrne       v8, [v6, #0]    @ neigh->mv[0]
+        ldreq       v8, [v6, #4]    @ neigh->mv[1]
+        moveq       v3, v4
+        teq         v1, v3
+        bne         10f
+        ldr         lr, =0xFFFCFFFC
+        ssub16      ip, v8, a3
+        ssub16      a3, a3, v8
+        sel         a3, a3, ip
+        ands        a3, a3, lr
+        @ drop through
+10:     it          ne
+        movne       a3, #1
+11:     subs        a2, a2, #1
+12:
+A       strbhs      a3, [v7], a4
+T       itt         hs
+T       strbhs      a3, [v7]
+T       addhs       v7, v7, a4
+        subs        a2, a2, #1
+        bhs         12b
+
+        ldm         sp, {a2, a3}
+        add         ip, sp, #16*4
+        subs        a1, a1, #1
+        add         v5, v5, a3
+        add         v6, v6, a3
+        bhi         1b
+        pop         {a2-a4,v1-v8,pc}
+
+20:     teq         v8, #3
+        bne         10b
+
+        teq         v1, v3
+        it          eq
+        teqeq       v2, v4
+        bne         40f
+        teq         v1, v2
+        bne         30f
+
+        ldrd        v1, v2, [v5]    @ curr->mv
+        ldrd        v3, v4, [v6]    @ neigh->mv
+        ldr         lr, =0xFFFCFFFC
+        ssub16      ip, v3, v1
+        ssub16      a3, v1, v3
+        sel         a3, a3, ip
+        ands        a3, a3, lr
+        bne         25f
+        ssub16      ip, v4, v2
+        ssub16      a3, v2, v4
+        sel         a3, a3, ip
+        ands        a3, a3, lr
+        beq         11b
+        @ drop through
+25:     ssub16      ip, v4, v1
+        ssub16      a3, v1, v4
+        sel         a3, a3, ip
+        ands        a3, a3, lr
+        bne         10b
+        ssub16      ip, v3, v2
+        ssub16      a3, v2, v3
+        sel         a3, a3, ip
+        ands        a3, a3, lr
+        b           10b
+
+30:     ldrd        v1, v2, [v5]    @ curr->mv
+        ldrd        v3, v4, [v6]    @ neigh->mv
+        ldr         lr, =0xFFFCFFFC
+        ssub16      ip, v3, v1
+        ssub16      a3, v1, v3
+        sel         a3, a3, ip
+        ands        a3, a3, lr
+        bne         10b
+        ssub16      ip, v4, v2
+        ssub16      a3, v2, v4
+        sel         a3, a3, ip
+        ands        a3, a3, lr
+        b           10b
+
+40:     teq         v1, v4
+        ite         eq
+        teqeq       v2, v3
+        bne         10b
+
+        ldrd        v1, v2, [v5]    @ curr->mv
+        ldrd        v3, v4, [v6]    @ neigh->mv
+        ldr         lr, =0xFFFCFFFC
+        b           25b
+
+90:     mov         a3, #1
+        b           11b
+endfunc
+
diff -Naur ffmpeg-3.2.4/libavcodec/arm/hevcdsp_epel_neon.S ffmpeg-3.2.4.patch/libavcodec/arm/hevcdsp_epel_neon.S
--- ffmpeg-3.2.4/libavcodec/arm/hevcdsp_epel_neon.S	1970-01-01 01:00:00.000000000 +0100
+++ ffmpeg-3.2.4.patch/libavcodec/arm/hevcdsp_epel_neon.S	2017-05-28 20:42:45.719088598 +0200
@@ -0,0 +1,337 @@
+/*
+ * Copyright (c) 2014 - 2015 Seppo Tomperi <seppo.tomperi@vtt.fi>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "libavutil/arm/asm.S"
+#include "neon.S"
+
+#define MAX_PB_SIZE #64
+
+.macro vextin_d4
+    vld1.8    {q10}, [r1], r2
+    vmov      d16, d20
+    vext.8    d17, d20, d21, #1
+    vext.8    d18, d20, d21, #2
+    vext.8    d19, d20, d21, #3
+.endm
+
+.macro vextin_d4_8
+    vld1.8    d16, [r1], r2
+    vext.8    d17, d16, d16, #1
+    vext.8    d18, d16, d16, #2
+    vext.8    d19, d16, d16, #3
+.endm
+
+.macro load_coeffs_16b coeffs
+    ldr      \coeffs, [\coeffs]
+    vdup.i8  d0, \coeffs
+    lsr      \coeffs, #8
+    vdup.i8  d1, \coeffs
+    lsr      \coeffs, #8
+    vdup.i8  d2, \coeffs
+    lsr      \coeffs, #8
+    vdup.i8  d3, \coeffs
+.endm
+
+.macro epel_filter_16b out=q12
+    vmull.u8 q3, d16, d0
+    vmull.u8 q11, d19, d3
+    vmull.u8 \out, d17, d1
+    vmull.u8 q10, d18, d2
+    vadd.s16 q3, q11
+    vadd.s16 \out, q10
+    vsub.s16 \out, q3
+.endm
+
+.macro load_coeffs_32b coeffs
+    ldr      \coeffs, [\coeffs]
+    vmov.i64 d4, #0
+    vmov.8   d4[0], \coeffs
+    lsr      \coeffs, #8
+    vmov.8   d4[2], \coeffs
+    lsr      \coeffs, #8
+    vmov.8   d4[4], \coeffs
+    lsr      \coeffs, #8
+    vmov.8   d4[6], \coeffs
+.endm
+
+.macro epel_filter_32b
+    vmull.s16 q3, d24, d4[0] //q12
+    vmull.s16 q4, d25, d4[0]
+    vmull.s16 q5, d30, d4[3] //q15
+    vmull.s16 q6, d31, d4[3]
+
+    vmull.s16 q7, d26, d4[1] // q13
+    vmull.s16 q8, d27, d4[1]
+    vmull.s16 q9, d28, d4[2] // q14
+    vmull.s16 q10, d29, d4[2]
+    vadd.s32 q3, q5
+    vadd.s32 q4, q6
+    vadd.s32 q7, q9
+    vadd.s32 q8, q10
+    vsub.s32 q7, q3
+    vsub.s32 q8, q4
+    vqshrn.s32  d6, q7, #6
+    vqshrn.s32  d7, q8, #6
+.endm
+
+.macro epel_filter_32b_4
+    vmull.s16 q3, d24, d4[0] //q12
+    vmull.s16 q5, d30, d4[3] //q15
+    vmull.s16 q7, d26, d4[1] // q13
+    vmull.s16 q9, d28, d4[2] // q14
+    vadd.s32 q3, q5
+    vadd.s32 q7, q9
+    vsub.s32 q7, q3
+    vqshrn.s32  d6, q7, #6
+.endm
+
+function ff_hevc_put_epel_h_neon_8, export=1
+        push   {r4-r7}
+        mov    r4, MAX_PB_SIZE
+        ldr    r7, [sp, #16] // mx
+        ldr    r5, [sp, #24] // width
+        sub    r7, #1
+        lsl    r7, #2
+        vpush {d8-d15}
+@ adr reaches if we are in thumb mode but not in arm
+T       adr    r12, epel_coeffs
+A       adrl   r12, epel_coeffs
+        add    r7, r12
+        sub       r1, #1
+        lsl       r4, #1
+        load_coeffs_16b r7
+        mov   r12, r3
+        mov   r6, r0
+        mov   r7, r1
+        cmp       r5, #6
+        bgt       8f
+        cmp       r5, #4
+        blt       2f
+        b         4f
+8:      subs r3, #1
+        pld [r1]
+        vextin_d4
+        epel_filter_16b
+        vst1.16    {q12}, [r0], r4
+        bne 8b
+        subs    r5, #8
+        beq  99f
+        mov       r3, r12
+        add       r6, #16
+        mov       r0, r6
+        add       r7, #8
+        mov       r1, r7
+        cmp       r5, #4
+        bgt       8b
+4:      subs r3, #1
+        pld [r1]
+        vextin_d4_8
+        epel_filter_16b
+        vst1.16    d24, [r0], r4
+        bne 4b
+        subs      r5, #4
+        beq       99f
+        mov       r3, r12
+        add       r6, #8
+        mov       r0, r6
+        add       r7, #4
+        mov       r1, r7
+2:      subs r3, #1
+        pld [r1]
+        vextin_d4_8
+        epel_filter_16b
+        vst1.32    d24[0], [r0], r4
+        bne 2b
+99:     vpop {d8-d15}
+        pop {r4-r7}
+        bx lr
+endfunc
+
+function ff_hevc_put_epel_v_neon_8, export=1
+        push   {r4-r7}
+        mov    r4, MAX_PB_SIZE
+        ldr    r7, [sp, #20] // my
+        ldr    r5, [sp, #24] // width
+        sub    r7, #1
+        lsl    r7, #2
+        vpush {d8-d15}
+T       adr    r12, epel_coeffs
+A       adrl   r12, epel_coeffs
+        add    r7, r12
+        load_coeffs_16b r7
+        sub       r1, r2
+        lsl       r4, #1
+        mov   r12, r3
+        mov   r6, r0
+        mov   r7, r1
+0:      pld [r1]
+        vld1.8    {d16}, [r1], r2
+        pld [r1]
+        vld1.8    {d17}, [r1], r2
+        pld [r1]
+        vld1.8    {d18}, [r1], r2
+        cmp       r5, #6
+        bgt       8f
+        cmp       r5, #4
+        blt       2f
+        b         4f
+8:      pld [r1]
+        vld1.8    {d19}, [r1], r2
+        subs r3, #1
+        epel_filter_16b
+        vst1.16    {q12}, [r0], r4
+        vmov d16, d17
+        vmov d17, d18
+        vmov d18, d19
+        bne 8b
+        subs    r5, #8
+        beq  99f
+        mov       r3, r12
+        add       r6, #16
+        mov       r0, r6
+        add       r7, #8
+        mov       r1, r7
+        b         0b
+4:      pld       [r1]
+        vld1.8    {d19}, [r1], r2
+        subs r3, #1
+        epel_filter_16b
+        vst1.16    d24, [r0], r4
+        vmov d16, d17
+        vmov d17, d18
+        vmov d18, d19
+        bne 4b
+        subs      r5, #4
+        beq       99f
+        mov       r3, r12
+        add       r6, #8
+        mov       r0, r6
+        add       r7, #4
+        mov       r1, r7
+        b         0b
+2:      pld [r1]
+        vld1.8    {d19}, [r1], r2
+        subs r3, #1
+        epel_filter_16b
+        vst1.32    d24[0], [r0], r4
+        vmov d16, d17
+        vmov d17, d18
+        vmov d18, d19
+        bne 2b
+99:     vpop {d8-d15}
+        pop {r4-r7}
+        bx lr
+endfunc
+
+function ff_hevc_put_epel_hv_neon_8, export=1
+        push   {r4-r7}
+        mov    r4, MAX_PB_SIZE
+        ldr    r6, [sp, #16] // mx
+        ldr    r7, [sp, #20] // my
+        ldr    r5, [sp, #24] // width
+        sub    r7, #1
+        lsl    r7, #2
+        vpush {d8-d15}
+        adr    r12, epel_coeffs
+        sub    r6, #1
+        lsl    r6, #2
+        add    r6, r12 // mx epel coeff offset
+        add    r7, r12
+        sub       r1, #1
+        sub       r1, r2
+        lsl       r4, #1
+        load_coeffs_16b r6
+        load_coeffs_32b r7
+        mov   r12, r3
+        mov   r6, r0
+        mov   r7, r1
+0:      pld   [r1]
+        vextin_d4
+        epel_filter_16b q12
+        pld   [r1]
+        vextin_d4
+        epel_filter_16b q13
+        pld   [r1]
+        vextin_d4
+        epel_filter_16b q14
+        cmp       r5, #6
+        bgt       8f
+        cmp       r5, #4
+        blt       2f
+        b         4f
+8:      pld     [r1]
+        vextin_d4
+        epel_filter_16b q15
+        subs r3, #1
+        epel_filter_32b
+        vst1.16    {q3}, [r0], r4
+        vmov q12, q13
+        vmov q13, q14
+        vmov q14, q15
+        bne 8b
+        subs    r5, #8
+        beq  99f
+        mov       r3, r12
+        add       r6, #16
+        mov       r0, r6
+        add       r7, #8
+        mov       r1, r7
+        b         0b
+4:      pld      [r1]
+        vextin_d4_8
+        epel_filter_16b q15
+        subs r3, #1
+        epel_filter_32b_4
+        vst1.16    d6, [r0], r4
+        vmov q12, q13
+        vmov q13, q14
+        vmov q14, q15
+        bne 4b
+        subs      r5, #4
+        beq       99f
+        mov       r3, r12
+        add       r6, #8
+        mov       r0, r6
+        add       r7, #4
+        mov       r1, r7
+        b         0b
+2:      pld      [r1]
+        vextin_d4_8
+        epel_filter_16b q15
+        subs r3, #1
+        epel_filter_32b_4
+        vst1.32    d6[0], [r0], r4
+        vmov q12, q13
+        vmov q13, q14
+        vmov q14, q15
+        bne 2b
+99:     vpop {d8-d15}
+        pop {r4-r7}
+        bx lr
+endfunc
+
+epel_coeffs:
+       .byte 2, 58, 10, 2
+       .byte 4, 54, 16, 2
+       .byte 6, 46, 28, 4
+       .byte 4, 36, 36, 4
+       .byte 4, 28, 46, 6
+       .byte 2, 16, 54, 4
+       .byte 2, 10, 58, 2
diff -Naur ffmpeg-3.2.4/libavcodec/arm/hevcdsp_init_neon.c ffmpeg-3.2.4.patch/libavcodec/arm/hevcdsp_init_neon.c
--- ffmpeg-3.2.4/libavcodec/arm/hevcdsp_init_neon.c	2016-03-29 04:25:10.000000000 +0200
+++ ffmpeg-3.2.4.patch/libavcodec/arm/hevcdsp_init_neon.c	2017-05-28 20:42:45.720088602 +0200
@@ -22,11 +22,26 @@
 #include "libavutil/arm/cpu.h"
 #include "libavcodec/hevcdsp.h"
 #include "hevcdsp_arm.h"
+#include "libavcodec/avcodec.h"
+#include "libavcodec/bit_depth_template.c"
 
 void ff_hevc_v_loop_filter_luma_neon(uint8_t *_pix, ptrdiff_t _stride, int _beta, int *_tc, uint8_t *_no_p, uint8_t *_no_q);
 void ff_hevc_h_loop_filter_luma_neon(uint8_t *_pix, ptrdiff_t _stride, int _beta, int *_tc, uint8_t *_no_p, uint8_t *_no_q);
 void ff_hevc_v_loop_filter_chroma_neon(uint8_t *_pix, ptrdiff_t _stride, int *_tc, uint8_t *_no_p, uint8_t *_no_q);
 void ff_hevc_h_loop_filter_chroma_neon(uint8_t *_pix, ptrdiff_t _stride, int *_tc, uint8_t *_no_p, uint8_t *_no_q);
+
+#ifdef RPI
+void ff_hevc_v_loop_filter_luma2_neon_8(uint8_t * _pix_r,
+                             unsigned int _stride, unsigned int beta, const int32_t tc[2],
+                             const uint8_t no_p[2], const uint8_t no_q[2],
+                             uint8_t * _pix_l);
+void ff_hevc_h_loop_filter_uv_neon_8(uint8_t * src, unsigned int stride, uint32_t tc4,
+                             unsigned int no_f);
+void ff_hevc_v_loop_filter_uv2_neon_8(uint8_t * src_r, unsigned int stride, uint32_t tc4,
+                             uint8_t * src_l,
+                             unsigned int no_f);
+#endif
+
 void ff_hevc_transform_4x4_neon_8(int16_t *coeffs, int col_limit);
 void ff_hevc_transform_8x8_neon_8(int16_t *coeffs, int col_limit);
 void ff_hevc_idct_4x4_dc_neon_8(int16_t *coeffs);
@@ -43,6 +58,31 @@
 void ff_hevc_transform_add_32x32_neon_8(uint8_t *_dst, int16_t *coeffs,
                                       ptrdiff_t stride);
 
+void ff_hevc_sao_band_w8_neon_8(uint8_t *_dst, uint8_t *_src, int8_t * offset_table, ptrdiff_t stride_src, ptrdiff_t stride_dst, int height);
+void ff_hevc_sao_band_w16_neon_8(uint8_t *_dst, uint8_t *_src, int8_t * offset_table, ptrdiff_t stride_src, ptrdiff_t stride_dst, int height);
+void ff_hevc_sao_band_w32_neon_8(uint8_t *_dst, uint8_t *_src, int8_t * offset_table, ptrdiff_t stride_src, ptrdiff_t stride_dst, int height);
+void ff_hevc_sao_band_w64_neon_8(uint8_t *_dst, uint8_t *_src, int8_t * offset_table, ptrdiff_t stride_src, ptrdiff_t stride_dst, int height);
+
+void ff_hevc_sao_edge_eo0_w32_neon_8(uint8_t *_dst, uint8_t *_src, ptrdiff_t stride_dst, ptrdiff_t stride_src, int height, int8_t *sao_offset_table);
+void ff_hevc_sao_edge_eo1_w32_neon_8(uint8_t *_dst, uint8_t *_src, ptrdiff_t stride_dst, ptrdiff_t stride_src, int height, int8_t *sao_offset_table);
+void ff_hevc_sao_edge_eo2_w32_neon_8(uint8_t *_dst, uint8_t *_src, ptrdiff_t stride_dst, ptrdiff_t stride_src, int height, int8_t *sao_offset_table);
+void ff_hevc_sao_edge_eo3_w32_neon_8(uint8_t *_dst, uint8_t *_src, ptrdiff_t stride_dst, ptrdiff_t stride_src, int height, int8_t *sao_offset_table);
+
+void ff_hevc_sao_edge_eo0_w64_neon_8(uint8_t *_dst, uint8_t *_src, ptrdiff_t stride_dst, ptrdiff_t stride_src, int height, int8_t *sao_offset_table);
+void ff_hevc_sao_edge_eo1_w64_neon_8(uint8_t *_dst, uint8_t *_src, ptrdiff_t stride_dst, ptrdiff_t stride_src, int height, int8_t *sao_offset_table);
+void ff_hevc_sao_edge_eo2_w64_neon_8(uint8_t *_dst, uint8_t *_src, ptrdiff_t stride_dst, ptrdiff_t stride_src, int height, int8_t *sao_offset_table);
+void ff_hevc_sao_edge_eo3_w64_neon_8(uint8_t *_dst, uint8_t *_src, ptrdiff_t stride_dst, ptrdiff_t stride_src, int height, int8_t *sao_offset_table);
+
+void ff_hevc_sao_edge_c_w64_neon_8(uint8_t *_dst, const uint8_t *_src, ptrdiff_t stride_dst, ptrdiff_t stride_src, int height,
+                                   const int16_t *sao_offset_table_u, const int16_t *sao_offset_table_v, int eo);
+
+void ff_hevc_sao_band_c_neon_8(uint8_t *_dst, const uint8_t *_src,
+                                  ptrdiff_t stride_dst, ptrdiff_t stride_src,
+                                  const int16_t *sao_offset_val_u, int sao_left_class_u,
+                                  const int16_t *sao_offset_val_v, int sao_left_class_v,
+                                  int width, int height);
+
+
 #define PUT_PIXELS(name) \
     void name(int16_t *dst, uint8_t *src, \
                                 ptrdiff_t srcstride, int height, \
@@ -58,6 +98,15 @@
 PUT_PIXELS(ff_hevc_put_pixels_w48_neon_8);
 PUT_PIXELS(ff_hevc_put_pixels_w64_neon_8);
 #undef PUT_PIXELS
+void ff_hevc_put_epel_h_neon_8(int16_t *dst, uint8_t *src,
+                                ptrdiff_t srcstride, int height,
+                                intptr_t mx, intptr_t my, int width);
+void ff_hevc_put_epel_v_neon_8(int16_t *dst, uint8_t *src,
+                                ptrdiff_t srcstride, int height,
+                                intptr_t mx, intptr_t my, int width);
+void ff_hevc_put_epel_hv_neon_8(int16_t *dst, uint8_t *src,
+                                ptrdiff_t srcstride, int height,
+                                intptr_t mx, intptr_t my, int width);
 
 static void (*put_hevc_qpel_neon[4][4])(int16_t *dst, ptrdiff_t dststride, uint8_t *src, ptrdiff_t srcstride,
                                    int height, int width);
@@ -142,14 +191,239 @@
     put_hevc_qpel_uw_neon[my][mx](dst, dststride, src, srcstride, width, height, src2, MAX_PB_SIZE);
 }
 
+static void ff_hevc_sao_band_neon_wrapper(uint8_t *_dst, uint8_t *_src, ptrdiff_t stride_dst, ptrdiff_t stride_src,
+                                          int16_t *sao_offset_val, int sao_left_class, int width, int height)
+{
+    pixel *dst = (pixel *)_dst;
+    pixel *src = (pixel *)_src;
+    int8_t offset_table[32] = { 0 };
+    int k, y, x;
+    int shift  = 3; // BIT_DEPTH - 5
+    int cwidth = 0;
+
+    stride_src /= sizeof(pixel);
+    stride_dst /= sizeof(pixel);
+
+    for (k = 0; k < 4; k++)
+        offset_table[(k + sao_left_class) & 31] = sao_offset_val[k + 1];
+
+    if (height % 8 == 0)
+        cwidth = width;
+
+    switch(cwidth){
+    case 8:
+        ff_hevc_sao_band_w8_neon_8(_dst, _src, offset_table, stride_src, stride_dst, height);
+        break;
+    case 16:
+        ff_hevc_sao_band_w16_neon_8(_dst, _src, offset_table, stride_src, stride_dst, height);
+        break;
+    case 32:
+        ff_hevc_sao_band_w32_neon_8(_dst, _src, offset_table, stride_src, stride_dst, height);
+        break;
+    case 64:
+        ff_hevc_sao_band_w64_neon_8(_dst, _src, offset_table, stride_src, stride_dst, height);
+        break;
+    default:
+        for (y = 0; y < height; y++) {
+            for (x = 0; x < width; x++)
+                dst[x] = av_clip_pixel(src[x] + offset_table[src[x] >> shift]);
+            dst += stride_dst;
+            src += stride_src;
+        }
+    }
+}
+
+static void ff_hevc_sao_band_c_neon_wrapper(uint8_t *_dst, const uint8_t *_src,
+                                  ptrdiff_t stride_dst, ptrdiff_t stride_src,
+                                  const int16_t *sao_offset_val_u, int sao_left_class_u,
+                                  const int16_t *sao_offset_val_v, int sao_left_class_v,
+                                  int width, int height)
+{
+    // Width 32 already dealt with
+    // width 16 code works in double lines
+    if (width == 16 && (height & 1) == 0) {
+        ff_hevc_sao_band_c_neon_8(_dst, _src, stride_src, stride_dst,
+                                          sao_offset_val_u, sao_left_class_u,
+                                          sao_offset_val_v, sao_left_class_v,
+                                          width, height);
+    }
+    else
+    {
+        const int shift  = 3; // BIT_DEPTH - 5
+        int k, y, x;
+        pixel *dst = (pixel *)_dst;
+        pixel *src = (pixel *)_src;
+        int8_t offset_table_u[32] = { 0 };
+        int8_t offset_table_v[32] = { 0 };
+
+        stride_src /= sizeof(pixel);
+        stride_dst /= sizeof(pixel);
+
+        for (k = 0; k < 4; k++)
+            offset_table_u[(k + sao_left_class_u) & 31] = sao_offset_val_u[k + 1];
+        for (k = 0; k < 4; k++)
+            offset_table_v[(k + sao_left_class_v) & 31] = sao_offset_val_v[k + 1];
+
+        for (y = 0; y < height; y++) {
+            for (x = 0; x < width * 2; x += 2)
+            {
+                dst[x + 0] = av_clip_pixel(src[x + 0] + offset_table_u[src[x + 0] >> shift]);
+                dst[x + 1] = av_clip_pixel(src[x + 1] + offset_table_v[src[x + 1] >> shift]);
+            }
+            dst += stride_dst;
+            src += stride_src;
+
+        }
+    }
+}
+
+#define CMP(a, b) ((a) > (b) ? 1 : ((a) == (b) ? 0 : -1))
+static void ff_hevc_sao_edge_neon_wrapper(uint8_t *_dst /* align 16 */, uint8_t *_src /* align 32 */, ptrdiff_t stride_dst,
+                                          int16_t *_sao_offset_val, int eo, int width, int height)
+{
+    static const uint8_t edge_idx[] = { 1, 2, 0, 3, 4 };
+    static const int8_t pos[4][2][2] = {
+        { { -1,  0 }, {  1, 0 } }, // horizontal
+        { {  0, -1 }, {  0, 1 } }, // vertical
+        { { -1, -1 }, {  1, 1 } }, // 45 degree
+        { {  1, -1 }, { -1, 1 } }, // 135 degree
+    };
+    int8_t sao_offset_val[8];  // padding of 3 for vld
+    ptrdiff_t stride_src = (2*MAX_PB_SIZE + FF_INPUT_BUFFER_PADDING_SIZE);
+    pixel *dst = (pixel *)_dst;
+    pixel *src = (pixel *)_src;
+    int a_stride, b_stride;
+    int x, y;
+    int cwidth = 0;
+
+    for (x = 0; x < 5; x++) {
+        sao_offset_val[x] = _sao_offset_val[edge_idx[x]];
+    }
+
+    if (height % 8 == 0)
+        cwidth = width;
+
+    stride_src /= sizeof(pixel);
+    stride_dst /= sizeof(pixel);
+
+    switch (cwidth) {
+    case 32:
+        switch(eo) {
+        case 0:
+            ff_hevc_sao_edge_eo0_w32_neon_8(dst, src, stride_dst, stride_src, height, sao_offset_val);
+            break;
+        case 1:
+            ff_hevc_sao_edge_eo1_w32_neon_8(dst, src, stride_dst, stride_src, height, sao_offset_val);
+            break;
+        case 2:
+            ff_hevc_sao_edge_eo2_w32_neon_8(dst, src, stride_dst, stride_src, height, sao_offset_val);
+            break;
+        case 3:
+            ff_hevc_sao_edge_eo3_w32_neon_8(dst, src, stride_dst, stride_src, height, sao_offset_val);
+            break;
+        }
+        break;
+    case 64:
+        switch(eo) {
+        case 0:
+            ff_hevc_sao_edge_eo0_w64_neon_8(dst, src, stride_dst, stride_src, height, sao_offset_val);
+            break;
+        case 1:
+            ff_hevc_sao_edge_eo1_w64_neon_8(dst, src, stride_dst, stride_src, height, sao_offset_val);
+            break;
+        case 2:
+            ff_hevc_sao_edge_eo2_w64_neon_8(dst, src, stride_dst, stride_src, height, sao_offset_val);
+            break;
+        case 3:
+            ff_hevc_sao_edge_eo3_w64_neon_8(dst, src, stride_dst, stride_src, height, sao_offset_val);
+            break;
+        }
+        break;
+    default:
+        a_stride = pos[eo][0][0] + pos[eo][0][1] * stride_src;
+        b_stride = pos[eo][1][0] + pos[eo][1][1] * stride_src;
+        for (y = 0; y < height; y++) {
+            for (x = 0; x < width; x++) {
+                int diff0         = CMP(src[x], src[x + a_stride]);
+                int diff1         = CMP(src[x], src[x + b_stride]);
+                int idx           = diff0 + diff1;
+                if (idx)
+                    dst[x] = av_clip_pixel(src[x] + sao_offset_val[idx+2]);
+            }
+            src += stride_src;
+            dst += stride_dst;
+        }
+    }
+}
+
+
+static void ff_hevc_sao_edge_c_neon_wrapper(uint8_t *_dst, const uint8_t *_src, ptrdiff_t stride_dst,
+                                  const int16_t *_sao_offset_val_u, const int16_t *_sao_offset_val_v,
+                                  int eo, int width, int height)
+{
+    const ptrdiff_t stride_src = (2*MAX_PB_SIZE + FF_INPUT_BUFFER_PADDING_SIZE) / sizeof(pixel);
+
+    if (width == 32 && (height & 7) == 0) {
+        ff_hevc_sao_edge_c_w64_neon_8(_dst, _src, stride_dst, stride_src, height, _sao_offset_val_u, _sao_offset_val_v, eo);
+    }
+    else
+    {
+        static const uint8_t edge_idx[] = { 1, 2, 0, 3, 4 };
+        static const int8_t pos[4][2][2] = {
+            { { -1,  0 }, {  1, 0 } }, // horizontal
+            { {  0, -1 }, {  0, 1 } }, // vertical
+            { { -1, -1 }, {  1, 1 } }, // 45 degree
+            { {  1, -1 }, { -1, 1 } }, // 135 degree
+        };
+        int8_t sao_offset_val_u[8];  // padding of 3 for vld
+        int8_t sao_offset_val_v[8];  // padding of 3 for vld
+        pixel *dst = (pixel *)_dst;
+        pixel *src = (pixel *)_src;
+        int a_stride, b_stride;
+        int x, y;
+
+        for (x = 0; x < 5; x++) {
+            sao_offset_val_u[x] = _sao_offset_val_u[edge_idx[x]];
+            sao_offset_val_v[x] = _sao_offset_val_v[edge_idx[x]];
+        }
+
+        a_stride = pos[eo][0][0] * 2 + pos[eo][0][1] * stride_src;
+        b_stride = pos[eo][1][0] * 2 + pos[eo][1][1] * stride_src;
+        for (y = 0; y < height; y++) {
+            for (x = 0; x < width * 2; x += 2) {
+                int diff0u = CMP(src[x], src[x + a_stride]);
+                int diff1u = CMP(src[x], src[x + b_stride]);
+                int diff0v = CMP(src[x+1], src[x+1 + a_stride]);
+                int diff1v = CMP(src[x+1], src[x+1 + b_stride]);
+                dst[x] = av_clip_pixel(src[x] + sao_offset_val_u[2 + diff0u + diff1u]);
+                dst[x+1] = av_clip_pixel(src[x+1] + sao_offset_val_v[2 + diff0v + diff1v]);
+            }
+            src += stride_src;
+            dst += stride_dst;
+        }
+    }
+}
+#undef CMP
+
+void ff_hevc_deblocking_boundary_strengths_neon(int pus, int dup, int in_inc, int out_inc,
+                                                int *curr_rpl0, int *curr_rpl1, int *neigh_rpl0, int *neigh_rpl1,
+                                                MvField *curr, MvField *neigh, uint8_t *bs);
+
 av_cold void ff_hevcdsp_init_neon(HEVCDSPContext *c, const int bit_depth)
 {
     if (bit_depth == 8) {
         int x;
         c->hevc_v_loop_filter_luma     = ff_hevc_v_loop_filter_luma_neon;
+        c->hevc_v_loop_filter_luma_c   = ff_hevc_v_loop_filter_luma_neon;
         c->hevc_h_loop_filter_luma     = ff_hevc_h_loop_filter_luma_neon;
+        c->hevc_h_loop_filter_luma_c   = ff_hevc_h_loop_filter_luma_neon;
         c->hevc_v_loop_filter_chroma   = ff_hevc_v_loop_filter_chroma_neon;
         c->hevc_h_loop_filter_chroma   = ff_hevc_h_loop_filter_chroma_neon;
+#ifdef RPI
+        c->hevc_v_loop_filter_luma2    = ff_hevc_v_loop_filter_luma2_neon_8;
+        c->hevc_h_loop_filter_uv       = ff_hevc_h_loop_filter_uv_neon_8;
+        c->hevc_v_loop_filter_uv2      = ff_hevc_v_loop_filter_uv2_neon_8;
+#endif
         c->idct[0]                     = ff_hevc_transform_4x4_neon_8;
         c->idct[1]                     = ff_hevc_transform_8x8_neon_8;
         c->idct_dc[0]                  = ff_hevc_idct_4x4_dc_neon_8;
@@ -161,6 +435,13 @@
         c->transform_add[2]            = ff_hevc_transform_add_16x16_neon_8;
         c->transform_add[3]            = ff_hevc_transform_add_32x32_neon_8;
         c->idct_4x4_luma               = ff_hevc_transform_luma_4x4_neon_8;
+        for (x = 0; x < sizeof c->sao_band_filter / sizeof *c->sao_band_filter; x++) {
+          c->sao_band_filter[x]        = ff_hevc_sao_band_neon_wrapper;
+          c->sao_band_filter_c[x]      = ff_hevc_sao_band_c_neon_wrapper;
+          c->sao_edge_filter[x]        = ff_hevc_sao_edge_neon_wrapper;
+          c->sao_edge_filter_c[x]      = ff_hevc_sao_edge_c_neon_wrapper;
+        }
+        c->sao_band_filter_c[2]        = ff_hevc_sao_band_c_neon_8;  // width=32
         put_hevc_qpel_neon[1][0]       = ff_hevc_put_qpel_v1_neon_8;
         put_hevc_qpel_neon[2][0]       = ff_hevc_put_qpel_v2_neon_8;
         put_hevc_qpel_neon[3][0]       = ff_hevc_put_qpel_v3_neon_8;
@@ -201,7 +482,21 @@
             c->put_hevc_qpel_bi[x][1][0]      = ff_hevc_put_qpel_bi_neon_wrapper;
             c->put_hevc_qpel_bi[x][0][1]      = ff_hevc_put_qpel_bi_neon_wrapper;
             c->put_hevc_qpel_bi[x][1][1]      = ff_hevc_put_qpel_bi_neon_wrapper;
+            c->put_hevc_epel[x][1][0]         = ff_hevc_put_epel_v_neon_8;
+            c->put_hevc_epel[x][0][1]         = ff_hevc_put_epel_h_neon_8;
+            c->put_hevc_epel[x][1][1]         = ff_hevc_put_epel_hv_neon_8;
         }
+        c->put_hevc_epel[0][0][0]  = ff_hevc_put_pixels_w2_neon_8;
+        c->put_hevc_epel[1][0][0]  = ff_hevc_put_pixels_w4_neon_8;
+        c->put_hevc_epel[2][0][0]  = ff_hevc_put_pixels_w6_neon_8;
+        c->put_hevc_epel[3][0][0]  = ff_hevc_put_pixels_w8_neon_8;
+        c->put_hevc_epel[4][0][0]  = ff_hevc_put_pixels_w12_neon_8;
+        c->put_hevc_epel[5][0][0]  = ff_hevc_put_pixels_w16_neon_8;
+        c->put_hevc_epel[6][0][0]  = ff_hevc_put_pixels_w24_neon_8;
+        c->put_hevc_epel[7][0][0]  = ff_hevc_put_pixels_w32_neon_8;
+        c->put_hevc_epel[8][0][0]  = ff_hevc_put_pixels_w48_neon_8;
+        c->put_hevc_epel[9][0][0]  = ff_hevc_put_pixels_w64_neon_8;
+
         c->put_hevc_qpel[0][0][0]  = ff_hevc_put_pixels_w2_neon_8;
         c->put_hevc_qpel[1][0][0]  = ff_hevc_put_pixels_w4_neon_8;
         c->put_hevc_qpel[2][0][0]  = ff_hevc_put_pixels_w6_neon_8;
@@ -221,4 +516,9 @@
         c->put_hevc_qpel_uni[8][0][0]  = ff_hevc_put_qpel_uw_pixels_w48_neon_8;
         c->put_hevc_qpel_uni[9][0][0]  = ff_hevc_put_qpel_uw_pixels_w64_neon_8;
     }
+
+    assert(offsetof(MvField, mv) == 0);
+    assert(offsetof(MvField, ref_idx) == 8);
+    assert(offsetof(MvField, pred_flag) == 10);
+    c->hevc_deblocking_boundary_strengths = ff_hevc_deblocking_boundary_strengths_neon;
 }
diff -Naur ffmpeg-3.2.4/libavcodec/arm/hevcdsp_sao_neon.S ffmpeg-3.2.4.patch/libavcodec/arm/hevcdsp_sao_neon.S
--- ffmpeg-3.2.4/libavcodec/arm/hevcdsp_sao_neon.S	1970-01-01 01:00:00.000000000 +0100
+++ ffmpeg-3.2.4.patch/libavcodec/arm/hevcdsp_sao_neon.S	2017-05-28 20:42:45.721088605 +0200
@@ -0,0 +1,862 @@
+/*
+ * Copyright (c) 2014 - 2015 Seppo Tomperi <seppo.tomperi@vtt.fi>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "libavutil/arm/asm.S"
+#include "neon.S"
+
+.macro init_sao_band
+        pld      [r1]
+        vld1.8   {q0, q1}, [r2]  // offset table
+        ldr       r2, [sp, #0]   // stride_dst
+        ldr      r12, [sp, #4]   // height
+        vmov.u8  q3, #128
+.endm
+
+// 128 in q3
+// input q8 - q11
+.macro sao_band_64
+        vtbl.8   d24, {d0, d1, d2, d3}, d24
+        vadd.s8  q8, q3
+        vtbl.8   d25, {d0, d1, d2, d3}, d25
+        vadd.s8  q9, q3
+        vtbl.8   d26, {d0, d1, d2, d3}, d26
+        vadd.s8  q10, q3
+        vtbl.8   d27, {d0, d1, d2, d3}, d27
+        vadd.s8  q11, q3
+        vtbl.8   d28, {d0, d1, d2, d3}, d28
+        vqadd.s8 q8, q12
+        vtbl.8   d29, {d0, d1, d2, d3}, d29
+        vqadd.s8 q9, q13
+        vtbl.8   d30, {d0, d1, d2, d3}, d30
+        vqadd.s8 q10, q14
+        vtbl.8   d31, {d0, d1, d2, d3}, d31
+        vsub.s8  q8, q3
+        vqadd.s8 q11, q15
+        vsub.s8  q9, q3
+        vsub.s8  q10, q3
+        vsub.s8  q11, q3
+.endm
+
+function ff_hevc_sao_band_w8_neon_8, export=1
+        init_sao_band
+1:      subs     r12, #8
+        vld1.8   {d16}, [r1, :64], r3
+        vld1.8   {d17}, [r1, :64], r3
+        vshr.u8  q12, q8, #3
+        vld1.8   {d18}, [r1, :64], r3
+        vld1.8   {d19}, [r1, :64], r3
+        vshr.u8  q13, q9, #3
+        vld1.8   {d20}, [r1, :64], r3
+        vld1.8   {d21}, [r1, :64], r3
+        vshr.u8  q14, q10, #3
+        vld1.8   {d22}, [r1, :64], r3
+        vld1.8   {d23}, [r1, :64], r3
+        vshr.u8  q15, q11, #3
+        sao_band_64
+        vst1.8  {d16}, [r0, :64], r2
+        vst1.8  {d17}, [r0, :64], r2
+        vst1.8  {d18}, [r0, :64], r2
+        vst1.8  {d19}, [r0, :64], r2
+        vst1.8  {d20}, [r0, :64], r2
+        vst1.8  {d21}, [r0, :64], r2
+        vst1.8  {d22}, [r0, :64], r2
+        vst1.8  {d23}, [r0, :64], r2
+        bne    1b
+
+        bx lr
+endfunc
+
+function ff_hevc_sao_band_w16_neon_8, export=1
+        init_sao_band
+1:      subs     r12, #4
+        vld1.8  {q8}, [r1, :128], r3
+        vshr.u8  q12, q8, #3
+        vld1.8  {q9}, [r1, :128], r3
+        vshr.u8  q13, q9, #3
+        vld1.8  {q10}, [r1, :128], r3
+        vshr.u8  q14, q10, #3
+        vld1.8  {q11}, [r1, :128], r3
+        vshr.u8  q15, q11, #3
+        sao_band_64
+        vst1.8   {q8}, [r0, :128], r2
+        vst1.8   {q9}, [r0, :128], r2
+        vst1.8   {q10}, [r0, :128], r2
+        vst1.8   {q11}, [r0, :128], r2
+        bne    1b
+
+        bx lr
+endfunc
+
+function ff_hevc_sao_band_w32_neon_8, export=1
+        init_sao_band
+1:      subs     r12, #2
+        vld1.8   {q8-q9}, [r1, :128], r3
+        vshr.u8  q12, q8, #3
+        vshr.u8  q13, q9, #3
+        vld1.8   {q10-q11}, [r1, :128], r3
+        vshr.u8  q14, q10, #3
+        vshr.u8  q15, q11, #3
+        sao_band_64
+        vst1.8   {q8-q9}, [r0, :128], r2
+        vst1.8   {q10-q11}, [r0, :128], r2
+        bne      1b
+
+        bx       lr
+endfunc
+
+function ff_hevc_sao_band_w64_neon_8, export=1
+        init_sao_band
+
+        push      {r4, lr}
+        subs      r12, #1
+        mov       r4, r1
+        it ne
+        addne     r4, r3
+
+1:      subs      r12, #1
+        vldm      r1, {q8-q11}
+        pld       [r4]
+        vshr.u8   q12, q8, #3
+        vshr.u8   q13, q9, #3
+        add       r1, r3
+        vshr.u8   q14, q10, #3
+        vshr.u8   q15, q11, #3
+        sao_band_64
+        it ne
+        addne     r4, r3
+        vstm      r0, {q8-q11}
+        add       r0, r2
+        bpl       1b
+
+        pop       {r4, pc}
+endfunc
+
+
+@ ff_hevc_sao_band_c_w64_neon_8(
+@   uint8_t * dst          [r0]
+@   uint8_t * src          [r1]
+@   uint32_t dst_stride    [r2]
+@   uint32_t src_stride    [r3]
+@   const int16_t * table1 sp[0]
+@   uint32_t offset1       sp[4]
+@   const int16_t * table2 sp[8]
+@   uint32_t offset2       sp[12]
+@   int width              sp[16]
+@   int height             sp[20]
+
+@ As this is often done in-place on the frame buffer it is worth preloading
+@ the pixel values but we want to beware of loading ouside our buffer to avoid
+@ loading stuff into the cache that should still be invalid (in use by QPU, VPU)
+
+function ff_hevc_sao_band_c_neon_8, export=1
+        mov     r12, sp
+        push   {r4-r8, lr}  // 24 bytes
+
+        ldm     r12, {r4-r7}
+
+        add     r4, #2
+        add     r6, #2
+        vld1.16 {d16}, [r4]    @ Unaligned
+        lsl     r5, r5, #3
+        vld1.16 {d18}, [r6]
+        pld     [r1]
+        vmov.i8  d17, #0
+        mov     r4, r1
+        vmov.i8  d19, #0
+        lsl     r7, r7, #3
+        vdup.8  q1, r5
+        ldr     r5, [r12, #16]  @ width
+        vdup.8  q2, r7
+        ldr     r12, [r12, #20]
+        vqmovn.s16 d0, q8
+        cmp     r5, #16         @ At some point we may want a table lookup
+        vqmovn.s16 d1, q9
+        vmov.i8 q3, #128
+        beq     16f
+
+        @ d0 U lookup
+        @ d1 V lookup
+        @ q1 U raw offset
+        @ q2 V raw offset
+        @ q3 #128
+
+        @ r4 = r1 = src - Inteded for preload pointer
+        @ r12 = height
+
+        @ Might (unlikely) be called with height == 1
+        subs      r12, #1
+        it ne
+        addne     r4, r3
+
+1:
+        subs      r12, #1
+        vld2.8    {q8-q9}, [r1, :128]!
+        vsub.u8   q12, q8, q1
+        vld2.8    {q10-q11}, [r1, :128], r3
+        vsub.u8   q14, q10, q1
+        vsub.u8   q13, q9, q2
+        sub       r1, #32
+        vsub.u8   q15, q11, q2
+        pld       [r4]
+        vshr.u8   q12, #3
+        vadd.s8   q8, q3
+        vshr.u8   q13, #3
+        vadd.s8   q9, q3
+
+        vtbl.8   d24, {d0}, d24
+        vshr.u8  q14, #3
+        vtbl.8   d25, {d0}, d25
+        vshr.u8  q15, #3
+        vtbl.8   d26, {d1}, d26
+        vadd.s8  q10, q3
+        vtbl.8   d27, {d1}, d27
+        vadd.s8  q11, q3
+        vtbl.8   d28, {d0}, d28
+        vqadd.s8 q8, q12
+        vtbl.8   d29, {d0}, d29
+        vqadd.s8 q9, q13
+        vtbl.8   d30, {d1}, d30
+        vqadd.s8 q10, q14
+        vtbl.8   d31, {d1}, d31
+        vsub.s8  q8, q3
+        vqadd.s8 q11, q15
+        vsub.s8  q9, q3
+        vsub.s8  q10, q3
+        vsub.s8  q11, q3
+
+        it ne
+        addne     r4, r3        @ Do not inc on final pass
+        vst2.8    {q8-q9}, [r0, :128]!
+        vst2.8    {q10-q11}, [r0, :128], r2
+        sub       r0, #32
+        bpl       1b
+
+        pop    {r4-r8, pc}
+
+@ -- width 16 (UV pairs) --
+16:
+        subs    r12, #2
+        it ne
+        addne   r4, r4, r3, lsl #1
+
+1:
+        subs      r12, #2
+        vld2.8    {q8-q9}, [r1, :128], r3
+        vsub.u8   q12, q8, q1
+        vld2.8    {q10-q11}, [r1, :128], r3
+        vsub.u8   q14, q10, q1
+        vsub.u8   q13, q9, q2
+        pld       [r4]
+        vsub.u8   q15, q11, q2
+        pld       [r4, r3]
+        vshr.u8  q12, #3
+        vadd.s8  q8, q3
+        vshr.u8  q13, #3
+        vadd.s8  q9, q3
+
+        vtbl.8   d24, {d0}, d24
+        vshr.u8  q14, #3
+        vtbl.8   d25, {d0}, d25
+        vshr.u8  q15, #3
+        vtbl.8   d26, {d1}, d26
+        vadd.s8  q10, q3
+        vtbl.8   d27, {d1}, d27
+        vadd.s8  q11, q3
+        vtbl.8   d28, {d0}, d28
+        vqadd.s8 q8, q12
+        vtbl.8   d29, {d0}, d29
+        vqadd.s8 q9, q13
+        vtbl.8   d30, {d1}, d30
+        vqadd.s8 q10, q14
+        vtbl.8   d31, {d1}, d31
+        vsub.s8  q8, q3
+        vqadd.s8 q11, q15
+        vsub.s8  q9, q3
+        vsub.s8  q10, q3
+        vsub.s8  q11, q3
+
+        it ne
+        addne   r4, r4, r3, lsl #1
+        vst2.8    {q8-q9}, [r0, :128], r2
+        vst2.8    {q10-q11}, [r0, :128], r2
+        bpl       1b
+
+        pop    {r4-r8, pc}
+
+endfunc
+
+
+.macro diff32 out0, out1, tmp0, tmp1, in0, in1, in2, in3
+        vcgt.u8 \out0, \in2, \in0  // c > a -> -1 , otherwise 0
+        vcgt.u8 \tmp0,  \in0, \in2  // a > c -> -1 , otherwise 0
+        vcgt.u8 \out1, \in3, \in1  // c > a -> -1 , otherwise 0 part 2
+        vcgt.u8 \tmp1,  \in1, \in3  // a > c -> -1 , otherwise 0 part 2
+        vsub.s8 \out0, \tmp0, \out0 // diff0
+        vsub.s8 \out1, \tmp1, \out1 // diff0 part 2
+.endm
+
+
+// input
+// a in q0 - q3
+// c in q4 - q7
+// b in q8 - q11
+// offset table r4,r5 and r6,r7
+//   r4,r5 applied to even samples; r6 r7 applied to odd - allows filtering of C
+// output in q0 - q3
+// clobbers q12 - q15
+
+@ a <- c <- b
+@
+@ It appears that Neon can stall if you try and use results too soon so we try to
+@ spread our instruction out
+
+.macro edgeidx64
+
+        vcgt.u8 q12, q4, q0  // c > a -> -1 , otherwise 0
+        vcgt.u8 q13, q5, q1
+        vcgt.u8 q14, q6, q2
+        vcgt.u8 q15, q7, q3
+
+        vcgt.u8 q0, q0, q4  // a > c -> -1 , otherwise 0
+        vcgt.u8 q1, q1, q5
+        vcgt.u8 q2, q2, q6
+        vcgt.u8 q3, q3, q7
+
+        vsub.s8 q0, q0, q12 // a = sign(c-a)
+        vsub.s8 q1, q1, q13
+        vsub.s8 q2, q2, q14
+        vsub.s8 q3, q3, q15
+
+        vcgt.u8 q12, q4, q8  // c > b -> -1 , otherwise 0
+        vcgt.u8 q13, q5, q9
+        vcgt.u8 q14, q6, q10
+        vcgt.u8 q15, q7, q11
+
+        vsub.s8 q0, q0, q12
+        vsub.s8 q1, q1, q13
+        vsub.s8 q2, q2, q14
+        vsub.s8 q3, q3, q15
+
+        vcgt.u8 q12, q8, q4  // c < b -> -1 , otherwise 0
+        vcgt.u8 q13, q9, q5
+        vcgt.u8 q14, q10, q6
+        vcgt.u8 q15, q11, q7
+
+        vadd.s8 q0, q0, q12  // a = sign(c-a) + sign(c-b)
+        vadd.s8 q1, q1, q13
+        vmov.u8 q12, #2
+        vadd.s8 q2, q2, q14
+        vadd.s8 q3, q3, q15
+
+        vadd.s8 q0, q0, q12
+        vadd.s8 q1, q1, q12
+        @ whilst vmov dn, rm, rn exists it is a vfp instruction
+        @ and causes a stall till neon pipe empty - so don't do that!
+        vmov    d26[0], r4
+        vmov    d26[1], r5
+        vmov    d27[0], r6
+        vmov    d27[1], r7
+        vadd.s8 q2, q2, q12
+        vuzp.8    q0, q1
+        vmov.u8 q15, #128
+        vadd.s8 q3, q3, q12 // a = 2 + sign(c-a) + sign(c-b)
+
+        vtbl.8  d0, {d26}, d0
+        vadd.s8 q12, q4, q15  // Add -128 so we can use saturating signed add
+
+        vtbl.8  d1, {d26}, d1
+        vadd.s8 q14, q5, q15
+
+        vtbl.8  d2, {d27}, d2
+        vuzp.8    q2, q3
+
+        vtbl.8  d3, {d27}, d3
+
+        vtbl.8  d4, {d26}, d4
+        vzip.8    q0, q1
+
+        vtbl.8  d5, {d26}, d5
+        vqadd.s8 q0, q0, q12
+        vqadd.s8 q1, q1, q14
+        vadd.s8 q12, q6, q15  // Add -128 so we can use saturating signed add
+
+        vtbl.8  d6, {d27}, d6
+        vadd.s8 q14, q7, q15  // Add -128 so we can use saturating signed add
+
+        vtbl.8  d7, {d27}, d7
+        vzip.8   q2, q3
+
+        vsub.s8 q0, q0, q15
+        vqadd.s8 q2, q2, q12
+        vqadd.s8 q3, q3, q14
+        vsub.s8 q1, q1, q15
+        vsub.s8 q2, q2, q15
+        vsub.s8 q3, q3, q15
+
+.endm
+
+function edge_w64_body
+        edgeidx64
+        vstm    r0, {q0-q3}
+        add     r0, r0, r2
+        bx       lr
+endfunc
+
+.macro init_edge_64
+        push   {r4-r8,lr}
+        ldr    r12, [sp, #24] // height
+        ldr    r5,  [sp, #28] // sao_offset_val_table
+        ldrd   r4, r5, [r5]
+        mov    r6, r4
+        mov    r7, r5
+.endm
+
+function ff_hevc_sao_edge_eo0_w64_neon_8, export=1
+        init_edge_64
+        vpush {d8-d15}
+        sub    r1, #8
+1:      subs    r12, #1
+        vld1.64  {d7}, [r1, :64]!
+        vld1.64  {q4-q5}, [r1, :128]! // load c
+        vld1.64  {q6-q7}, [r1, :128]!
+        vld1.64  {d24}, [r1, :64], r3
+        sub      r1, #72
+        // load a
+        vext.8 q0, q3, q4, #15
+        vext.8 q1, q4, q5, #15
+        vext.8 q2, q5, q6, #15
+        vext.8 q3, q6, q7, #15
+        // load b
+        vext.8 q8, q4, q5, #1
+        vext.8 q9, q5, q6, #1
+        vext.8 q10, q6, q7, #1
+        vext.8 q11, q7, q12, #1
+        bl    edge_w64_body
+        bne   1b
+        vpop  {d8-d15}
+        pop   {r4-r8,pc}
+endfunc
+
+function ff_hevc_sao_edge_eo1_w64_neon_8, export=1
+        init_edge_64
+        vpush {d8-d15}
+        sub     r1, r3
+        // load a
+        vld1.8  {q0-q1}, [r1, :128]!
+        vld1.8  {q2-q3}, [r1, :128], r3
+        sub     r1, #32
+        // load c
+        vld1.8  {q4-q5}, [r1, :128]!
+        vld1.8  {q6-q7}, [r1, :128], r3
+        sub     r1, #32
+1:      subs    r12, #1
+        // load b
+        vld1.8  {q8-q9}, [r1, :128]!
+        vld1.8  {q10-q11}, [r1, :128], r3
+        sub     r1, #32
+        bl      edge_w64_body
+        // copy c to a
+        vmov.64 q0, q4
+        vmov.64 q1, q5
+        vmov.64 q2, q6
+        vmov.64 q3, q7
+        // copy b to c
+        vmov.64 q4, q8
+        vmov.64 q5, q9
+        vmov.64 q6, q10
+        vmov.64 q7, q11
+        bne   1b
+        vpop  {d8-d15}
+        pop   {r4-r8,pc}
+endfunc
+
+function ff_hevc_sao_edge_eo2_w64_neon_8, export=1
+        init_edge_64
+        vpush {d8-d15}
+1:      sub     r1, r3
+        // load a
+        // TODO: fix unaligned load
+        //       don't reload a like in eo1
+        sub     r1, #1
+        vld1.8  {q0-q1}, [r1]!
+        vld1.8  {q2-q3}, [r1], r3
+        sub     r1, #31
+        subs    r12, #1
+        // load c
+        vld1.8  {q4-q5}, [r1, :128]!
+        vld1.8  {q6-q7}, [r1, :128], r3
+        sub     r1, #32
+        // load b
+        add     r1, #1
+        vld1.8  {q8-q9}, [r1]!
+        vld1.8  {q10-q11}, [r1]
+        sub     r1, #33
+        bl      edge_w64_body
+        bne   1b
+        vpop  {d8-d15}
+        pop   {r4-r8,pc}
+endfunc
+
+function ff_hevc_sao_edge_eo3_w64_neon_8, export=1
+        init_edge_64
+        vpush {d8-d15}
+1:      sub     r1, r3
+        // load a
+        // TODO: fix unaligned load
+        //       don't reload a like in eo1
+        add     r1, #1
+        vld1.8  {q0-q1}, [r1]!
+        vld1.8  {q2-q3}, [r1], r3
+        sub     r1, #33
+        subs    r12, #1
+        // load c
+        vld1.8  {q4-q5}, [r1, :128]!
+        vld1.8  {q6-q7}, [r1, :128], r3
+        sub     r1, #32
+        // load b
+        sub     r1, #1
+        vld1.8  {q8-q9}, [r1]!
+        vld1.8  {q10-q11}, [r1]
+        sub     r1, #31
+        bl      edge_w64_body
+        bne   1b
+        vpop  {d8-d15}
+        pop   {r4-r8,pc}
+endfunc
+
+
+@ void ff_hevc_sao_edge_c_eo1_w64_neon_8(
+@   uint8_t *_dst,               r0
+@   uint8_t *_src,               r1
+@   ptrdiff_t stride_dst,        r2
+@   ptrdiff_t stride_src,        r3
+@   int height,                  sp[0]
+@   int16_t *sao_offset_table_u,  sp[4]
+@   int16_t *sao_offset_table_v); sp[8]
+@   int eo                        sp[12]
+
+function ff_hevc_sao_edge_c_w64_neon_8, export=1
+        push   {r4-r8,lr}     // 6 reg = 24
+        ldr    r5,  [sp, #28] // sao_offset_val_table_u
+        ldr    r7,  [sp, #32] // sao_offset_val_table_v
+
+        @ Load and rearrange offsets
+        @ Also "convert" from 16bit to 8bit
+        ldrb    r4, [r5, #2]
+        ldrb    r8, [r5, #4]
+        ldrb    r6, [r7, #2]
+        ldrb    r12, [r7, #4]
+        orr     r4, r4, r8, lsl #8
+        orr     r6, r6, r12, lsl #8
+        ldrb    r8, [r5, #6]
+        ldrb    r12, [r7, #6]
+        orr     r4, r4, r8, lsl #24
+        orr     r6, r6, r12, lsl #24
+        ldrb    r5, [r5, #8]
+        ldrb    r7, [r7, #8]
+
+        ldr     r12, [sp, #36] // e0
+        adr     r8, edge_c_tbl_w64
+        ldr     r8, [r8, r12, lsl #2]
+
+        ldr     r12, [sp, #24] // height
+        vpush   {d8-d15}
+        mov     pc, r8
+
+edge_c_tbl_w64:
+        .word   ff_hevc_sao_edge_c_eo0_w64_neon_8
+        .word   ff_hevc_sao_edge_c_eo1_w64_neon_8
+        .word   ff_hevc_sao_edge_c_eo2_w64_neon_8
+        .word   ff_hevc_sao_edge_c_eo3_w64_neon_8
+
+ff_hevc_sao_edge_c_eo0_w64_neon_8:
+        sub    r1, #8
+1:      subs    r12, #1
+        vld1.64  {d7}, [r1, :64]!
+        vld1.64  {q4-q5}, [r1, :128]! // load c
+        vld1.64  {q6-q7}, [r1, :128]!
+        vld1.64  {d24}, [r1, :64], r3
+        sub      r1, #72
+        // load a
+        vext.8 q0, q3, q4, #14
+        vext.8 q1, q4, q5, #14
+        vext.8 q2, q5, q6, #14
+        vext.8 q3, q6, q7, #14
+        // load b
+        vext.8 q8, q4, q5, #2
+        vext.8 q9, q5, q6, #2
+        vext.8 q10, q6, q7, #2
+        vext.8 q11, q7, q12, #2
+        bl    edge_w64_body
+        bne   1b
+        vpop  {d8-d15}
+        pop   {r4-r8,pc}
+
+ff_hevc_sao_edge_c_eo1_w64_neon_8:
+        sub     r1, r3
+        // load a
+        vldm    r1, {q0-q3}
+        add     r1, r3
+        // load c
+        vldm    r1, {q4-q7}
+        add     r1, r3
+1:      subs    r12, #1
+        // load b
+        vldm    r1, {q8-q11}
+        add     r1, r3
+        bl      edge_w64_body
+        // copy c to a
+        vmov.64 q0, q4
+        vmov.64 q1, q5
+        vmov.64 q2, q6
+        vmov.64 q3, q7
+        // copy b to c
+        vmov.64 q4, q8
+        vmov.64 q5, q9
+        vmov.64 q6, q10
+        vmov.64 q7, q11
+        bne   1b
+        vpop  {d8-d15}
+        pop   {r4-r8,pc}
+
+ff_hevc_sao_edge_c_eo2_w64_neon_8:
+1:      sub     r1, r3
+        // load a
+        // TODO: fix unaligned load
+        //       don't reload a like in eo1
+        sub     r1, #2
+        vld1.8  {q0-q1}, [r1]!
+        vld1.8  {q2-q3}, [r1], r3
+        sub     r1, #30
+        subs    r12, #1
+        // load c
+        vld1.8  {q4-q5}, [r1, :128]!
+        vld1.8  {q6-q7}, [r1, :128], r3
+        sub     r1, #32
+        // load b
+        add     r1, #2
+        vld1.8  {q8-q9}, [r1]!
+        vld1.8  {q10-q11}, [r1]
+        sub     r1, #34
+        bl      edge_w64_body
+        bne   1b
+        vpop  {d8-d15}
+        pop   {r4-r8,pc}
+
+ff_hevc_sao_edge_c_eo3_w64_neon_8:
+1:      sub     r1, r3
+        // load a
+        // TODO: fix unaligned load
+        //       don't reload a like in eo1
+        add     r1, #2
+        vld1.8  {q0-q1}, [r1]!
+        vld1.8  {q2-q3}, [r1], r3
+        sub     r1, #34
+        subs    r12, #1
+        // load c
+        vld1.8  {q4-q5}, [r1, :128]!
+        vld1.8  {q6-q7}, [r1, :128], r3
+        sub     r1, #32
+        // load b
+        sub     r1, #2
+        vld1.8  {q8-q9}, [r1]!
+        vld1.8  {q10-q11}, [r1]
+        sub     r1, #30
+        bl      edge_w64_body
+        bne   1b
+        vpop  {d8-d15}
+        pop   {r4-r8,pc}
+endfunc
+
+
+.macro init_edge_32
+        ldr     r12, [sp, #4] // sao_offset_val_table
+        vld1.32 {d31}, [r12]
+        ldr     r12, [sp] // height
+.endm
+
+.macro diff out0, tmp0, in0, in1
+        vcgt.u8 \out0, \in1, \in0  // c > a -> -1 , otherwise 0
+        vcgt.u8 \tmp0,  \in0, \in1  // a > c -> -1 , otherwise 0
+        vsub.s8 \out0, \tmp0, \out0 // diff0
+.endm
+
+.macro table32
+        vmov.s8  q10, #2
+        vadd.s8  q0, q10
+        vadd.s8  q1, q10
+        vmov.s8  q10, #128
+        vtbl.8   d0, {d31}, d0
+        vadd.s8  q11, q2, q10
+        vtbl.8   d1, {d31}, d1
+        vadd.s8  q12, q3, q10
+        vtbl.8   d2, {d31}, d2
+        vqadd.s8 q11, q0
+        vtbl.8   d3, {d31}, d3
+        vqadd.s8 q12, q1
+        vsub.s8  q0, q11, q10
+        vsub.s8  q1, q12, q10
+        vst1.8   {q0-q1}, [r0, :128], r2
+.endm
+
+function ff_hevc_sao_edge_eo0_w32_neon_8, export=1
+        init_edge_32
+        vpush {q4-q7}
+        sub     r1, #4
+1:      subs    r12, #1
+        vld1.8  {q13-q14}, [r1]!
+        vld1.32 d30, [r1], r3
+        sub     r1, #32
+        // a
+        vext.8   q0, q13, q14, #3
+        vext.8   q1, q14, q15, #3
+        vshr.u64 d24, d30, #24
+        // c
+        vext.8   q2, q13, q14, #4
+        vext.8   q3, q14, q15, #4
+        vshr.u64 d16, d30, #32
+        // diff0
+        diff32 q13, q14, q4, q5, q0, q1, q2, q3
+        diff   d18, d25, d24, d16
+        // -diff1
+        vext.s8 q0, q13, q14, #1
+        vext.s8 q1, q14, q9, #1
+
+        vsub.s8 q0, q13, q0 //diff0 + diff1
+        vsub.s8 q1, q14, q1
+        table32
+        bne     1b
+        vpop {q4-q7}
+
+        bx      lr
+endfunc
+
+function ff_hevc_sao_edge_eo1_w32_neon_8, export=1
+        init_edge_32
+        vpush {q4-q7}
+        // load a
+        sub     r1, r3
+        vld1.8  {q0-q1}, [r1, :128], r3
+        // load c
+        vld1.8  {q2-q3}, [r1, :128], r3
+        diff32 q12, q13, q0, q1, q0, q1, q2, q3 // CMP ( c, a )
+1:      subs    r12, #1
+        // load b
+        vld1.8  {q8-q9}, [r1, :128], r3
+        diff32 q4, q5, q10, q11, q8, q9, q2, q3 // CMP ( c, b )
+        vadd.s8 q0, q4, q12 //diff0 + diff1
+        vadd.s8 q1, q5, q13
+        table32
+        // CMP ( c, a )
+        vneg.s8 q12, q4
+        vneg.s8 q13, q5
+        // c
+        vmov.64 q2, q8
+        vmov.64 q3, q9
+        bne     1b
+        vpop {q4-q7}
+        bx      lr
+endfunc
+
+function ff_hevc_sao_edge_eo2_w32_neon_8, export=1
+        init_edge_32
+        vpush   {d8-d15}
+        // load a
+        sub     r1, r3
+        sub     r1, #8
+        vld1.8  {q10-q11}, [r1, :64]!
+        vld1.8  {d24}, [r1, :64], r3
+        sub     r1, #32
+        vext.8  q0, q10, q11, #7
+        vext.8  q1, q11, q12, #7
+        // load c
+        vld1.8  {d9}, [r1, :64]!
+        vld1.8  {q2-q3}, [r1, :64], r3
+        sub     r1, #8
+        vext.8  q4, q4, q2, #15
+1:      subs    r12, #1
+        // load b
+        vld1.8  {q10-q11}, [r1, :64]!
+        vld1.8  {q12}, [r1, :64], r3
+        sub     r1, #32
+        vext.8  q8, q10, q11, #9
+        vext.8  q9, q11, q12, #9
+        vext.8  q6, q10, q11, #8
+        vext.8  q7, q11, q12, #8
+        vext.8  q5, q10, q11, #7
+        diff32 q12, q13, q0, q1, q0, q1, q2, q3
+        diff32 q0, q1, q10, q11,  q8, q9, q2, q3
+        vadd.s8 q0, q12 //diff0 + diff1
+        vadd.s8 q1, q13
+        table32
+        // inputs for next loop iteration
+        // a
+        vmov.8  q0, q4
+        vext.8  q1, q2, q3, #15
+        // c
+        vmov.8  q2, q6
+        vmov.8  q3, q7
+        vmov.8  q4, q5
+        bne     1b
+        vpop    {d8-d15}
+        bx      lr
+endfunc
+
+function ff_hevc_sao_edge_eo3_w32_neon_8, export=1
+        init_edge_32
+        sub     r1, r3
+        // load a
+        vld1.8  {q10-q11}, [r1, :64]!
+        vld1.8  {d24}, [r1, :64], r3
+        sub     r1, #32
+        vext.8  q0, q10, q11, #1
+        vext.8  q1, q11, q12, #1
+        // load c
+        vld1.8  {q2-q3}, [r1, :64]!
+        vld1.8  {d30}, [r1, :64], r3
+        sub     r1, #40
+1:      subs    r12, #1
+        // load b
+        vld1.8  {q10-q11}, [r1, :64]!
+        vld1.8  {q12}, [r1, :64], r3
+        sub     r1, #32
+        vext.8  q8, q10, q11, #7
+        vext.8  q9, q11, q12, #7
+        vext.8  q14, q12, q10, #7
+
+        diff32 q12, q13, q0, q1, q0, q1, q2, q3
+        diff32 q0, q1, q10, q11,  q8, q9, q2, q3
+
+        vadd.s8 q0, q12 //diff0 + diff1
+        vadd.s8 q1, q13
+        table32
+
+        // inputs for next loop iteration
+        // a
+        vext.8  q0, q2, q3, #1
+        vext.8  q1, q3, q15, #1
+        // c
+        vext.8  q2, q8, q9, #1
+        vext.8  q3, q9, q14, #1
+        vext.8  d30, d28, d2, #1
+        bne     1b
+        bx      lr
+endfunc
+
diff -Naur ffmpeg-3.2.4/libavcodec/arm/hevc_misc_neon.S ffmpeg-3.2.4.patch/libavcodec/arm/hevc_misc_neon.S
--- ffmpeg-3.2.4/libavcodec/arm/hevc_misc_neon.S	1970-01-01 01:00:00.000000000 +0100
+++ ffmpeg-3.2.4.patch/libavcodec/arm/hevc_misc_neon.S	2017-05-28 20:42:45.717088591 +0200
@@ -0,0 +1,62 @@
+#include "libavutil/arm/asm.S"
+#include "neon.S"
+
+@ rpi_zap_coeff_vals_neon(
+@   uint16_t * buf,          [r0]
+@   unsigned int log_n_m2)   [r1]
+
+function rpi_zap_coeff_vals_neon, export=1
+        vmov.i64 q8, #0
+        adr     r12, zc_tab
+        vmov.i64 q9, #0
+        tst     r0, #63
+        vmov.i64 q10, #0
+        add     r0, #63
+        vmov.i64 q11, #0
+        and     r0, #~63
+        ldr     pc, [r12, r1, lsl #2]
+
+zc_tab:
+        .word   zc_lc2
+        .word   zc_lc3
+        .word   zc_lc4
+        .word   zc_lc5
+
+@ 4*4*2: "32 bytes" 64 or 0 depending on dst address
+zc_lc2:
+        it eq
+        vstmeq  r0, {q8-q11}
+        bx      lr
+
+@ 16*16*2 = 512 = 64 * 8
+zc_lc4:
+        vstm    r0!, {q8-q11}
+        vstm    r0!, {q8-q11}
+        vstm    r0!, {q8-q11}
+        vstm    r0!, {q8-q11}
+        vstm    r0!, {q8-q11}
+        vstm    r0!, {q8-q11}
+@ 8*8*2 = 128
+zc_lc3:
+        vstm    r0!, {q8-q11}
+        vstm    r0,  {q8-q11}
+        bx      lr
+
+@ 32*32*2 = 2048 = 128 * 16
+zc_lc5:
+        vmov.i64 q12, #0
+        vmov.i64 q13, #0
+        vmov.i64 q14, #0
+        vmov.i64 q15, #0
+        mov     r2, #4
+1:
+        vstm    r0!, {q8-q15}
+        subs    r2, #1
+        vstm    r0!, {q8-q15}
+        vstm    r0!, {q8-q15}
+        vstm    r0!, {q8-q15}
+        bne     1b
+        bx      lr
+
+endfunc
+
diff -Naur ffmpeg-3.2.4/libavcodec/arm/Makefile ffmpeg-3.2.4.patch/libavcodec/arm/Makefile
--- ffmpeg-3.2.4/libavcodec/arm/Makefile	2017-02-10 14:25:26.000000000 +0100
+++ ffmpeg-3.2.4.patch/libavcodec/arm/Makefile	2017-05-28 20:42:45.714088580 +0200
@@ -131,9 +131,12 @@
 NEON-OBJS-$(CONFIG_LLAUDDSP)           += arm/lossless_audiodsp_neon.o
 NEON-OBJS-$(CONFIG_DCA_DECODER)        += arm/synth_filter_neon.o
 NEON-OBJS-$(CONFIG_HEVC_DECODER)       += arm/hevcdsp_init_neon.o       \
+                                          arm/hevc_misc_neon.o          \
                                           arm/hevcdsp_deblock_neon.o    \
+                                          arm/hevcdsp_epel_neon.o       \
                                           arm/hevcdsp_idct_neon.o       \
-                                          arm/hevcdsp_qpel_neon.o
+                                          arm/hevcdsp_qpel_neon.o       \
+                                          arm/hevcdsp_sao_neon.o
 NEON-OBJS-$(CONFIG_RV30_DECODER)       += arm/rv34dsp_neon.o
 NEON-OBJS-$(CONFIG_RV40_DECODER)       += arm/rv34dsp_neon.o            \
                                           arm/rv40dsp_neon.o
diff -Naur ffmpeg-3.2.4/libavcodec/avcodec.h ffmpeg-3.2.4.patch/libavcodec/avcodec.h
--- ffmpeg-3.2.4/libavcodec/avcodec.h	2017-02-10 14:25:26.000000000 +0100
+++ ffmpeg-3.2.4.patch/libavcodec/avcodec.h	2017-05-28 20:43:41.666286716 +0200
@@ -412,6 +412,8 @@
     AV_CODEC_ID_SHEERVIDEO,
     AV_CODEC_ID_YLC,
 
+    AV_CODEC_ID_H264_MVC,
+
     /* various PCM "codecs" */
     AV_CODEC_ID_FIRST_AUDIO = 0x10000,     ///< A dummy id pointing at the start of audio codecs
     AV_CODEC_ID_PCM_S16LE = 0x10000,
@@ -2879,6 +2881,7 @@
 #define FF_BUG_MS               8192 ///< Work around various bugs in Microsoft's broken decoders.
 #define FF_BUG_TRUNCATED       16384
 #define FF_BUG_IEDGE           32768
+#define FF_BUG_GMC_UNSUPPORTED 65536
 
     /**
      * strictly follow the standard (MPEG-4, ...).
@@ -3233,6 +3236,9 @@
 #define FF_PROFILE_H264_HIGH_444_PREDICTIVE  244
 #define FF_PROFILE_H264_HIGH_444_INTRA       (244|FF_PROFILE_H264_INTRA)
 #define FF_PROFILE_H264_CAVLC_444            44
+#define FF_PROFILE_H264_MULTIVIEW_HIGH       118
+#define FF_PROFILE_H264_STEREO_HIGH          128
+#define FF_PROFILE_H264_MULTIVIEW_HIGH_DEPTH 138
 
 #define FF_PROFILE_VC1_SIMPLE   0
 #define FF_PROFILE_VC1_MAIN     1
@@ -3564,6 +3570,12 @@
      */
     int trailing_padding;
 
+    /**
+     * Opaque pointer for use by replacement get_buffer2 code
+     *
+     * @author jc (08/02/2016)
+     */
+    void * get_buffer_context;
 } AVCodecContext;
 
 AVRational av_codec_get_pkt_timebase         (const AVCodecContext *avctx);
diff -Naur ffmpeg-3.2.4/libavcodec/cabac.h ffmpeg-3.2.4.patch/libavcodec/cabac.h
--- ffmpeg-3.2.4/libavcodec/cabac.h	2017-02-10 14:25:26.000000000 +0100
+++ ffmpeg-3.2.4.patch/libavcodec/cabac.h	2017-05-28 20:42:45.725088620 +0200
@@ -43,7 +43,14 @@
 typedef struct CABACContext{
     int low;
     int range;
-    int outstanding_count;
+    union
+    {
+        int outstanding_count;
+        struct {
+            uint16_t bits;
+            uint16_t range;
+        } by22;
+    };
     const uint8_t *bytestream_start;
     const uint8_t *bytestream;
     const uint8_t *bytestream_end;
diff -Naur ffmpeg-3.2.4/libavcodec/codec_desc.c ffmpeg-3.2.4.patch/libavcodec/codec_desc.c
--- ffmpeg-3.2.4/libavcodec/codec_desc.c	2017-02-10 14:25:26.000000000 +0100
+++ ffmpeg-3.2.4.patch/libavcodec/codec_desc.c	2017-05-28 20:42:45.727088627 +0200
@@ -1564,6 +1564,13 @@
         .props     = AV_CODEC_PROP_LOSSLESS,
         .mime_types= MT("image/png"),
     },
+    {
+        .id        = AV_CODEC_ID_H264_MVC,
+        .type      = AVMEDIA_TYPE_VIDEO,
+        .name      = "h264_mvc",
+        .long_name = NULL_IF_CONFIG_SMALL("H264 MVC"),
+        .props     = AV_CODEC_PROP_LOSSY,
+    },
 
     /* various PCM "codecs" */
     {
diff -Naur ffmpeg-3.2.4/libavcodec/h264.h ffmpeg-3.2.4.patch/libavcodec/h264.h
--- ffmpeg-3.2.4/libavcodec/h264.h	2017-02-10 14:25:26.000000000 +0100
+++ ffmpeg-3.2.4.patch/libavcodec/h264.h	2017-05-28 20:44:24.351436408 +0200
@@ -41,7 +41,9 @@
     H264_NAL_END_STREAM      = 11,
     H264_NAL_FILLER_DATA     = 12,
     H264_NAL_SPS_EXT         = 13,
+    H264_NAL_SPS_SUBSET      = 15,
     H264_NAL_AUXILIARY_SLICE = 19,
+    H264_NAL_SLICE_EXT       = 20,
 };
 
 #endif /* AVCODEC_H264_H */
diff -Naur ffmpeg-3.2.4/libavcodec/h264_parser.c ffmpeg-3.2.4.patch/libavcodec/h264_parser.c
--- ffmpeg-3.2.4/libavcodec/h264_parser.c	2017-02-10 14:25:26.000000000 +0100
+++ ffmpeg-3.2.4.patch/libavcodec/h264_parser.c	2017-05-28 20:45:07.870583994 +0200
@@ -62,6 +62,8 @@
     int parse_last_mb;
     int64_t reference_dts;
     int last_frame_num, last_picture_structure;
+    int is_mvc;
+    int slice_ext;
 } H264ParseContext;
 
 
@@ -109,24 +111,27 @@
         } else if (state <= 5) {
             int nalu_type = buf[i] & 0x1F;
             if (nalu_type == H264_NAL_SEI || nalu_type == H264_NAL_SPS ||
-                nalu_type == H264_NAL_PPS || nalu_type == H264_NAL_AUD) {
+                nalu_type == H264_NAL_PPS || nalu_type == H264_NAL_AUD ||
+                nalu_type == H264_NAL_SPS_SUBSET) {
                 if (pc->frame_start_found) {
                     i++;
                     goto found;
                 }
             } else if (nalu_type == H264_NAL_SLICE || nalu_type == H264_NAL_DPA ||
-                       nalu_type == H264_NAL_IDR_SLICE) {
+                       nalu_type == H264_NAL_IDR_SLICE || (p->is_mvc && nalu_type == H264_NAL_SLICE_EXT)) {
                 state += 8;
+
+                p->slice_ext = (nalu_type == H264_NAL_SLICE_EXT);
                 continue;
             }
             state = 7;
         } else {
             p->parse_history[p->parse_history_count++] = buf[i];
-            if (p->parse_history_count > 5) {
+            if (p->parse_history_count > 8) {
                 unsigned int mb, last_mb = p->parse_last_mb;
                 GetBitContext gb;
 
-                init_get_bits(&gb, p->parse_history, 8*p->parse_history_count);
+                init_get_bits8(&gb, p->parse_history + 3*p->slice_ext, p->parse_history_count - 3*p->slice_ext);
                 p->parse_history_count = 0;
                 mb= get_ue_golomb_long(&gb);
                 p->parse_last_mb = mb;
@@ -149,7 +154,7 @@
     pc->frame_start_found = 0;
     if (p->is_avc)
         return next_avc;
-    return i - (state & 5) - 5 * (state > 7);
+    return i - (state & 5) - 8 * (state > 7);
 }
 
 static int scan_mmco_reset(AVCodecParserContext *s, GetBitContext *gb,
@@ -594,7 +599,8 @@
         }
     }
 
-    parse_nal_units(s, avctx, buf, buf_size);
+    if (!p->is_mvc)
+        parse_nal_units(s, avctx, buf, buf_size);
 
     if (avctx->framerate.num)
         avctx->time_base = av_inv_q(av_mul_q(avctx->framerate, (AVRational){avctx->ticks_per_frame, 1}));
@@ -651,7 +657,7 @@
         if ((state & 0xFFFFFF00) != 0x100)
             break;
         nalu_type = state & 0x1F;
-        if (nalu_type == H264_NAL_SPS) {
+        if (nalu_type == H264_NAL_SPS || nalu_type == H264_NAL_SPS_SUBSET) {
             has_sps = 1;
         } else if (nalu_type == H264_NAL_PPS)
             has_pps = 1;
@@ -702,4 +708,24 @@
     .parser_parse   = h264_parse,
     .parser_close   = h264_close,
     .split          = h264_split,
+};
+
+static av_cold int init_mvc(AVCodecParserContext *s)
+{
+    H264ParseContext *p = s->priv_data;
+    int ret = init(s);
+    if (ret < 0)
+        return ret;
+
+    p->is_mvc = 1;
+    return 0;
+}
+
+AVCodecParser ff_h264_mvc_parser = {
+    .codec_ids      = { AV_CODEC_ID_H264_MVC },
+    .priv_data_size = sizeof(H264ParseContext),
+    .parser_init    = init_mvc,
+    .parser_parse   = h264_parse,
+    .parser_close   = h264_close,
+    .split          = h264_split,
 };
diff -Naur ffmpeg-3.2.4/libavcodec/hevc.c ffmpeg-3.2.4.patch/libavcodec/hevc.c
--- ffmpeg-3.2.4/libavcodec/hevc.c	2017-02-10 14:25:26.000000000 +0100
+++ ffmpeg-3.2.4.patch/libavcodec/hevc.c	2017-05-28 21:05:03.074076070 +0200
@@ -41,8 +41,196 @@
 #include "hevc.h"
 #include "profiles.h"
 
+#ifdef RPI
+  #include "rpi_qpu.h"
+  #include "rpi_shader.h"
+  #include "rpi_shader_cmd.h"
+  #include "rpi_zc.h"
+
+  // Define RPI_CACHE_UNIF_MVS to write motion vector uniform stream to cached memory
+  #define RPI_CACHE_UNIF_MVS  1
+
+  // Define RPI_SIMULATE_QPUS for debugging to run QPU code on the ARMs (*rotted*)
+  //#define RPI_SIMULATE_QPUS
+  #ifdef RPI_WORKER
+    #include "pthread.h"
+  #endif
+
+  static void worker_core(HEVCContext * const s);
+
+  // We can pred any block height but annoyingly if we we do then the TMU cache
+  // explodes and it goes even slower :-(
+  #if 0
+  #define Y_P_MAX_H     16
+  #define Y_B_MAX_H     16
+  #else
+  #define Y_P_MAX_H     64
+  #define Y_B_MAX_H     64
+  #endif
+#endif
+
+// #define DISABLE_MC
+
+#define DISABLE_CHROMA 0
+#define DEBUG_DECODE_N 0   // 0 = do all, n = frames idr onwards
+
+#define PACK2(hi,lo) (((hi) << 16) | ((lo) & 0xffff))
+
+#ifndef av_mod_uintp2
+static av_always_inline av_const unsigned av_mod_uintp2_c(unsigned a, unsigned p)
+{
+    return a & ((1 << p) - 1);
+}
+#   define av_mod_uintp2   av_mod_uintp2_c
+#endif
+
+#define Y_B_ONLY 0
+
 const uint8_t ff_hevc_pel_weight[65] = { [2] = 0, [4] = 1, [6] = 2, [8] = 3, [12] = 4, [16] = 5, [24] = 6, [32] = 7, [48] = 8, [64] = 9 };
 
+
+#if RPI_INTER
+
+#define MC_DUMMY_X (-32)
+#define MC_DUMMY_Y (-32)
+
+// Each luma QPU processes 2*RPI_NUM_CHUNKS 64x64 blocks
+// Each chroma QPU processes 3*RPI_NUM_CHUNKS 64x64 blocks, but requires two commands for B blocks
+// For each block of 64*64 the smallest block size is 8x4
+// We also need an extra command for the setup information
+
+#define UV_COMMANDS_PER_QPU (1 + RPI_NUM_CHUNKS*(64*64)*2/(8*4))
+// The QPU code for UV blocks only works up to a block width of 8
+#define RPI_CHROMA_BLOCK_WIDTH 8
+
+#define ENCODE_COEFFS(c0, c1, c2, c3) (((c0) & 0xff) | ((c1) & 0xff) << 8 | ((c2) & 0xff) << 16 | ((c3) & 0xff) << 24)
+
+// TODO Chroma only needs 4 taps
+
+// Actual filter goes -ve, +ve, +ve, -ve using these values
+static const uint32_t rpi_filter_coefs[8] = {
+        ENCODE_COEFFS(  0,  64,   0,  0),
+        ENCODE_COEFFS(  2,  58,  10,  2),
+        ENCODE_COEFFS(  4,  54,  16,  2),
+        ENCODE_COEFFS(  6,  46,  28,  4),
+        ENCODE_COEFFS(  4,  36,  36,  4),
+        ENCODE_COEFFS(  4,  28,  46,  6),
+        ENCODE_COEFFS(  2,  16,  54,  4),
+        ENCODE_COEFFS(  2,  10,  58,  2)
+};
+
+#define Y_COMMANDS_PER_QPU ((1+RPI_NUM_CHUNKS*(64*64)/(8*4)))
+
+#endif
+
+
+#ifdef RPI_WORKER
+
+typedef struct worker_global_env_s
+{
+    volatile int arm_load;
+    pthread_mutex_t lock;
+
+    unsigned int arm_y;
+    unsigned int arm_c;
+    unsigned int gpu_y;
+    unsigned int gpu_c;
+} worker_global_env_t;
+
+static worker_global_env_t worker_global_env =
+{
+    .lock = PTHREAD_MUTEX_INITIALIZER
+};
+
+
+//#define LOG_ENTER printf("Enter %s: p0=%d p1=%d (%d jobs) %p\n", __func__,s->pass0_job,s->pass1_job,s->worker_tail-s->worker_head,s);
+//#define LOG_EXIT printf("Exit %s: p0=%d p1=%d (%d jobs) %p\n", __func__,s->pass0_job,s->pass1_job,s->worker_tail-s->worker_head,s);
+
+#define LOG_ENTER
+#define LOG_EXIT
+
+// Call this when we have completed pass0 and wish to trigger pass1 for the current job
+static void worker_submit_job(HEVCContext *s)
+{
+  LOG_ENTER
+  pthread_mutex_lock(&s->worker_mutex);
+  s->worker_tail++;
+  s->pass0_job = (s->pass0_job + 1) % RPI_MAX_JOBS; // Move onto the next slot
+  pthread_cond_broadcast(&s->worker_cond_tail); // Let people know that the tail has moved
+  pthread_mutex_unlock(&s->worker_mutex);
+  LOG_EXIT
+}
+
+// Call this to say we have completed pass1
+static void worker_complete_job(HEVCContext *s)
+{
+  LOG_ENTER
+  pthread_mutex_lock(&s->worker_mutex);
+  s->worker_head++;
+  s->pass1_job = (s->pass1_job + 1) % RPI_MAX_JOBS; // Move onto the next slot
+  pthread_cond_broadcast(&s->worker_cond_head); // Let people know that the head has moved
+  pthread_mutex_unlock(&s->worker_mutex);
+  LOG_EXIT
+}
+
+// Call this to wait for all jobs to have completed at the end of a frame
+static void worker_wait(HEVCContext *s)
+{
+  LOG_ENTER
+  pthread_mutex_lock(&s->worker_mutex);
+  while( s->worker_head !=s->worker_tail)
+  {
+    pthread_cond_wait(&s->worker_cond_head, &s->worker_mutex);
+  }
+  pthread_mutex_unlock(&s->worker_mutex);
+  LOG_EXIT
+}
+
+// Call worker_pass0_ready to wait until the s->pass0_job slot becomes
+// available to receive the next job.
+static void worker_pass0_ready(HEVCContext *s)
+{
+  LOG_ENTER
+    pthread_mutex_lock(&s->worker_mutex);
+    // tail is number of submitted jobs
+    // head is number of completed jobs
+    // tail-head is number of outstanding jobs in the queue
+    // we need to ensure there is at least 1 space left for us to use
+    while( s->worker_tail - s->worker_head >= RPI_MAX_JOBS)
+    {
+      // Wait until another job is completed
+      pthread_cond_wait(&s->worker_cond_head, &s->worker_mutex);
+    }
+    pthread_mutex_unlock(&s->worker_mutex);
+  LOG_EXIT
+}
+
+static void *worker_start(void *arg)
+{
+  HEVCContext *s = (HEVCContext *)arg;
+  while(1) {
+    pthread_mutex_lock(&s->worker_mutex);
+
+    while( !s->kill_worker && s->worker_tail - s->worker_head <= 0)
+    {
+      pthread_cond_wait(&s->worker_cond_tail, &s->worker_mutex);
+    }
+    pthread_mutex_unlock(&s->worker_mutex);
+
+    if (s->kill_worker) {
+      break;
+    }
+    LOG_ENTER
+    worker_core(s);
+
+    worker_complete_job(s);
+    LOG_EXIT
+  }
+  return NULL;
+}
+
+#endif
+
 /**
  * NOTE: Each function hls_foo correspond to the function foo in the
  * specification (HLS stands for High Level Syntax).
@@ -55,6 +243,32 @@
 /* free everything allocated  by pic_arrays_init() */
 static void pic_arrays_free(HEVCContext *s)
 {
+#ifdef RPI
+    int job;
+    for(job=0;job<RPI_MAX_JOBS;job++) {
+      if (s->coeffs_buf_arm[job][0]) {
+        gpu_free(&s->coeffs_buf_default[job]);
+        s->coeffs_buf_arm[job][0] = 0;
+      }
+      if (s->coeffs_buf_arm[job][2]) {
+        gpu_free(&s->coeffs_buf_accelerated[job]);
+        s->coeffs_buf_arm[job][2] = 0;
+      }
+    }
+#endif
+#ifdef RPI_DEBLOCK_VPU
+    {
+        int i;
+        for (i = 0; i != RPI_DEBLOCK_VPU_Q_COUNT; ++i) {
+            struct dblk_vpu_q_s * const dvq = s->dvq_ents + i;
+
+            if (dvq->vpu_cmds_arm) {
+                gpu_free(&dvq->deblock_vpu_gmem);
+              dvq->vpu_cmds_arm = 0;
+            }
+        }
+    }
+#endif
     av_freep(&s->sao);
     av_freep(&s->deblock);
 
@@ -91,6 +305,89 @@
     int ctb_count        = sps->ctb_width * sps->ctb_height;
     int min_pu_size      = sps->min_pu_width * sps->min_pu_height;
 
+#ifdef RPI
+    const int coefs_in_ctb = (1 << sps->log2_ctb_size) * (1 << sps->log2_ctb_size);
+    const int coefs_per_luma = 64*64*RPI_CHUNK_SIZE*RPI_NUM_CHUNKS;
+    const int coefs_per_chroma = (coefs_per_luma * 2) >> sps->vshift[1] >> sps->hshift[1];
+    const int coefs_per_row = coefs_per_luma + coefs_per_chroma;
+    int job;
+
+    av_assert0(sps);
+//    s->max_ctu_count = sps->ctb_width;
+//    printf("CTB with=%d\n", sps->ctb_width);
+//    s->max_ctu_count = coefs_per_luma / coefs_in_ctb;
+    s->max_ctu_count = FFMIN(coefs_per_luma / coefs_in_ctb, sps->ctb_width);
+    s->ctu_per_y_chan = s->max_ctu_count / QPU_N_Y;
+    s->ctu_per_uv_chan = s->max_ctu_count / QPU_N_UV;
+
+    for(job=0;job<RPI_MAX_JOBS;job++) {
+        for(job=0;job<RPI_MAX_JOBS;job++) {
+            gpu_malloc_cached(sizeof(int16_t) * coefs_per_row, &s->coeffs_buf_default[job]);
+            s->coeffs_buf_arm[job][0] = (int16_t*) s->coeffs_buf_default[job].arm;
+            if (!s->coeffs_buf_arm[job][0])
+                goto fail;
+
+            gpu_malloc_cached(sizeof(int16_t) * (coefs_per_row + 32*32), &s->coeffs_buf_accelerated[job]);  // We prefetch past the end so provide an extra blocks worth of data
+            s->coeffs_buf_arm[job][2] = (int16_t*) s->coeffs_buf_accelerated[job].arm;
+            s->coeffs_buf_vc[job][2] = s->coeffs_buf_accelerated[job].vc;
+            if (!s->coeffs_buf_arm[job][2])
+                goto fail;
+            s->coeffs_buf_arm[job][3] = coefs_per_row + s->coeffs_buf_arm[job][2];  // This points to just beyond the end of the buffer.  Coefficients fill in backwards.
+            s->coeffs_buf_vc[job][3] = sizeof(int16_t) * coefs_per_row + s->coeffs_buf_vc[job][2];
+        }
+    }
+#endif
+#ifdef RPI_DEBLOCK_VPU
+    {
+        int i;
+        s->enable_rpi_deblock = !sps->sao_enabled;
+        s->setup_width = (sps->width+15) / 16;
+        s->setup_height = (sps->height+15) / 16;
+        s->uv_setup_width = ( (sps->width >> sps->hshift[1]) + 15) / 16;
+        s->uv_setup_height = ( (sps->height >> sps->vshift[1]) + 15) / 16;
+
+        for (i = 0; i != RPI_DEBLOCK_VPU_Q_COUNT; ++i)
+        {
+            struct dblk_vpu_q_s * const dvq = s->dvq_ents + i;
+            const unsigned int cmd_size = (sizeof(*dvq->vpu_cmds_arm) * 3 + 15) & ~15;
+            const unsigned int y_size = (sizeof(*dvq->y_setup_arm) * s->setup_width * s->setup_height + 15) & ~15;
+            const unsigned int uv_size = (sizeof(*dvq->uv_setup_arm) * s->uv_setup_width * s->uv_setup_height + 15) & ~15;
+            const unsigned int total_size =- cmd_size + y_size + uv_size;
+            int p_vc;
+            uint8_t * p_arm;
+ #if RPI_VPU_DEBLOCK_CACHED
+            gpu_malloc_cached(total_size, &dvq->deblock_vpu_gmem);
+ #else
+            gpu_malloc_uncached(total_size, &dvq->deblock_vpu_gmem);
+ #endif
+            p_vc = dvq->deblock_vpu_gmem.vc;
+            p_arm = dvq->deblock_vpu_gmem.arm;
+
+            // Zap all
+            memset(p_arm, 0, dvq->deblock_vpu_gmem.numbytes);
+
+            // Subdivide
+            dvq->vpu_cmds_arm = (void*)p_arm;
+            dvq->vpu_cmds_vc = p_vc;
+
+            p_arm += cmd_size;
+            p_vc += cmd_size;
+
+            dvq->y_setup_arm = (void*)p_arm;
+            dvq->y_setup_vc = (void*)p_vc;
+
+            p_arm += y_size;
+            p_vc += y_size;
+
+            dvq->uv_setup_arm = (void*)p_arm;
+            dvq->uv_setup_vc = (void*)p_vc;
+        }
+
+        s->dvq_n = 0;
+        s->dvq = s->dvq_ents + s->dvq_n;
+    }
+#endif
+
     s->bs_width  = (width  >> 2) + 1;
     s->bs_height = (height >> 2) + 1;
 
@@ -137,6 +434,29 @@
     return AVERROR(ENOMEM);
 }
 
+static void default_pred_weight_table(HEVCContext * const s)
+{
+  unsigned int i;
+  s->sh.luma_log2_weight_denom = 0;
+  s->sh.chroma_log2_weight_denom = 0;
+  for (i = 0; i < s->sh.nb_refs[L0]; i++) {
+      s->sh.luma_weight_l0[i] = 1;
+      s->sh.luma_offset_l0[i] = 0;
+      s->sh.chroma_weight_l0[i][0] = 1;
+      s->sh.chroma_offset_l0[i][0] = 0;
+      s->sh.chroma_weight_l0[i][1] = 1;
+      s->sh.chroma_offset_l0[i][1] = 0;
+  }
+  for (i = 0; i < s->sh.nb_refs[L1]; i++) {
+      s->sh.luma_weight_l1[i] = 1;
+      s->sh.luma_offset_l1[i] = 0;
+      s->sh.chroma_weight_l1[i][0] = 1;
+      s->sh.chroma_offset_l1[i][0] = 0;
+      s->sh.chroma_weight_l1[i][1] = 1;
+      s->sh.chroma_offset_l1[i][1] = 0;
+  }
+}
+
 static void pred_weight_table(HEVCContext *s, GetBitContext *gb)
 {
     int i = 0;
@@ -331,7 +651,7 @@
 static int set_sps(HEVCContext *s, const HEVCSPS *sps, enum AVPixelFormat pix_fmt)
 {
     #define HWACCEL_MAX (CONFIG_HEVC_DXVA2_HWACCEL + CONFIG_HEVC_D3D11VA_HWACCEL + CONFIG_HEVC_VAAPI_HWACCEL + CONFIG_HEVC_VDPAU_HWACCEL)
-    enum AVPixelFormat pix_fmts[HWACCEL_MAX + 2], *fmt = pix_fmts;
+    enum AVPixelFormat pix_fmts[HWACCEL_MAX + 4], *fmt = pix_fmts;
     int ret, i;
 
     pic_arrays_free(s);
@@ -350,6 +670,12 @@
     switch (sps->pix_fmt) {
     case AV_PIX_FMT_YUV420P:
     case AV_PIX_FMT_YUVJ420P:
+#if RPI_HEVC_SAND
+        // Currently geometry calc is stuffed for big sizes
+        if (sps->width < 2048 && sps->height <= 1088) {
+            *fmt++ = AV_PIX_FMT_SAND128;
+        }
+#endif
 #if CONFIG_HEVC_DXVA2_HWACCEL
         *fmt++ = AV_PIX_FMT_DXVA2_VLD;
 #endif
@@ -383,6 +709,7 @@
         ret = ff_thread_get_format(s->avctx, pix_fmts);
         if (ret < 0)
             goto fail;
+
         s->avctx->pix_fmt = ret;
     }
     else {
@@ -405,11 +732,12 @@
         for(c_idx = 0; c_idx < c_count; c_idx++) {
             int w = sps->width >> sps->hshift[c_idx];
             int h = sps->height >> sps->vshift[c_idx];
+            // ******** Very very nasty allocation kludge for plaited Chroma
             s->sao_pixel_buffer_h[c_idx] =
-                av_malloc((w * 2 * sps->ctb_height) <<
+                av_malloc((w * 2 * sps->ctb_height * (1 + (c_idx == 1))) <<
                           sps->pixel_shift);
             s->sao_pixel_buffer_v[c_idx] =
-                av_malloc((h * 2 * sps->ctb_width) <<
+                av_malloc((h * 2 * sps->ctb_width  * (1 + (c_idx == 1))) <<
                           sps->pixel_shift);
         }
     }
@@ -677,6 +1005,11 @@
                 (s->ps.pps->weighted_bipred_flag && sh->slice_type == B_SLICE)) {
                 pred_weight_table(s, gb);
             }
+            else
+            {
+              // Give us unit weights
+              default_pred_weight_table(s);
+            }
 
             sh->max_num_merge_cand = 5 - get_ue_golomb_long(gb);
             if (sh->max_num_merge_cand < 1 || sh->max_num_merge_cand > 5) {
@@ -934,6 +1267,34 @@
     return 0;
 }
 
+#ifdef RPI
+static void rpi_intra_pred(HEVCContext *s, int log2_trafo_size, int x0, int y0, int c_idx)
+{
+    // U & V done on U call in the case of sliced frames
+    if (rpi_sliced_frame(s->frame) && c_idx > 1)
+        return;
+
+    if (s->enable_rpi) {
+        HEVCLocalContext *lc = s->HEVClc;
+        HEVCPredCmd *cmd = s->univ_pred_cmds[s->pass0_job] + s->num_pred_cmds[s->pass0_job]++;
+        cmd->type = RPI_PRED_INTRA;
+        cmd->size = log2_trafo_size;
+        cmd->na = (lc->na.cand_bottom_left<<4) + (lc->na.cand_left<<3) + (lc->na.cand_up_left<<2) + (lc->na.cand_up<<1) + lc->na.cand_up_right;
+        cmd->c_idx = c_idx;
+        cmd->i_pred.x = x0;
+        cmd->i_pred.y = y0;
+        cmd->i_pred.mode = c_idx ? lc->tu.intra_pred_mode_c :  lc->tu.intra_pred_mode;
+    }
+    else if (rpi_sliced_frame(s->frame) && c_idx != 0) {
+        s->hpc.intra_pred_c[log2_trafo_size - 2](s, x0, y0, c_idx);
+    }
+    else {
+        s->hpc.intra_pred[log2_trafo_size - 2](s, x0, y0, c_idx);
+    }
+
+}
+#endif
+
 static int hls_transform_unit(HEVCContext *s, int x0, int y0,
                               int xBase, int yBase, int cb_xBase, int cb_yBase,
                               int log2_cb_size, int log2_trafo_size,
@@ -946,8 +1307,11 @@
     if (lc->cu.pred_mode == MODE_INTRA) {
         int trafo_size = 1 << log2_trafo_size;
         ff_hevc_set_neighbour_available(s, x0, y0, trafo_size, trafo_size);
-
+#ifdef RPI
+        rpi_intra_pred(s, log2_trafo_size, x0, y0, 0);
+#else
         s->hpc.intra_pred[log2_trafo_size - 2](s, x0, y0, 0);
+#endif
     }
 
     if (cbf_luma || cbf_cb[0] || cbf_cr[0] ||
@@ -1033,7 +1397,11 @@
             for (i = 0; i < (s->ps.sps->chroma_format_idc == 2 ? 2 : 1); i++) {
                 if (lc->cu.pred_mode == MODE_INTRA) {
                     ff_hevc_set_neighbour_available(s, x0, y0 + (i << log2_trafo_size_c), trafo_size_h, trafo_size_v);
+#ifdef RPI
+                    rpi_intra_pred(s, log2_trafo_size_c, x0, y0 + (i << log2_trafo_size_c), 1);
+#else
                     s->hpc.intra_pred[log2_trafo_size_c - 2](s, x0, y0 + (i << log2_trafo_size_c), 1);
+#endif
                 }
                 if (cbf_cb[i])
                     ff_hevc_hls_residual_coding(s, x0, y0 + (i << log2_trafo_size_c),
@@ -1062,7 +1430,11 @@
             for (i = 0; i < (s->ps.sps->chroma_format_idc == 2 ? 2 : 1); i++) {
                 if (lc->cu.pred_mode == MODE_INTRA) {
                     ff_hevc_set_neighbour_available(s, x0, y0 + (i << log2_trafo_size_c), trafo_size_h, trafo_size_v);
+#ifdef RPI
+                    rpi_intra_pred(s, log2_trafo_size_c, x0, y0 + (i << log2_trafo_size_c), 2);
+#else
                     s->hpc.intra_pred[log2_trafo_size_c - 2](s, x0, y0 + (i << log2_trafo_size_c), 2);
+#endif
                 }
                 if (cbf_cr[i])
                     ff_hevc_hls_residual_coding(s, x0, y0 + (i << log2_trafo_size_c),
@@ -1091,7 +1463,11 @@
                 if (lc->cu.pred_mode == MODE_INTRA) {
                     ff_hevc_set_neighbour_available(s, xBase, yBase + (i << log2_trafo_size),
                                                     trafo_size_h, trafo_size_v);
+#ifdef RPI
+                    rpi_intra_pred(s, log2_trafo_size, xBase, yBase + (i << log2_trafo_size), 1);
+#else
                     s->hpc.intra_pred[log2_trafo_size - 2](s, xBase, yBase + (i << log2_trafo_size), 1);
+#endif
                 }
                 if (cbf_cb[i])
                     ff_hevc_hls_residual_coding(s, xBase, yBase + (i << log2_trafo_size),
@@ -1101,7 +1477,11 @@
                 if (lc->cu.pred_mode == MODE_INTRA) {
                     ff_hevc_set_neighbour_available(s, xBase, yBase + (i << log2_trafo_size),
                                                 trafo_size_h, trafo_size_v);
+#ifdef RPI
+                    rpi_intra_pred(s, log2_trafo_size, xBase, yBase + (i << log2_trafo_size), 2);
+#else
                     s->hpc.intra_pred[log2_trafo_size - 2](s, xBase, yBase + (i << log2_trafo_size), 2);
+#endif
                 }
                 if (cbf_cr[i])
                     ff_hevc_hls_residual_coding(s, xBase, yBase + (i << log2_trafo_size),
@@ -1113,26 +1493,46 @@
             int trafo_size_h = 1 << (log2_trafo_size_c + s->ps.sps->hshift[1]);
             int trafo_size_v = 1 << (log2_trafo_size_c + s->ps.sps->vshift[1]);
             ff_hevc_set_neighbour_available(s, x0, y0, trafo_size_h, trafo_size_v);
+#ifdef RPI
+            rpi_intra_pred(s, log2_trafo_size_c, x0, y0, 1);
+            rpi_intra_pred(s, log2_trafo_size_c, x0, y0, 2);
+#else
             s->hpc.intra_pred[log2_trafo_size_c - 2](s, x0, y0, 1);
             s->hpc.intra_pred[log2_trafo_size_c - 2](s, x0, y0, 2);
+#endif
             if (s->ps.sps->chroma_format_idc == 2) {
                 ff_hevc_set_neighbour_available(s, x0, y0 + (1 << log2_trafo_size_c),
                                                 trafo_size_h, trafo_size_v);
+#ifdef RPI
+                rpi_intra_pred(s, log2_trafo_size_c, x0, y0 + (1 << log2_trafo_size_c), 1);
+                rpi_intra_pred(s, log2_trafo_size_c, x0, y0 + (1 << log2_trafo_size_c), 2);
+#else
                 s->hpc.intra_pred[log2_trafo_size_c - 2](s, x0, y0 + (1 << log2_trafo_size_c), 1);
                 s->hpc.intra_pred[log2_trafo_size_c - 2](s, x0, y0 + (1 << log2_trafo_size_c), 2);
+#endif
             }
         } else if (blk_idx == 3) {
             int trafo_size_h = 1 << (log2_trafo_size + 1);
             int trafo_size_v = 1 << (log2_trafo_size + s->ps.sps->vshift[1]);
             ff_hevc_set_neighbour_available(s, xBase, yBase,
                                             trafo_size_h, trafo_size_v);
+#ifdef RPI
+            rpi_intra_pred(s, log2_trafo_size, xBase, yBase, 1);
+            rpi_intra_pred(s, log2_trafo_size, xBase, yBase, 2);
+#else
             s->hpc.intra_pred[log2_trafo_size - 2](s, xBase, yBase, 1);
             s->hpc.intra_pred[log2_trafo_size - 2](s, xBase, yBase, 2);
+#endif
             if (s->ps.sps->chroma_format_idc == 2) {
                 ff_hevc_set_neighbour_available(s, xBase, yBase + (1 << (log2_trafo_size)),
                                                 trafo_size_h, trafo_size_v);
+#ifdef RPI
+                rpi_intra_pred(s, log2_trafo_size, xBase, yBase + (1 << (log2_trafo_size)), 1);
+                rpi_intra_pred(s, log2_trafo_size, xBase, yBase + (1 << (log2_trafo_size)), 2);
+#else
                 s->hpc.intra_pred[log2_trafo_size - 2](s, xBase, yBase + (1 << (log2_trafo_size)), 1);
                 s->hpc.intra_pred[log2_trafo_size - 2](s, xBase, yBase + (1 << (log2_trafo_size)), 2);
+#endif
             }
         }
     }
@@ -1278,47 +1678,120 @@
     return 0;
 }
 
-static int hls_pcm_sample(HEVCContext *s, int x0, int y0, int log2_cb_size)
+
+static int pcm_extract(HEVCContext * const s, const uint8_t * pcm, const int length, const int x0, const int y0, const int cb_size)
 {
-    HEVCLocalContext *lc = s->HEVClc;
     GetBitContext gb;
-    int cb_size   = 1 << log2_cb_size;
-    int stride0   = s->frame->linesize[0];
-    uint8_t *dst0 = &s->frame->data[0][y0 * stride0 + (x0 << s->ps.sps->pixel_shift)];
-    int   stride1 = s->frame->linesize[1];
-    uint8_t *dst1 = &s->frame->data[1][(y0 >> s->ps.sps->vshift[1]) * stride1 + ((x0 >> s->ps.sps->hshift[1]) << s->ps.sps->pixel_shift)];
-    int   stride2 = s->frame->linesize[2];
-    uint8_t *dst2 = &s->frame->data[2][(y0 >> s->ps.sps->vshift[2]) * stride2 + ((x0 >> s->ps.sps->hshift[2]) << s->ps.sps->pixel_shift)];
-
-    int length         = cb_size * cb_size * s->ps.sps->pcm.bit_depth +
-                         (((cb_size >> s->ps.sps->hshift[1]) * (cb_size >> s->ps.sps->vshift[1])) +
-                          ((cb_size >> s->ps.sps->hshift[2]) * (cb_size >> s->ps.sps->vshift[2]))) *
-                          s->ps.sps->pcm.bit_depth_chroma;
-    const uint8_t *pcm = skip_bytes(&lc->cc, (length + 7) >> 3);
     int ret;
 
-    if (!s->sh.disable_deblocking_filter_flag)
-        ff_hevc_deblocking_boundary_strengths(s, x0, y0, log2_cb_size);
-
     ret = init_get_bits(&gb, pcm, length);
     if (ret < 0)
         return ret;
 
-    s->hevcdsp.put_pcm(dst0, stride0, cb_size, cb_size,     &gb, s->ps.sps->pcm.bit_depth);
-    if (s->ps.sps->chroma_format_idc) {
-        s->hevcdsp.put_pcm(dst1, stride1,
+#ifdef RPI
+    if (rpi_sliced_frame(s->frame)) {
+        s->hevcdsp.put_pcm(rpi_sliced_frame_pos_y(s->frame, x0, y0),
+                           s->frame->linesize[0],
+                           cb_size, cb_size, &gb, s->ps.sps->pcm.bit_depth);
+
+        s->hevcdsp.put_pcm_c(rpi_sliced_frame_pos_c(s->frame, x0 >> s->ps.sps->hshift[1], y0 >> s->ps.sps->vshift[1]),
+                           s->frame->linesize[1],
                            cb_size >> s->ps.sps->hshift[1],
                            cb_size >> s->ps.sps->vshift[1],
                            &gb, s->ps.sps->pcm.bit_depth_chroma);
-        s->hevcdsp.put_pcm(dst2, stride2,
-                           cb_size >> s->ps.sps->hshift[2],
-                           cb_size >> s->ps.sps->vshift[2],
-                           &gb, s->ps.sps->pcm.bit_depth_chroma);
     }
+    else
+#endif
+    {
+        const int stride0   = s->frame->linesize[0];
+        uint8_t * const dst0 = &s->frame->data[0][y0 * stride0 + (x0 << s->ps.sps->pixel_shift)];
+        const int   stride1 = s->frame->linesize[1];
+        uint8_t * const dst1 = &s->frame->data[1][(y0 >> s->ps.sps->vshift[1]) * stride1 + ((x0 >> s->ps.sps->hshift[1]) << s->ps.sps->pixel_shift)];
+        const int   stride2 = s->frame->linesize[2];
+        uint8_t * const dst2 = &s->frame->data[2][(y0 >> s->ps.sps->vshift[2]) * stride2 + ((x0 >> s->ps.sps->hshift[2]) << s->ps.sps->pixel_shift)];
+
+        s->hevcdsp.put_pcm(dst0, stride0, cb_size, cb_size, &gb, s->ps.sps->pcm.bit_depth);
+        if (s->ps.sps->chroma_format_idc) {
+            s->hevcdsp.put_pcm(dst1, stride1,
+                               cb_size >> s->ps.sps->hshift[1],
+                               cb_size >> s->ps.sps->vshift[1],
+                               &gb, s->ps.sps->pcm.bit_depth_chroma);
+            s->hevcdsp.put_pcm(dst2, stride2,
+                               cb_size >> s->ps.sps->hshift[2],
+                               cb_size >> s->ps.sps->vshift[2],
+                               &gb, s->ps.sps->pcm.bit_depth_chroma);
+        }
 
+    }
     return 0;
 }
 
+#ifdef RPI
+int16_t * rpi_alloc_coeff_buf(HEVCContext * const s, const int buf_no, const int n)
+{
+    int16_t * const coeffs = (buf_no != 3) ?
+        s->coeffs_buf_arm[s->pass0_job][buf_no] + s->num_coeffs[s->pass0_job][buf_no] :
+        s->coeffs_buf_arm[s->pass0_job][buf_no] - s->num_coeffs[s->pass0_job][buf_no] - n;
+    s->num_coeffs[s->pass0_job][buf_no] += n;
+    return coeffs;
+}
+#endif
+
+// x * 2^(y*2)
+static inline unsigned int xyexp2(const unsigned int x, const unsigned int y)
+{
+    return x << (y * 2);
+}
+
+static int hls_pcm_sample(HEVCContext * const s, const int x0, const int y0, unsigned int log2_cb_size)
+{
+    // Length in bits
+    const unsigned int length = xyexp2(s->ps.sps->pcm.bit_depth, log2_cb_size) +
+        xyexp2(s->ps.sps->pcm.bit_depth_chroma, log2_cb_size - s->ps.sps->vshift[1]) +
+        xyexp2(s->ps.sps->pcm.bit_depth_chroma, log2_cb_size - s->ps.sps->vshift[2]);
+
+    const uint8_t * const pcm = skip_bytes(&s->HEVClc->cc, (length + 7) >> 3);
+
+    if (!s->sh.disable_deblocking_filter_flag)
+        ff_hevc_deblocking_boundary_strengths(s, x0, y0, log2_cb_size);
+
+#ifdef RPI
+    if (s->enable_rpi) {
+        // Copy coeffs
+        const int blen = (length + 7) >> 3;
+        // Round allocated bytes up to nearest 32 to avoid alignment confusion
+        // Allocation is in int16_t s
+        // As we are only using 1 byte per sample and the coeff buffer allows 2 per
+        // sample this rounding doesn't affect the total size we need to allocate for
+        // the coeff buffer
+        int16_t * const coeffs = rpi_alloc_coeff_buf(s, 0, ((blen + 31) & ~31) >> 1);
+        memcpy(coeffs, pcm, blen);
+
+        // Our coeff stash assumes that any partially allocated 64byte lump
+        // is zeroed so make that true.
+        {
+            uint8_t * const eopcm = (uint8_t *)coeffs + blen;
+            if ((-(intptr_t)eopcm & 63) != 0)
+                memset(eopcm, 0, -(intptr_t)eopcm & 63);
+        }
+
+        // Add command
+        {
+            HEVCPredCmd * const cmd = s->univ_pred_cmds[s->pass0_job] + s->num_pred_cmds[s->pass0_job]++;
+            cmd->type = RPI_PRED_I_PCM;
+            cmd->size = log2_cb_size;
+            cmd->i_pcm.src = coeffs;
+            cmd->i_pcm.x = x0;
+            cmd->i_pcm.y = y0;
+            cmd->i_pcm.src_len = length;
+        }
+        return 0;
+    }
+#endif
+
+    return pcm_extract(s, pcm, length, x0, y0, 1 << log2_cb_size);
+}
+
 /**
  * 8.5.3.2.2.1 Luma sample unidirectional interpolation process
  *
@@ -1335,6 +1808,91 @@
  * @param luma_offset additive offset applied to the luma prediction value
  */
 
+#if RPI_INTER
+static void rpi_luma_mc_uni(HEVCContext *s, uint8_t *dst, ptrdiff_t dststride,
+                        AVFrame *ref, const Mv *mv, int x_off, int y_off,
+                        int block_w, int block_h, int luma_weight, int luma_offset)
+{
+    HEVCMvCmd *cmd = s->unif_mv_cmds_y[s->pass0_job] + s->num_mv_cmds_y[s->pass0_job]++;
+    cmd->cmd = RPI_CMD_LUMA_UNI;
+    cmd->dst = dst;
+    cmd->dststride = dststride;
+    cmd->src = ref->data[0];
+    cmd->srcstride = ref->linesize[0];
+    cmd->mv = *mv;
+    cmd->x_off = x_off;
+    cmd->y_off = y_off;
+    cmd->block_w = block_w;
+    cmd->block_h = block_h;
+    cmd->weight = luma_weight;
+    cmd->offset = luma_offset;
+}
+
+static void rpi_luma_mc_bi(HEVCContext *s, uint8_t *dst, ptrdiff_t dststride,
+                       AVFrame *ref0, const Mv *mv0, int x_off, int y_off,
+                       int block_w, int block_h, AVFrame *ref1, const Mv *mv1,
+                       const struct MvField * const current_mv)
+{
+    HEVCMvCmd *cmd = s->unif_mv_cmds_y[s->pass0_job] + s->num_mv_cmds_y[s->pass0_job]++;
+    cmd->cmd = RPI_CMD_LUMA_BI;
+    cmd->dst = dst;
+    cmd->dststride = dststride;
+    cmd->src = ref0->data[0];
+    cmd->srcstride = ref0->linesize[0];
+    cmd->mv = *mv0;
+    cmd->x_off = x_off;
+    cmd->y_off = y_off;
+    cmd->block_w = block_w;
+    cmd->block_h = block_h;
+    cmd->src1 = ref1->data[0];
+    cmd->srcstride1 = ref1->linesize[0];
+    cmd->mv1 = *mv1;
+    cmd->ref_idx[0] = current_mv->ref_idx[0];
+    cmd->ref_idx[1] = current_mv->ref_idx[1];
+}
+
+static inline void rpi_chroma_mc_uni(HEVCContext *s, uint8_t *dst0,
+                          ptrdiff_t dststride, uint8_t *src0, ptrdiff_t srcstride,
+                          int x_off, int y_off, int block_w, int block_h, const Mv * const mv, int chroma_weight, int chroma_offset)
+{
+    HEVCMvCmd *cmd = s->unif_mv_cmds_c[s->pass0_job] + s->num_mv_cmds_c[s->pass0_job]++;
+    cmd->cmd = RPI_CMD_CHROMA_UNI;
+    cmd->dst = dst0;
+    cmd->dststride = dststride;
+    cmd->src = src0;
+    cmd->srcstride = srcstride;
+    cmd->mv = *mv;
+    cmd->x_off = x_off;
+    cmd->y_off = y_off;
+    cmd->block_w = block_w;
+    cmd->block_h = block_h;
+    cmd->weight = chroma_weight;
+    cmd->offset = chroma_offset;
+}
+
+static inline void rpi_chroma_mc_bi(HEVCContext *s, uint8_t *dst0, ptrdiff_t dststride, AVFrame *ref0, AVFrame *ref1,
+                         int x_off, int y_off, int block_w, int block_h, const struct MvField * const current_mv, int cidx)
+{
+    HEVCMvCmd *cmd = s->unif_mv_cmds_c[s->pass0_job] + s->num_mv_cmds_c[s->pass0_job]++;
+    cmd->cmd = RPI_CMD_CHROMA_BI+cidx;
+    cmd->dst = dst0;
+    cmd->dststride = dststride;
+    cmd->src = ref0->data[cidx+1];
+    cmd->srcstride = ref0->linesize[cidx+1];
+    cmd->mv = current_mv->mv[0];
+    cmd->mv1 = current_mv->mv[1];
+    cmd->x_off = x_off;
+    cmd->y_off = y_off;
+    cmd->block_w = block_w;
+    cmd->block_h = block_h;
+    cmd->src1 = ref1->data[cidx+1];
+    cmd->srcstride1 = ref1->linesize[cidx+1];
+    cmd->ref_idx[0] = current_mv->ref_idx[0];
+    cmd->ref_idx[1] = current_mv->ref_idx[1];
+}
+
+#endif
+
 static void luma_mc_uni(HEVCContext *s, uint8_t *dst, ptrdiff_t dststride,
                         AVFrame *ref, const Mv *mv, int x_off, int y_off,
                         int block_w, int block_h, int luma_weight, int luma_offset)
@@ -1350,6 +1908,10 @@
                            (s->sh.slice_type == B_SLICE && s->ps.pps->weighted_bipred_flag);
     int idx              = ff_hevc_pel_weight[block_w];
 
+#ifdef DISABLE_MC
+    return;
+#endif
+
     x_off += mv->x >> 2;
     y_off += mv->y >> 2;
     src   += y_off * srcstride + (x_off * (1 << s->ps.sps->pixel_shift));
@@ -1396,7 +1958,7 @@
  * @param mv1 motion vector1 (relative to block position) to get pixel data from
  * @param current_mv current motion vector structure
  */
- static void luma_mc_bi(HEVCContext *s, uint8_t *dst, ptrdiff_t dststride,
+static void luma_mc_bi(HEVCContext *s, uint8_t *dst, ptrdiff_t dststride,
                        AVFrame *ref0, const Mv *mv0, int x_off, int y_off,
                        int block_w, int block_h, AVFrame *ref1, const Mv *mv1, struct MvField *current_mv)
 {
@@ -1420,6 +1982,10 @@
     uint8_t *src0  = ref0->data[0] + y_off0 * src0stride + (int)((unsigned)x_off0 << s->ps.sps->pixel_shift);
     uint8_t *src1  = ref1->data[0] + y_off1 * src1stride + (int)((unsigned)x_off1 << s->ps.sps->pixel_shift);
 
+#ifdef DISABLE_MC
+    return;
+#endif
+
     if (x_off0 < QPEL_EXTRA_BEFORE || y_off0 < QPEL_EXTRA_AFTER ||
         x_off0 >= pic_width - block_w - QPEL_EXTRA_AFTER ||
         y_off0 >= pic_height - block_h - QPEL_EXTRA_AFTER) {
@@ -1505,6 +2071,10 @@
     intptr_t _mx         = mx << (1 - hshift);
     intptr_t _my         = my << (1 - vshift);
 
+#ifdef DISABLE_MC
+    return;
+#endif
+
     x_off += mv->x >> (2 + hshift);
     y_off += mv->y >> (2 + vshift);
     src0  += y_off * srcstride + (x_off * (1 << s->ps.sps->pixel_shift));
@@ -1569,6 +2139,10 @@
     int hshift = s->ps.sps->hshift[1];
     int vshift = s->ps.sps->vshift[1];
 
+#ifdef DISABLE_MC
+    return;
+#endif
+
     intptr_t mx0 = av_mod_uintp2(mv0->x, 2 + hshift);
     intptr_t my0 = av_mod_uintp2(mv0->y, 2 + vshift);
     intptr_t mx1 = av_mod_uintp2(mv1->x, 2 + hshift);
@@ -1696,14 +2270,423 @@
     }
 }
 
-static void hls_prediction_unit(HEVCContext *s, int x0, int y0,
-                                int nPbW, int nPbH,
-                                int log2_cb_size, int partIdx, int idx)
+
+#if RPI_INTER
+
+static HEVCRpiLumaPred *
+rpi_nxt_pred_y(HEVCContext *const s, const unsigned int load_val)
+{
+    HEVCRpiLumaPred * yp = s->curr_pred_y;
+    HEVCRpiLumaPred * ypt = yp + 1;
+    for (unsigned int i = 1; i != QPU_N_GRP_Y; ++i, ++ypt) {
+        if (ypt->load < yp->load)
+            yp = ypt;
+    }
+
+//        yp->load += load_val;
+    ++yp->load;
+    return yp;
+}
+
+static void
+rpi_pred_y(HEVCContext *const s, const int x0, const int y0,
+           const int nPbW, const int nPbH,
+           const Mv *const mv,
+           const int weight_mul,
+           const int weight_offset,
+           AVFrame *const src_frame)
+{
+    const unsigned int y_off = rpi_sliced_frame_off_y(s->frame, x0, y0);
+
+//    rpi_luma_mc_uni(s, s->frame->data[0] + y_off, s->frame->linesize[0], src_frame,
+//                    mv, x0, y0, nPbW, nPbH,
+//                    weight_mul, weight_offset);
+
+    {
+        const unsigned int mx          = mv->x & 3;
+        const unsigned int my          = mv->y & 3;
+        const unsigned int my_mx       = (my << 8) | mx;
+        const uint32_t     my2_mx2_my_mx = (my_mx << 16) | my_mx;
+        const int x1_m3 = x0 + (mv->x >> 2) - 3;
+        const int y1_m3 = y0 + (mv->y >> 2) - 3;
+        const uint32_t src_vc_address_y = get_vc_address_y(src_frame);
+        uint32_t dst_addr = get_vc_address_y(s->frame) + y_off;
+        const uint32_t wo = PACK2(weight_offset * 2 + 1, weight_mul);
+
+        // Potentially we could change the assembly code to support taller sizes in one go
+        for (int start_y = 0; start_y < nPbH; start_y += Y_P_MAX_H, dst_addr += s->frame->linesize[0] * 16)
+        {
+            const uint32_t src_yx_y = y1_m3 + start_y;
+            int start_x = 0;
+            const int bh = FFMIN(nPbH - start_y, Y_P_MAX_H);
+
+#if 1
+            // As Y-pred operates on two independant 8-wide src blocks we can merge
+            // this pred with the previous one if it the previous one is 8 pel wide,
+            // the same height as the current block, immediately to the left of our
+            // current dest block and mono-pred.
+
+            qpu_mc_pred_y_t *const last_y8_p = s->last_y8_p;
+            if (last_y8_p != NULL && last_y8_p->p.h == bh && last_y8_p->p.dst_addr + 8 == dst_addr)
+            {
+                const int bw = FFMIN(nPbW, 8);
+                qpu_mc_pred_y_t *const last_y8_lx = s->last_y8_lx;
+
+                last_y8_lx->next_src2_x = x1_m3;
+                last_y8_lx->next_src2_y = src_yx_y;
+                last_y8_lx->next_src2_base = src_vc_address_y;
+                last_y8_p->p.w += bw;
+                last_y8_p->p.mymx21 = PACK2(my2_mx2_my_mx, last_y8_p->p.mymx21);
+                last_y8_p->p.wo2 = wo;
+
+                s->last_y8_p = NULL;
+                s->last_y8_lx = NULL;
+                start_x = bw;
+#if RPI_TSTATS
+                ++s->tstats.y_pred1_y8_merge;
+#endif
+            }
+#endif
+
+            for (; start_x < nPbW; start_x += 16)
+            {
+                const int bw = FFMIN(nPbW - start_x, 16);
+                HEVCRpiLumaPred * const yp = rpi_nxt_pred_y(s, bh + 7);
+                qpu_mc_pred_y_t *const cmd_lx = yp->last_lx;
+                qpu_mc_pred_y_t *const cmd_y = yp->qpu_mc_curr;
+#if RPI_TSTATS
+                {
+                    HEVCRpiStats *const ts = &s->tstats;
+                    if (mx == 0 && my == 0)
+                        ++ts->y_pred1_x0y0;
+                    else if (mx == 0)
+                        ++ts->y_pred1_x0;
+                    else if (my == 0)
+                        ++ts->y_pred1_y0;
+                    else
+                        ++ts->y_pred1_xy;
+
+                    if (nPbW > 8)
+                        ++ts->y_pred1_wgt8;
+                    else
+                        ++ts->y_pred1_wle8;
+
+                    if (nPbH > 16)
+                        ++ts->y_pred1_hgt16;
+                    else
+                        ++ts->y_pred1_hle16;
+                }
+#endif
+                cmd_y[-1].next_fn = s->qpu_filter;
+                cmd_lx->next_src1_x = x1_m3 + start_x;
+                cmd_lx->next_src1_y = src_yx_y;
+                cmd_lx->next_src1_base = src_vc_address_y;
+                if (bw <= 8)
+                {
+                    cmd_lx->next_src2_x = MC_DUMMY_X;
+                    cmd_lx->next_src2_y = MC_DUMMY_Y;
+                    cmd_lx->next_src2_base = s->qpu_dummy_frame;
+                }
+                else
+                {
+                    cmd_lx->next_src2_x = x1_m3 + start_x + 8;
+                    cmd_lx->next_src2_y = src_yx_y;
+                    cmd_lx->next_src2_base = src_vc_address_y;
+                }
+                cmd_y->p.w = bw;
+                cmd_y->p.h = bh;
+                cmd_y->p.mymx21 = my2_mx2_my_mx;
+                cmd_y->p.wo1 = wo;
+                cmd_y->p.wo2 = wo;
+                cmd_y->p.dst_addr =  dst_addr + start_x;
+                yp->last_lx = cmd_y;
+                yp->qpu_mc_curr = cmd_y + 1;
+
+                if (bw == 8) {
+                    s->last_y8_lx = cmd_lx;
+                    s->last_y8_p = cmd_y;
+                }
+            }
+        }
+    }
+}
+
+static void
+rpi_pred_y_b(HEVCContext * const s,
+           const int x0, const int y0,
+           const int nPbW, const int nPbH,
+           const struct MvField *const mv_field,
+           AVFrame *const src_frame,
+           AVFrame *const src_frame2)
+{
+    const unsigned int y_off = rpi_sliced_frame_off_y(s->frame, x0, y0);
+    const Mv * const mv  = mv_field->mv + 0;
+    const Mv * const mv2 = mv_field->mv + 1;
+
+//    rpi_luma_mc_bi(s, s->frame->data[0] + y_off, s->frame->linesize[0], src_frame,
+//           mv, x0, y0, nPbW, nPbH,
+//           src_frame2, mv2, mv_field);
+    {
+        const unsigned int mx          = mv->x & 3;
+        const unsigned int my          = mv->y & 3;
+        const unsigned int my_mx = (my<<8) | mx;
+        const unsigned int mx2          = mv2->x & 3;
+        const unsigned int my2          = mv2->y & 3;
+        const unsigned int my2_mx2 = (my2<<8) | mx2;
+        const uint32_t     my2_mx2_my_mx = (my2_mx2 << 16) | my_mx;
+        const int x1 = x0 + (mv->x >> 2) - 3;
+        const int y1 = y0 + (mv->y >> 2) - 3;
+        const int x2 = x0 + (mv2->x >> 2) - 3;
+        const int y2 = y0 + (mv2->y >> 2) - 3;
+        const unsigned int ref_idx0 = mv_field->ref_idx[0];
+        const unsigned int ref_idx1 = mv_field->ref_idx[1];
+        const uint32_t wt_offset = s->sh.luma_offset_l0[ref_idx0] +
+                     s->sh.luma_offset_l1[ref_idx1] + 1;
+        const uint32_t wo1 = PACK2(wt_offset, s->sh.luma_weight_l0[ref_idx0]);
+        const uint32_t wo2 = PACK2(wt_offset, s->sh.luma_weight_l1[ref_idx1]);
+
+        uint32_t dst = get_vc_address_y(s->frame) + y_off;
+        const uint32_t src1_base = get_vc_address_y(src_frame);
+        const uint32_t src2_base = get_vc_address_y(src_frame2);
+
+        for (int start_y=0; start_y < nPbH; start_y += Y_B_MAX_H)
+        {
+            const unsigned int bh = FFMIN(nPbH - start_y, Y_B_MAX_H);
+
+            for (int start_x=0; start_x < nPbW; start_x += 8)
+            { // B blocks work 8 at a time
+                HEVCRpiLumaPred * const yp = rpi_nxt_pred_y(s, bh + 7);
+                qpu_mc_pred_y_t *const cmd_lx = yp->last_lx;
+                qpu_mc_pred_y_t *const cmd_y = yp->qpu_mc_curr;
+#if RPI_TSTATS
+              {
+                  HEVCRpiStats *const ts = &s->tstats;
+                  const unsigned int mmx = mx | mx2;
+                  const unsigned int mmy = my | my2;
+                  if (mmx == 0 && mmy == 0)
+                      ++ts->y_pred2_x0y0;
+                  else if (mmx == 0)
+                      ++ts->y_pred2_x0;
+                  else if (mmy == 0)
+                      ++ts->y_pred2_y0;
+                  else
+                      ++ts->y_pred2_xy;
+
+                  if (nPbH > 16)
+                      ++ts->y_pred2_hgt16;
+                  else
+                      ++ts->y_pred2_hle16;
+              }
+#endif
+              cmd_y[-1].next_fn = s->qpu_filter_b;
+              cmd_lx->next_src1_x = x1 + start_x;
+              cmd_lx->next_src1_y = y1 + start_y;
+              cmd_lx->next_src1_base = src1_base;
+              cmd_lx->next_src2_x = x2 + start_x;
+              cmd_lx->next_src2_y = y2 + start_y;
+              cmd_lx->next_src2_base = src2_base;
+              cmd_y->p.w = FFMIN(nPbW - start_x, 8);
+              cmd_y->p.h = bh;
+              cmd_y->p.mymx21 = my2_mx2_my_mx;
+              cmd_y->p.wo1 = wo1;
+              cmd_y->p.wo2 = wo2;
+              cmd_y->p.dst_addr =  dst + start_x;
+              yp->last_lx = cmd_y;
+              yp->qpu_mc_curr = cmd_y + 1;
+          }
+          dst += s->frame->linesize[0] * 16;
+        }
+    }
+}
+
+
+static HEVCRpiChromaPred *
+rpi_nxt_pred_c(HEVCContext *const s, const unsigned int load_val)
+{
+    HEVCRpiChromaPred * cp = s->curr_pred_c;
+    HEVCRpiChromaPred * cpt = cp + 1;
+    for (unsigned int i = 1; i != QPU_N_GRP_UV; ++i, ++cpt) {
+        if (cpt->load < cp->load)
+            cp = cpt;
+    }
+    // Actual use of load_val is noticably better but we haven't sorted Q length problems yet
+    ++cp->load;
+//    cp->load += load_val;
+    return cp;
+}
+
+static void
+rpi_pred_c(HEVCContext * const s, const int x0_c, const int y0_c,
+  const int nPbW_c, const int nPbH_c,
+  const Mv * const mv,
+  const int16_t * const c_weights,
+  const int16_t * const c_offsets,
+  AVFrame * const src_frame)
+{
+
+    const unsigned int c_off = rpi_sliced_frame_off_c(s->frame, x0_c, y0_c);
+#if 0
+    av_assert0(s->frame->linesize[1] == s->frame->linesize[2]);
+
+    rpi_chroma_mc_uni(s, s->frame->data[1] + c_off, s->frame->linesize[1], src_frame->data[1], src_frame->linesize[1],
+                x0_c, y0_c, nPbW_c, nPbH_c, mv,
+                c_weights[0], c_offsets[0]);
+
+    rpi_chroma_mc_uni(s, s->frame->data[2] + c_off, s->frame->linesize[2], src_frame->data[2], src_frame->linesize[2],
+                x0_c, y0_c, nPbW_c, nPbH_c, mv,
+                c_weights[1], c_offsets[1]);
+#endif
+    {
+        const int hshift           = s->ps.sps->hshift[1];
+        const int vshift           = s->ps.sps->vshift[1];
+
+        const int x1_c = x0_c + (mv->x >> (2 + hshift)) - 1;
+        const int y1_c = y0_c + (mv->y >> (2 + hshift)) - 1;
+        const uint32_t src_base_u = get_vc_address_u(src_frame);
+        const uint32_t x_coeffs = rpi_filter_coefs[av_mod_uintp2(mv->x, 2 + hshift) << (1 - hshift)];
+        const uint32_t y_coeffs = rpi_filter_coefs[av_mod_uintp2(mv->y, 2 + vshift) << (1 - vshift)];
+        const uint32_t wo_u = PACK2(c_offsets[0] * 2 + 1, c_weights[0]);
+        const uint32_t wo_v = PACK2(c_offsets[1] * 2 + 1, c_weights[1]);
+        uint32_t dst_base_u = get_vc_address_u(s->frame) + c_off;
+
+        for(int start_y=0;start_y < nPbH_c;start_y+=16)
+        {
+            const int bh = FFMIN(nPbH_c-start_y, 16);
+
+            for(int start_x=0; start_x < nPbW_c; start_x+=RPI_CHROMA_BLOCK_WIDTH)
+            {
+                HEVCRpiChromaPred * const cp = rpi_nxt_pred_c(s, bh + 3);
+                qpu_mc_pred_c_t * const u = cp->qpu_mc_curr;
+                qpu_mc_pred_c_t * const last_l0 = cp->last_l0;
+                const int bw = FFMIN(nPbW_c-start_x, RPI_CHROMA_BLOCK_WIDTH);
+
+                u[-1].next_fn  = s->qpu_filter_uv;
+                last_l0->next_src_x = x1_c + start_x;
+                last_l0->next_src_y = y1_c + start_y;
+                last_l0->next_src_base_c = src_base_u;
+                u[0].p.h = bh;
+                u[0].p.w = bw;
+                u[0].p.coeffs_x = x_coeffs;
+                u[0].p.coeffs_y = y_coeffs;
+                u[0].p.wo_u = wo_u;
+                u[0].p.wo_v = wo_v;
+                u[0].p.dst_addr_c = dst_base_u + start_x * 2;
+                cp->last_l0 = u;
+                cp->qpu_mc_curr = u + 1;
+            }
+
+            dst_base_u += s->frame->linesize[1] * 16;
+        }
+    }
+  return;
+}
+
+static void
+rpi_pred_c_b(HEVCContext * const s, const int x0_c, const int y0_c,
+  const int nPbW_c, const int nPbH_c,
+  const struct MvField * const mv_field,
+  const int16_t * const c_weights,
+  const int16_t * const c_offsets,
+  const int16_t * const c_weights2,
+  const int16_t * const c_offsets2,
+  AVFrame * const src_frame,
+  AVFrame * const src_frame2)
+{
+    const unsigned int c_off = rpi_sliced_frame_off_c(s->frame, x0_c, y0_c);
+#if 0
+    rpi_chroma_mc_bi(s, s->frame->data[1] + c_off, s->frame->linesize[1], src_frame, src_frame2,
+                 x0_c, y0_c, nPbW_c, nPbH_c, mv_field, 0);
+
+    rpi_chroma_mc_bi(s, s->frame->data[2] + c_off, s->frame->linesize[2], src_frame, src_frame2,
+                 x0_c, y0_c, nPbW_c, nPbH_c, mv_field, 1);
+#endif
+    {
+        const int hshift = s->ps.sps->hshift[1];
+        const int vshift = s->ps.sps->vshift[1];
+        const Mv * const mv = mv_field->mv + 0;
+        const Mv * const mv2 = mv_field->mv + 1;
+
+        const unsigned int mx = av_mod_uintp2(mv->x, 2 + hshift);
+        const unsigned int my = av_mod_uintp2(mv->y, 2 + vshift);
+        const uint32_t coefs0_x = rpi_filter_coefs[mx << (1 - hshift)];
+        const uint32_t coefs0_y = rpi_filter_coefs[my << (1 - vshift)]; // Fractional part of motion vector
+        const int x1_c = x0_c + (mv->x >> (2 + hshift)) - 1;
+        const int y1_c = y0_c + (mv->y >> (2 + hshift)) - 1;
+
+        const unsigned int mx2 = av_mod_uintp2(mv2->x, 2 + hshift);
+        const unsigned int my2 = av_mod_uintp2(mv2->y, 2 + vshift);
+        const uint32_t coefs1_x = rpi_filter_coefs[mx2 << (1 - hshift)];
+        const uint32_t coefs1_y = rpi_filter_coefs[my2 << (1 - vshift)]; // Fractional part of motion vector
+
+        const int x2_c = x0_c + (mv2->x >> (2 + hshift)) - 1;
+        const int y2_c = y0_c + (mv2->y >> (2 + hshift)) - 1;
+
+        uint32_t dst_base_u = get_vc_address_u(s->frame) + c_off;
+
+        for (int start_y = 0; start_y < nPbH_c; start_y += 16) {
+          const unsigned int bh = FFMIN(nPbH_c-start_y, 16);
+
+          // We are allowed 3/4 powers of two as well as powers of 2
+          av_assert2(bh == 16 || bh == 12 || bh == 8 || bh == 6 || bh == 4 || bh == 2);
+
+          for (int start_x=0; start_x < nPbW_c; start_x += RPI_CHROMA_BLOCK_WIDTH) {
+              const unsigned int bw = FFMIN(nPbW_c-start_x, RPI_CHROMA_BLOCK_WIDTH);
+
+              HEVCRpiChromaPred * const cp = rpi_nxt_pred_c(s, bh * 2 + 3);
+              qpu_mc_pred_c_t * const u = cp->qpu_mc_curr;
+              qpu_mc_pred_c_t * const last_l0 = cp->last_l0;
+              qpu_mc_pred_c_t * const last_l1 = cp->last_l1;
+
+              u[-1].next_fn = s->qpu_filter_uv_b0;
+              last_l0->next_src_x = x1_c + start_x;
+              last_l0->next_src_y = y1_c + start_y;
+              last_l0->next_src_base_c = get_vc_address_u(src_frame);
+
+              u[0].next_fn = 0;  // Ignored - 2 block cmd
+              u[0].next_src_x = x2_c + start_x;
+              u[0].next_src_y = y2_c + start_y;
+              u[0].next_src_base_c = get_vc_address_u(src_frame2);
+
+              u[0].b0.h = (bh<16 ? bh : 16);
+              u[0].b0.w = (bw<RPI_CHROMA_BLOCK_WIDTH ? bw : RPI_CHROMA_BLOCK_WIDTH);
+              u[0].b0.coeffs_x = coefs0_x;
+              u[0].b0.coeffs_y = coefs0_y;
+              u[0].b0.weight_u = c_weights[0]; // Weight L0 U
+              u[0].b0.weight_v = c_weights[1]; // Weight L0 V
+              u[0].b0.dummy0 = 0;  // Intermediate results are not written back in first pass of B filtering
+
+              last_l1->next_src_x = x2_c + start_x;
+              last_l1->next_src_y = y2_c + start_y;
+              last_l1->next_src_base_c = get_vc_address_u(src_frame2);
+
+              u[1].b1.dummy0 = 0;  // w,h inherited from b0
+              u[1].b1.coeffs_x = coefs1_x;
+              u[1].b1.coeffs_y = coefs1_y;
+              u[1].b1.wo_u = PACK2(c_offsets[0] + c_offsets2[0] + 1, c_weights2[0]);
+              u[1].b1.wo_v = PACK2(c_offsets[1] + c_offsets2[1] + 1, c_weights2[1]);
+              u[1].b1.dst_addr_c = dst_base_u + start_x * 2;
+
+              cp->last_l0 = u;
+              cp->last_l1 = u + 1;
+              cp->qpu_mc_curr = u + 2;
+          }
+
+          dst_base_u += s->frame->linesize[1] * 16;
+        }
+    }
+}
+#endif
+
+
+
+static void hls_prediction_unit(HEVCContext * const s, const int x0, const int y0,
+                                const int nPbW, const int nPbH,
+                                const unsigned int log2_cb_size, const unsigned int partIdx, const unsigned int idx)
 {
 #define POS(c_idx, x, y)                                                              \
     &s->frame->data[c_idx][((y) >> s->ps.sps->vshift[c_idx]) * s->frame->linesize[c_idx] + \
                            (((x) >> s->ps.sps->hshift[c_idx]) << s->ps.sps->pixel_shift)]
-    HEVCLocalContext *lc = s->HEVClc;
+    HEVCLocalContext * const lc = s->HEVClc;
     int merge_idx = 0;
     struct MvField current_mv = {{{ 0 }}};
 
@@ -1721,8 +2704,7 @@
     int y_cb             = y0 >> log2_min_cb_size;
     int x_pu, y_pu;
     int i, j;
-
-    int skip_flag = SAMPLE_CTB(s->skip_flag, x_cb, y_cb);
+    const int skip_flag = SAMPLE_CTB(s->skip_flag, x_cb, y_cb);
 
     if (!skip_flag)
         lc->pu.merge_flag = ff_hevc_merge_flag_decode(s);
@@ -1766,12 +2748,29 @@
         int nPbW_c = nPbW >> s->ps.sps->hshift[1];
         int nPbH_c = nPbH >> s->ps.sps->vshift[1];
 
-        luma_mc_uni(s, dst0, s->frame->linesize[0], ref0->frame,
+#if RPI_INTER
+        if (s->enable_rpi) {
+            rpi_pred_y(s, x0, y0, nPbW, nPbH, current_mv.mv + 0,
+              s->sh.luma_weight_l0[current_mv.ref_idx[0]], s->sh.luma_offset_l0[current_mv.ref_idx[0]],
+              ref0->frame);
+        } else
+#endif
+        {
+            luma_mc_uni(s, dst0, s->frame->linesize[0], ref0->frame,
                     &current_mv.mv[0], x0, y0, nPbW, nPbH,
                     s->sh.luma_weight_l0[current_mv.ref_idx[0]],
                     s->sh.luma_offset_l0[current_mv.ref_idx[0]]);
+        }
 
         if (s->ps.sps->chroma_format_idc) {
+#if RPI_INTER
+            if (s->enable_rpi) {
+                rpi_pred_c(s, x0_c, y0_c, nPbW_c, nPbH_c, current_mv.mv + 0,
+                  s->sh.chroma_weight_l0[current_mv.ref_idx[0]], s->sh.chroma_offset_l0[current_mv.ref_idx[0]],
+                  ref0->frame);
+                return;
+            }
+#endif
             chroma_mc_uni(s, dst1, s->frame->linesize[1], ref0->frame->data[1], ref0->frame->linesize[1],
                           0, x0_c, y0_c, nPbW_c, nPbH_c, &current_mv,
                           s->sh.chroma_weight_l0[current_mv.ref_idx[0]][0], s->sh.chroma_offset_l0[current_mv.ref_idx[0]][0]);
@@ -1785,12 +2784,29 @@
         int nPbW_c = nPbW >> s->ps.sps->hshift[1];
         int nPbH_c = nPbH >> s->ps.sps->vshift[1];
 
-        luma_mc_uni(s, dst0, s->frame->linesize[0], ref1->frame,
+#if RPI_INTER
+        if (s->enable_rpi) {
+            rpi_pred_y(s, x0, y0, nPbW, nPbH, current_mv.mv + 1,
+              s->sh.luma_weight_l1[current_mv.ref_idx[1]], s->sh.luma_offset_l1[current_mv.ref_idx[1]],
+              ref1->frame);
+        } else
+#endif
+        {
+            luma_mc_uni(s, dst0, s->frame->linesize[0], ref1->frame,
                     &current_mv.mv[1], x0, y0, nPbW, nPbH,
                     s->sh.luma_weight_l1[current_mv.ref_idx[1]],
                     s->sh.luma_offset_l1[current_mv.ref_idx[1]]);
+        }
 
         if (s->ps.sps->chroma_format_idc) {
+#if RPI_INTER
+            if (s->enable_rpi) {
+                rpi_pred_c(s, x0_c, y0_c, nPbW_c, nPbH_c, current_mv.mv + 1,
+                  s->sh.chroma_weight_l1[current_mv.ref_idx[1]], s->sh.chroma_offset_l1[current_mv.ref_idx[1]],
+                  ref1->frame);
+                return;
+            }
+#endif
             chroma_mc_uni(s, dst1, s->frame->linesize[1], ref1->frame->data[1], ref1->frame->linesize[1],
                           1, x0_c, y0_c, nPbW_c, nPbH_c, &current_mv,
                           s->sh.chroma_weight_l1[current_mv.ref_idx[1]][0], s->sh.chroma_offset_l1[current_mv.ref_idx[1]][0]);
@@ -1805,11 +2821,31 @@
         int nPbW_c = nPbW >> s->ps.sps->hshift[1];
         int nPbH_c = nPbH >> s->ps.sps->vshift[1];
 
-        luma_mc_bi(s, dst0, s->frame->linesize[0], ref0->frame,
+#if RPI_INTER
+        if (s->enable_rpi) {
+            rpi_pred_y_b(s, x0, y0, nPbW, nPbH, &current_mv, ref0->frame, ref1->frame);
+        } else
+#endif
+        {
+            luma_mc_bi(s, dst0, s->frame->linesize[0], ref0->frame,
                    &current_mv.mv[0], x0, y0, nPbW, nPbH,
                    ref1->frame, &current_mv.mv[1], &current_mv);
+        }
 
         if (s->ps.sps->chroma_format_idc) {
+#if RPI_INTER
+          if (s->enable_rpi) {
+              rpi_pred_c_b(s, x0_c, y0_c, nPbW_c, nPbH_c,
+                           &current_mv,
+                           s->sh.chroma_weight_l0[current_mv.ref_idx[0]],
+                           s->sh.chroma_offset_l0[current_mv.ref_idx[0]],
+                           s->sh.chroma_weight_l1[current_mv.ref_idx[1]],
+                           s->sh.chroma_offset_l1[current_mv.ref_idx[1]],
+                           ref0->frame,
+                           ref1->frame);
+                return;
+            }
+#endif
             chroma_mc_bi(s, dst1, s->frame->linesize[1], ref0->frame, ref1->frame,
                          x0_c, y0_c, nPbW_c, nPbH_c, &current_mv, 0);
 
@@ -2084,7 +3120,9 @@
                 intra_prediction_unit_default_value(s, x0, y0, log2_cb_size);
                 ret = hls_pcm_sample(s, x0, y0, log2_cb_size);
                 if (s->ps.sps->pcm.loop_filter_disable_flag)
+                {
                     set_deblocking_bypass(s, x0, y0, log2_cb_size);
+                }
 
                 if (ret < 0)
                     return ret;
@@ -2307,6 +3345,529 @@
     lc->ctb_up_left_flag = ((x_ctb > 0) && (y_ctb > 0)  && (ctb_addr_in_slice-1 >= s->ps.sps->ctb_width) && (s->ps.pps->tile_id[ctb_addr_ts] == s->ps.pps->tile_id[s->ps.pps->ctb_addr_rs_to_ts[ctb_addr_rs-1 - s->ps.sps->ctb_width]]));
 }
 
+#ifdef RPI
+static void rpi_execute_dblk_cmds(HEVCContext *s)
+{
+    int n;
+    int job = s->pass1_job;
+    int ctb_size    = 1 << s->ps.sps->log2_ctb_size;
+    int (*p)[2] = s->dblk_cmds[job];
+    for(n = s->num_dblk_cmds[job]; n>0 ;n--,p++) {
+        ff_hevc_hls_filters(s, (*p)[0], (*p)[1], ctb_size);
+    }
+    s->num_dblk_cmds[job] = 0;
+}
+
+#if 0
+static void rpi_execute_transform(HEVCContext *s)
+{
+    int i=2;
+    int job = s->pass1_job;
+    /*int j;
+    int16_t *coeffs = s->coeffs_buf_arm[job][i];
+    for(j=s->num_coeffs[job][i]; j > 0; j-= 16*16, coeffs+=16*16) {
+        s->hevcdsp.idct[4-2](coeffs, 16);
+    }
+    i=3;
+    coeffs = s->coeffs_buf_arm[job][i] - s->num_coeffs[job][i];
+    for(j=s->num_coeffs[job][i]; j > 0; j-= 32*32, coeffs+=32*32) {
+        s->hevcdsp.idct[5-2](coeffs, 32);
+    }*/
+
+    rpi_cache_flush_one_gm_ptr(&s->coeffs_buf_accelerated[job], RPI_CACHE_FLUSH_MODE_WB_INVALIDATE);
+    s->vpu_id = vpu_post_code2( vpu_get_fn(), vpu_get_constants(), s->coeffs_buf_vc[job][2],
+                               s->num_coeffs[job][2] >> 8, s->coeffs_buf_vc[job][3] - sizeof(int16_t) * s->num_coeffs[job][3],
+                               s->num_coeffs[job][3] >> 10, 0, &s->coeffs_buf_accelerated[job]);
+    //vpu_execute_code( vpu_get_fn(), vpu_get_constants(), s->coeffs_buf_vc[2], s->num_coeffs[2] >> 8, s->coeffs_buf_vc[3], s->num_coeffs[3] >> 10, 0);
+    //gpu_cache_flush(&s->coeffs_buf_accelerated);
+    //vpu_wait(s->vpu_id);
+
+    for(i=0;i<4;i++)
+        s->num_coeffs[job][i] = 0;
+}
+#endif
+
+
+// I-pred, transform_and_add for all blocks types done here
+// All ARM
+static void rpi_execute_pred_cmds(HEVCContext * const s)
+{
+  int i;
+  int job = s->pass1_job;
+  const HEVCPredCmd *cmd = s->univ_pred_cmds[job];
+#ifdef RPI_WORKER
+  HEVCLocalContextIntra *lc = &s->HEVClcIntra;
+#else
+  HEVCLocalContext *lc = s->HEVClc;
+#endif
+
+  for(i = s->num_pred_cmds[job]; i > 0; i--, cmd++) {
+//      printf("i=%d cmd=%p job1=%d job0=%d\n",i,cmd,s->pass1_job,s->pass0_job);
+
+      switch (cmd->type)
+      {
+          case RPI_PRED_INTRA:
+              lc->tu.intra_pred_mode_c = lc->tu.intra_pred_mode = cmd->i_pred.mode;
+              lc->na.cand_bottom_left  = (cmd->na >> 4) & 1;
+              lc->na.cand_left         = (cmd->na >> 3) & 1;
+              lc->na.cand_up_left      = (cmd->na >> 2) & 1;
+              lc->na.cand_up           = (cmd->na >> 1) & 1;
+              lc->na.cand_up_right     = (cmd->na >> 0) & 1;
+              if (!rpi_sliced_frame(s->frame) || cmd->c_idx == 0)
+                  s->hpc.intra_pred[cmd->size - 2](s, cmd->i_pred.x, cmd->i_pred.y, cmd->c_idx);
+              else
+                  s->hpc.intra_pred_c[cmd->size - 2](s, cmd->i_pred.x, cmd->i_pred.y, cmd->c_idx);
+              break;
+
+          case RPI_PRED_ADD_RESIDUAL:
+              s->hevcdsp.transform_add[cmd->size - 2](cmd->ta.dst, (int16_t *)cmd->ta.buf, cmd->ta.stride);
+#ifdef RPI_PRECLEAR
+              memset(cmd->buf, 0, sizeof(int16_t) << (cmd->size * 2)); // Clear coefficients here while they are in the cache
+#endif
+              break;
+          case RPI_PRED_ADD_RESIDUAL_U:
+              s->hevcdsp.add_residual_u[cmd->size - 2](cmd->ta.dst, (int16_t *)cmd->ta.buf, cmd->ta.stride);
+              break;
+          case RPI_PRED_ADD_RESIDUAL_V:
+              s->hevcdsp.add_residual_v[cmd->size - 2](cmd->ta.dst, (int16_t *)cmd->ta.buf, cmd->ta.stride);
+              break;
+
+          case RPI_PRED_I_PCM:
+              pcm_extract(s, cmd->i_pcm.src, cmd->i_pcm.src_len, cmd->i_pcm.x, cmd->i_pcm.y, 1 << cmd->size);
+              break;
+
+          default:
+              av_log(NULL, AV_LOG_PANIC, "Bad command %d in worker pred Q\n", cmd->type);
+              abort();
+      }
+  }
+  s->num_pred_cmds[job] = 0;
+}
+
+// Do any inter-pred that we want to do in software
+// With both RPI_INTER_QPU && RPI_LUMA_QPU defined we should do nothing here
+// All ARM
+static void do_yc_inter_cmds(HEVCContext * const s, const HEVCMvCmd *cmd, unsigned int n, const int b_only)
+{
+    unsigned int cidx;
+    AVFrame myref;
+    AVFrame myref1;
+    struct MvField mymv;
+
+    for(; n>0 ; n--, cmd++) {
+        av_assert0(0);
+
+        switch(cmd->cmd) {
+        case RPI_CMD_LUMA_UNI:
+            if (b_only)
+                break;
+            myref.data[0] = cmd->src;
+            myref.linesize[0] = cmd->srcstride;
+            luma_mc_uni(s, cmd->dst, cmd->dststride, &myref, &cmd->mv, cmd->x_off, cmd->y_off, cmd->block_w, cmd->block_h, cmd->weight, cmd->offset);
+            break;
+        case RPI_CMD_LUMA_BI:
+            myref.data[0] = cmd->src;
+            myref.linesize[0] = cmd->srcstride;
+            myref1.data[0] = cmd->src1;
+            myref1.linesize[0] = cmd->srcstride1;
+            mymv.ref_idx[0] = cmd->ref_idx[0];
+            mymv.ref_idx[1] = cmd->ref_idx[1];
+            luma_mc_bi(s, cmd->dst, cmd->dststride,
+                       &myref, &cmd->mv, cmd->x_off, cmd->y_off, cmd->block_w, cmd->block_h,
+                       &myref1, &cmd->mv1, &mymv);
+            break;
+        case RPI_CMD_CHROMA_UNI:
+            if (b_only)
+                break;
+            mymv.mv[0] = cmd->mv;
+            chroma_mc_uni(s, cmd->dst,
+                          cmd->dststride, cmd->src, cmd->srcstride, 0,
+                          cmd->x_off, cmd->y_off, cmd->block_w, cmd->block_h, &mymv, cmd->weight, cmd->offset);
+            break;
+        case RPI_CMD_CHROMA_BI:
+        case RPI_CMD_CHROMA_BI+1:
+            cidx = cmd->cmd - RPI_CMD_CHROMA_BI;
+            myref.data[cidx+1] = cmd->src;
+            myref.linesize[cidx+1] = cmd->srcstride;
+            myref1.data[cidx+1] = cmd->src1;
+            myref1.linesize[cidx+1] = cmd->srcstride1;
+            mymv.ref_idx[0] = cmd->ref_idx[0];
+            mymv.ref_idx[1] = cmd->ref_idx[1];
+            mymv.mv[0] = cmd->mv;
+            mymv.mv[1] = cmd->mv1;
+            chroma_mc_bi(s, cmd->dst, cmd->dststride, &myref, &myref1,
+                         cmd->x_off, cmd->y_off, cmd->block_w, cmd->block_h, &mymv, cidx);
+            break;
+        }
+    }
+}
+
+static void rpi_execute_inter_cmds(HEVCContext *s, const int qpu_luma, const int qpu_chroma, const int luma_b_only, const int chroma_b_only)
+{
+    const int job = s->pass1_job;
+
+    if (!qpu_luma || luma_b_only)
+        do_yc_inter_cmds(s, s->unif_mv_cmds_y[job], s->num_mv_cmds_y[job], qpu_luma);
+    s->num_mv_cmds_y[job] = 0;
+    if (!qpu_chroma || chroma_b_only)
+        do_yc_inter_cmds(s, s->unif_mv_cmds_c[job], s->num_mv_cmds_c[job], qpu_chroma);
+    s->num_mv_cmds_c[job] = 0;
+}
+
+#endif
+
+#ifdef RPI
+// Set initial uniform job values & zero ctu_count
+static void rpi_begin(HEVCContext *s)
+{
+#if RPI_INTER
+    int job = s->pass0_job;
+    int i;
+
+    const uint16_t pic_width_y        = s->ps.sps->width;
+    const uint16_t pic_height_y       = s->ps.sps->height;
+
+    const uint16_t pic_width_c        = s->ps.sps->width >> s->ps.sps->hshift[1];
+    const uint16_t pic_height_c       = s->ps.sps->height >> s->ps.sps->vshift[1];
+
+    for(i=0; i < QPU_N_UV;i++) {
+        HEVCRpiChromaPred * const cp = s->jobs[job].chroma_mvs + i;
+        qpu_mc_pred_c_t * u = cp->qpu_mc_base;
+
+        // Chroma setup is a double block with L0 fetch
+        // and other stuff in the 1st block and L1 fetch
+        // in the 2nd along with a lot of dummy vars
+        // This could be packed a lot tighter but it would make
+        // L0, L1 management a lot harder
+
+        u->next_fn = 0;
+        u->next_src_x = 0;
+        u->next_src_y = 0;
+        u->next_src_base_c = 0;
+        u->s0.pic_cw = pic_width_c;
+        u->s0.pic_ch = pic_height_c;
+        u->s0.stride2 = rpi_sliced_frame_stride2(s->frame);
+        u->s0.stride1 = s->frame->linesize[1];
+        u->s0.wdenom = s->sh.chroma_log2_weight_denom + 6;
+        u->s0.dummy0 = 0;
+        cp->last_l0 = u;
+        ++u;
+
+        u->next_fn = 0;
+        u->next_src_x = 0;
+        u->next_src_y = 0;
+        u->next_src_base_c = 0;
+        u->s1.dummy0 = 0;
+        u->s1.dummy1 = 0;
+        u->s1.dummy2 = 0;
+        u->s1.dummy3 = 0;
+        u->s1.dummy4 = 0;
+        u->s1.dummy5 = 0;
+        cp->last_l1 = u;
+        ++u;
+
+        cp->load = 0;
+        cp->qpu_mc_curr = u;
+    }
+    s->curr_pred_c = NULL;
+
+    for(i=0;i < QPU_N_Y;i++) {
+        HEVCRpiLumaPred * const yp = s->jobs[job].luma_mvs + i;
+        qpu_mc_pred_y_t * y = yp->qpu_mc_base;
+
+        y->next_src1_x = 0;
+        y->next_src1_y = 0;
+        y->next_src1_base = 0;
+        y->next_src2_x = 0;
+        y->next_src2_y = 0;
+        y->next_src2_base = 0;
+        y->s.pic_h = pic_height_y;
+        y->s.pic_w = pic_width_y;
+        y->s.stride2 = rpi_sliced_frame_stride2(s->frame);
+        y->s.stride1 = s->frame->linesize[0];
+        y->s.wdenom = s->sh.luma_log2_weight_denom + 6;
+        y->s.dummy0 = 0;
+        y->next_fn = 0;
+        yp->last_lx = y;
+        ++y;
+
+        yp->load = 0;
+        yp->qpu_mc_curr = y;
+    }
+    s->curr_pred_y = NULL;
+    s->last_y8_p = NULL;
+    s->last_y8_lx = NULL;
+#endif
+    s->ctu_count = 0;
+}
+#endif
+
+
+#if RPI_INTER
+static unsigned int mc_terminate_y(HEVCContext * const s, const int job)
+{
+    unsigned int i;
+    const uint32_t exit_fn = qpu_fn(mc_exit);
+    const uint32_t exit_fn2 = qpu_fn(mc_interrupt_exit12);
+    unsigned int tc = 0;
+    HEVCRpiJob * const jb = s->jobs + job;
+
+    // Add final commands to Q
+    for(i = 0; i != QPU_N_Y; ++i) {
+        HEVCRpiLumaPred * const yp = jb->luma_mvs + i;
+        qpu_mc_pred_y_t *const px = yp->qpu_mc_curr - 1; // *** yp->last_lx;
+
+        // We will always have had L0 if we have L1 so only test L0
+        if (px != yp->qpu_mc_base)
+            tc = 1;
+
+        yp->qpu_mc_curr[-1].next_fn = (i != QPU_N_Y - 1) ? exit_fn : exit_fn2;  // Actual fn ptr
+
+        // Need to set the srcs for L0 & L1 to something that can be (pointlessly) prefetched
+        px->next_src1_x = MC_DUMMY_X;
+        px->next_src1_y = MC_DUMMY_Y;
+        px->next_src1_base = s->qpu_dummy_frame;
+        px->next_src2_x = MC_DUMMY_X;
+        px->next_src2_y = MC_DUMMY_Y;
+        px->next_src2_base = s->qpu_dummy_frame;
+
+        yp->last_lx = NULL;
+    }
+
+    return tc;
+}
+
+#define MC_EXIT_FN_C2(n) mc_interrupt_exit ## n ## c
+#define MC_EXIT_FN_C(n) MC_EXIT_FN_C2(n)
+
+static unsigned int mc_terminate_uv(HEVCContext * const s, const int job)
+{
+    unsigned int i;
+    const uint32_t exit_fn = qpu_fn(mc_exit_c);
+    const uint32_t exit_fn2 = qpu_fn(MC_EXIT_FN_C(QPU_N_UV));
+    unsigned int tc = 0;
+    HEVCRpiJob * const jb = s->jobs + job;
+
+    // Add final commands to Q
+    for(i = 0; i != QPU_N_UV; ++i) {
+        HEVCRpiChromaPred * const cp = jb->chroma_mvs + i;
+        qpu_mc_pred_c_t *const p0 = cp->last_l0;
+        qpu_mc_pred_c_t *const p1 = cp->last_l1;
+
+        // We will always have had L0 if we have L1 so only test L0
+        if (p0 != cp->qpu_mc_base)
+            tc = 1;
+
+        cp->qpu_mc_curr[-1].next_fn = (i != QPU_N_UV - 1) ? exit_fn : exit_fn2;  // Actual fn ptr
+
+        // Need to set the srcs for L0 & L1 to something that can be (pointlessly) prefetched
+        p0->next_src_x = MC_DUMMY_X;
+        p0->next_src_y = MC_DUMMY_Y;
+        p0->next_src_base_c = s->qpu_dummy_frame;
+        p1->next_src_x = MC_DUMMY_X;
+        p1->next_src_y = MC_DUMMY_Y;
+        p1->next_src_base_c = s->qpu_dummy_frame;;
+
+        cp->last_l0 = NULL;
+        cp->last_l1 = NULL;
+    }
+
+    return tc;
+}
+#endif
+
+#ifdef RPI
+
+
+static void flush_frame(HEVCContext *s,AVFrame *frame)
+{
+  rpi_cache_flush_env_t * rfe = rpi_cache_flush_init();
+  rpi_cache_flush_add_frame(rfe, frame, RPI_CACHE_FLUSH_MODE_WB_INVALIDATE);
+  rpi_cache_flush_finish(rfe);
+}
+
+
+// Core execution tasks
+static void worker_core(HEVCContext * const s)
+{
+    worker_global_env_t * const wg = &worker_global_env;
+    int arm_cost = 0;
+//    vpu_qpu_wait_h sync_c;
+    vpu_qpu_wait_h sync_y;
+    int qpu_luma = 0;
+    int qpu_chroma = 0;
+    int gpu_load;
+    int arm_load;
+    static const int arm_const_cost = 2;
+
+//    static int z = 0;
+
+    const int job = s->pass1_job;
+    unsigned int flush_start = 0;
+    unsigned int flush_count = 0;
+
+    const vpu_qpu_job_h vqj = vpu_qpu_job_new();
+    rpi_cache_flush_env_t * const rfe = rpi_cache_flush_init();
+
+    if (s->num_coeffs[job][3] + s->num_coeffs[job][2] != 0) {
+        vpu_qpu_job_add_vpu(vqj,
+            vpu_get_fn(),
+            vpu_get_constants(),
+            s->coeffs_buf_vc[job][2],
+            s->num_coeffs[job][2] >> 8,
+            s->coeffs_buf_vc[job][3] - sizeof(int16_t) * s->num_coeffs[job][3],
+            s->num_coeffs[job][3] >> 10,
+            0);
+
+        rpi_cache_flush_add_gm_ptr(rfe, s->coeffs_buf_accelerated + job, RPI_CACHE_FLUSH_MODE_WB_INVALIDATE);
+    }
+
+
+#if RPI_INTER
+    pthread_mutex_lock(&wg->lock);
+
+//    ++z;
+    gpu_load = vpu_qpu_current_load();
+    arm_load = avpriv_atomic_int_get(&wg->arm_load);
+#if 0 // Y_B_ONLY
+    qpu_luma =  gpu_load + 2 < arm_load;
+    qpu_chroma = gpu_load < arm_load + 8;
+#elif 0
+    qpu_luma =  gpu_load < arm_load + 2;
+    qpu_chroma = gpu_load < arm_load + 8;
+#else
+    qpu_chroma = 1;
+    qpu_luma = 1;
+#endif
+
+    arm_cost = !qpu_chroma * 2 + !qpu_luma * 3;
+    avpriv_atomic_int_add_and_fetch(&wg->arm_load, arm_cost + arm_const_cost);
+
+    wg->gpu_c += qpu_chroma;
+    wg->gpu_y += qpu_luma;
+    wg->arm_c += !qpu_chroma;
+    wg->arm_y += !qpu_luma;
+
+
+//    if ((z & 511) == 0) {
+//        printf("Arm load=%d, GPU=%d, chroma=%d/%d, luma=%d/%d    \n", arm_load, gpu_load, wg->gpu_c, wg->arm_c, wg->gpu_y, wg->arm_y);
+//    }
+
+
+    {
+        int (*d)[2] = s->dblk_cmds[job];
+        unsigned int high=(*d)[1];
+        int n;
+
+        flush_start = high;
+        for(n = s->num_dblk_cmds[job]; n>0 ;n--,d++) {
+            unsigned int y = (*d)[1];
+            flush_start = FFMIN(flush_start, y);
+            high=FFMAX(high,y);
+        }
+        // Avoid flushing past end of frame
+        flush_count = FFMIN(high + (1 << s->ps.sps->log2_ctb_size), s->frame->height) - flush_start;
+    }
+
+#if !DISABLE_CHROMA
+    if (qpu_chroma && mc_terminate_uv(s, job) != 0)
+    {
+        HEVCRpiJob * const jb = s->jobs + job;
+        const uint32_t code = qpu_fn(mc_setup_c);
+        uint32_t * p;
+        unsigned int i;
+        uint32_t mail_uv[QPU_N_UV * QPU_MAIL_EL_VALS];
+
+        for (p = mail_uv, i = 0; i != QPU_N_UV; ++i) {
+            *p++ = jb->chroma_mvs_gptr.vc + ((uint8_t *)jb->chroma_mvs[i].qpu_mc_base - jb->chroma_mvs_gptr.arm);
+            *p++ = code;
+        }
+
+        vpu_qpu_job_add_qpu(vqj, QPU_N_UV, 2, mail_uv);
+
+#if RPI_CACHE_UNIF_MVS
+        rpi_cache_flush_add_gm_ptr(rfe, &jb->chroma_mvs_gptr, RPI_CACHE_FLUSH_MODE_WB_INVALIDATE);
+#endif
+        rpi_cache_flush_add_frame_lines(rfe, s->frame, RPI_CACHE_FLUSH_MODE_WB_INVALIDATE,
+          flush_start, flush_count, s->ps.sps->vshift[1], 0, 1);
+    }
+#endif
+
+// We can take a sync here and try to locally overlap QPU processing with ARM
+// but testing showed a slightly negative benefit with noticable extra complexity
+//    vpu_qpu_job_add_sync_this(vqj, &sync_c);
+
+    if (qpu_luma && mc_terminate_y(s, job) != 0)
+    {
+        HEVCRpiJob * const jb = s->jobs + job;
+        const uint32_t code = qpu_fn(mc_setup);
+        uint32_t * p;
+        unsigned int i;
+        uint32_t mail_y[QPU_N_Y * QPU_MAIL_EL_VALS];
+
+        for (p = mail_y, i = 0; i != QPU_N_Y; ++i) {
+            *p++ = jb->luma_mvs_gptr.vc + ((uint8_t *)jb->luma_mvs[i].qpu_mc_base - jb->luma_mvs_gptr.arm);
+            *p++ = code;
+        }
+
+        vpu_qpu_job_add_qpu(vqj, QPU_N_Y, 4, mail_y);
+
+#if RPI_CACHE_UNIF_MVS
+        rpi_cache_flush_add_gm_ptr(rfe, &jb->luma_mvs_gptr, RPI_CACHE_FLUSH_MODE_WB_INVALIDATE);
+#endif
+        rpi_cache_flush_add_frame_lines(rfe, s->frame, RPI_CACHE_FLUSH_MODE_WB_INVALIDATE,
+          flush_start, flush_count, s->ps.sps->vshift[1], 1, 0);
+    }
+
+    pthread_mutex_unlock(&wg->lock);
+
+#endif
+
+    vpu_qpu_job_add_sync_this(vqj, &sync_y);
+
+    // Having accumulated some commands - do them
+    rpi_cache_flush_finish(rfe);
+    vpu_qpu_job_finish(vqj);
+
+    memset(s->num_coeffs[job], 0, sizeof(s->num_coeffs[job]));  //???? Surely we haven't done the smaller
+
+#if Y_B_ONLY
+    if (qpu_luma)
+        vpu_qpu_wait(&sync_y);
+#endif
+    // Perform inter prediction
+    rpi_execute_inter_cmds(s, qpu_luma, qpu_chroma, Y_B_ONLY, 0);
+
+    // Wait for transform completion
+
+    // Perform intra prediction and residual reconstruction
+    avpriv_atomic_int_add_and_fetch(&wg->arm_load, -arm_cost);
+#if Y_B_ONLY
+    if (!qpu_luma)
+        vpu_qpu_wait(&sync_y);
+#else
+    vpu_qpu_wait(&sync_y);
+#endif
+    rpi_execute_pred_cmds(s);
+
+    // Perform deblocking for CTBs in this row
+    rpi_execute_dblk_cmds(s);
+
+    avpriv_atomic_int_add_and_fetch(&wg->arm_load, -arm_const_cost);
+}
+
+static void rpi_do_all_passes(HEVCContext *s)
+{
+    // Do the various passes - common with the worker code
+    worker_core(s);
+    // Prepare next batch
+    rpi_begin(s);
+}
+
+
+
+#endif
+
 static int hls_decode_entry(AVCodecContext *avctxt, void *isFilterThread)
 {
     HEVCContext *s  = avctxt->priv_data;
@@ -2316,6 +3877,18 @@
     int y_ctb       = 0;
     int ctb_addr_ts = s->ps.pps->ctb_addr_rs_to_ts[s->sh.slice_ctb_addr_rs];
 
+#ifdef RPI
+    s->enable_rpi = s->ps.sps->bit_depth == 8 &&
+        s->frame->format == AV_PIX_FMT_SAND128 &&
+        !s->ps.pps->cross_component_prediction_enabled_flag;
+
+    if (!s->enable_rpi) {
+      if (s->ps.pps->cross_component_prediction_enabled_flag)
+        printf("Cross component\n");
+    }
+#endif
+    //printf("L0=%d L1=%d\n",s->sh.nb_refs[L1],s->sh.nb_refs[L1]);
+
     if (!ctb_addr_ts && s->sh.dependent_slice_segment_flag) {
         av_log(s->avctx, AV_LOG_ERROR, "Impossible initial tile.\n");
         return AVERROR_INVALIDDATA;
@@ -2329,6 +3902,14 @@
         }
     }
 
+#ifdef RPI_WORKER
+    s->pass0_job = 0;
+    s->pass1_job = 0;
+#endif
+#ifdef RPI
+    rpi_begin(s);
+#endif
+
     while (more_data && ctb_addr_ts < s->ps.sps->ctb_size) {
         int ctb_addr_rs = s->ps.pps->ctb_addr_ts_to_rs[ctb_addr_ts];
 
@@ -2336,6 +3917,7 @@
         y_ctb = (ctb_addr_rs / ((s->ps.sps->width + ctb_size - 1) >> s->ps.sps->log2_ctb_size)) << s->ps.sps->log2_ctb_size;
         hls_decode_neighbour(s, x_ctb, y_ctb, ctb_addr_ts);
 
+
         ff_hevc_cabac_init(s, ctb_addr_ts);
 
         hls_sao_param(s, x_ctb >> s->ps.sps->log2_ctb_size, y_ctb >> s->ps.sps->log2_ctb_size);
@@ -2344,7 +3926,52 @@
         s->deblock[ctb_addr_rs].tc_offset   = s->sh.tc_offset;
         s->filter_slice_edges[ctb_addr_rs]  = s->sh.slice_loop_filter_across_slices_enabled_flag;
 
+#if RPI_INTER
+        s->curr_pred_c = s->jobs[s->pass0_job].chroma_mvs + (s->ctu_count * QPU_N_GRP_UV) % QPU_N_UV;
+        s->curr_pred_y = s->jobs[s->pass0_job].luma_mvs + (s->ctu_count * QPU_N_GRP_Y) % QPU_N_Y;
+#endif
+
         more_data = hls_coding_quadtree(s, x_ctb, y_ctb, s->ps.sps->log2_ctb_size, 0);
+
+#ifdef RPI
+        if (s->enable_rpi) {
+          //av_assert0(s->num_dblk_cmds[s->pass0_job]>=0);
+          //av_assert0(s->num_dblk_cmds[s->pass0_job]<RPI_MAX_DEBLOCK_CMDS);
+          //av_assert0(s->pass0_job<RPI_MAX_JOBS);
+          //av_assert0(s->pass0_job>=0);
+          s->dblk_cmds[s->pass0_job][s->num_dblk_cmds[s->pass0_job]][0] = x_ctb;
+          s->dblk_cmds[s->pass0_job][s->num_dblk_cmds[s->pass0_job]++][1] = y_ctb;
+          s->ctu_count++;
+
+          if ( s->ctu_count >= s->max_ctu_count ) {
+#ifdef RPI_WORKER
+            if (s->used_for_ref)
+            {
+//              printf("%d %d/%d job=%d, x,y=%d,%d\n",s->ctu_count,s->num_dblk_cmds[s->pass0_job],RPI_MAX_DEBLOCK_CMDS,s->pass0_job, x_ctb, y_ctb);
+
+//                worker_wait(s);
+              // Split work load onto separate threads so we make as rapid progress as possible with this frame
+              // Pass on this job to worker thread
+              worker_submit_job(s);
+
+              // Make sure we have space to prepare the next job
+              worker_pass0_ready(s);
+
+              // Prepare the next batch of commands
+              rpi_begin(s);
+            } else {
+              // Non-ref frame so do it all on this thread
+              rpi_do_all_passes(s);
+            }
+#else
+            rpi_do_all_passes(s);
+#endif
+          }
+
+        }
+#endif
+
+
         if (more_data < 0) {
             s->tab_slice_address[ctb_addr_rs] = -1;
             return more_data;
@@ -2353,9 +3980,42 @@
 
         ctb_addr_ts++;
         ff_hevc_save_states(s, ctb_addr_ts);
+#ifdef RPI
+        if (s->enable_rpi)
+            continue;
+#endif
         ff_hevc_hls_filters(s, x_ctb, y_ctb, ctb_size);
     }
 
+#ifdef RPI
+
+#ifdef RPI_WORKER
+    // Wait for the worker to finish all its jobs
+    if (s->enable_rpi) {
+        worker_wait(s);
+    }
+#endif
+
+    // Finish off any half-completed rows
+    if (s->enable_rpi && s->ctu_count) {
+        rpi_do_all_passes(s);
+    }
+
+#if RPI_TSTATS
+    {
+        HEVCRpiStats *const ts = &s->tstats;
+
+        printf("=== P: xy00:%5d/%5d/%5d/%5d h16gl:%5d/%5d w8gl:%5d/%5d y8m:%d\n    B: xy00:%5d/%5d/%5d/%5d h16gl:%5d/%5d\n",
+               ts->y_pred1_xy, ts->y_pred1_x0, ts->y_pred1_y0, ts->y_pred1_x0y0,
+               ts->y_pred1_hgt16, ts->y_pred1_hle16, ts->y_pred1_wgt8, ts->y_pred1_wle8, ts->y_pred1_y8_merge,
+               ts->y_pred2_xy, ts->y_pred2_x0, ts->y_pred2_y0, ts->y_pred2_x0y0,
+               ts->y_pred2_hgt16, ts->y_pred2_hle16);
+        memset(ts, 0, sizeof(*ts));
+    }
+#endif
+
+#endif
+
     if (x_ctb + ctb_size >= s->ps.sps->width &&
         y_ctb + ctb_size >= s->ps.sps->height)
         ff_hevc_hls_filter(s, x_ctb, y_ctb, ctb_size);
@@ -2390,6 +4050,11 @@
     s = s1->sList[self_id];
     lc = s->HEVClc;
 
+#ifdef RPI
+    s->enable_rpi = 0;
+    //printf("Wavefront\n");
+#endif
+
     if(ctb_row) {
         ret = init_get_bits8(&lc->gb, s->data + s->sh.offset[ctb_row - 1], s->sh.size[ctb_row - 1]);
 
@@ -2770,6 +4435,32 @@
         if (ret < 0)
             return ret;
 
+        // The definition of _N unit types is "non-reference for other frames
+        // with the same temporal_id" so they may/will be ref frames for pics
+        // with a higher temporal_id.
+        s->used_for_ref = s->ps.sps->max_sub_layers > s->temporal_id + 1 ||
+            !(s->nal_unit_type == NAL_TRAIL_N ||
+                        s->nal_unit_type == NAL_TSA_N   ||
+                        s->nal_unit_type == NAL_STSA_N  ||
+                        s->nal_unit_type == NAL_RADL_N  ||
+                        s->nal_unit_type == NAL_RASL_N);
+
+#if DEBUG_DECODE_N
+        {
+            static int z = 0;
+            if (IS_IDR(s)) {
+                z = 1;
+            }
+            if (z != 0 && z++ > DEBUG_DECODE_N) {
+                s->is_decoded = 0;
+                break;
+            }
+        }
+#endif
+        if (!s->used_for_ref && s->avctx->skip_frame >= AVDISCARD_NONREF) {
+            s->is_decoded = 0;
+            break;
+        }
         if (s->max_ra == INT_MAX) {
             if (s->nal_unit_type == NAL_CRA_NUT || IS_BLA(s)) {
                 s->max_ra = s->poc;
@@ -2893,10 +4584,19 @@
         }
     }
 
-fail:
-    if (s->ref && s->threads_type == FF_THREAD_FRAME)
+fail:  // Also success path
+    if (s->ref && s->threads_type == FF_THREAD_FRAME) {
+#if RPI_INTER
+        rpi_flush_ref_frame_progress(s, &s->ref->tf, s->ps.sps->height);
+#endif
         ff_thread_report_progress(&s->ref->tf, INT_MAX, 0);
-
+    }
+#if RPI_INTER
+    else if (s->ref && s->enable_rpi) {
+      // When running single threaded we need to flush the whole frame
+      flush_frame(s,s->frame);
+    }
+#endif
     return ret;
 }
 
@@ -3067,6 +4767,41 @@
     return AVERROR(ENOMEM);
 }
 
+#ifdef RPI_WORKER
+static av_cold void hevc_init_worker(HEVCContext *s)
+{
+    int err;
+    pthread_cond_init(&s->worker_cond_head, NULL);
+    pthread_cond_init(&s->worker_cond_tail, NULL);
+    pthread_mutex_init(&s->worker_mutex, NULL);
+
+    s->worker_tail=0;
+    s->worker_head=0;
+    s->kill_worker=0;
+    err = pthread_create(&s->worker_thread, NULL, worker_start, s);
+    if (err) {
+        printf("Failed to create worker thread\n");
+        exit(-1);
+    }
+}
+
+static av_cold void hevc_exit_worker(HEVCContext *s)
+{
+    void *res;
+    s->kill_worker=1;
+    pthread_cond_broadcast(&s->worker_cond_tail);
+    pthread_join(s->worker_thread, &res);
+
+    pthread_cond_destroy(&s->worker_cond_head);
+    pthread_cond_destroy(&s->worker_cond_tail);
+    pthread_mutex_destroy(&s->worker_mutex);
+
+    s->worker_tail=0;
+    s->worker_head=0;
+    s->kill_worker=0;
+}
+#endif
+
 static av_cold int hevc_decode_free(AVCodecContext *avctx)
 {
     HEVCContext       *s = avctx->priv_data;
@@ -3078,6 +4813,29 @@
 
     av_freep(&s->cabac_state);
 
+#ifdef RPI
+
+#ifdef RPI_WORKER
+    hevc_exit_worker(s);
+#endif
+
+    for(i=0;i<RPI_MAX_JOBS;i++) {
+
+        av_freep(&s->unif_mv_cmds_y[i]);
+        av_freep(&s->unif_mv_cmds_c[i]);
+        av_freep(&s->univ_pred_cmds[i]);
+
+#if RPI_INTER
+        gpu_free(&s->jobs[i].chroma_mvs_gptr);
+        gpu_free(&s->jobs[i].luma_mvs_gptr);
+#endif
+    }
+
+    vpu_qpu_term();
+
+    av_rpi_zc_uninit(avctx);
+#endif
+
     for (i = 0; i < 3; i++) {
         av_freep(&s->sao_pixel_buffer_h[i]);
         av_freep(&s->sao_pixel_buffer_v[i]);
@@ -3119,10 +4877,25 @@
     return 0;
 }
 
+#ifdef RPI
+#ifdef RPI_PRECLEAR
+static av_cold void memclear16(int16_t *p, int n)
+{
+  vpu_execute_code( vpu_get_fn(), p, n, 0, 0, 0, 1);
+  //int i;
+  //for(i=0;i<n;i++)
+  //  p[i] = 0;
+}
+#endif
+#endif
+
 static av_cold int hevc_init_context(AVCodecContext *avctx)
 {
     HEVCContext *s = avctx->priv_data;
     int i;
+#ifdef RPI
+    unsigned int job;
+#endif
 
     s->avctx = avctx;
 
@@ -3132,6 +4905,77 @@
     s->HEVClcList[0] = s->HEVClc;
     s->sList[0] = s;
 
+#ifdef RPI
+    // Whilst FFmpegs init fn is only called once the close fn is called as
+    // many times as we have threads (init_thread_copy is called for the
+    // threads).  So to match init & term put the init here where it will be
+    // called by both init & copy
+    av_rpi_zc_init(avctx);
+
+    if (vpu_qpu_init() != 0)
+        goto fail;
+
+    for(job = 0; job < RPI_MAX_JOBS; job++) {
+        s->unif_mv_cmds_y[job] = av_mallocz(sizeof(HEVCMvCmd)*RPI_MAX_MV_CMDS_Y);
+        if (!s->unif_mv_cmds_y[job])
+            goto fail;
+        s->unif_mv_cmds_c[job] = av_mallocz(sizeof(HEVCMvCmd)*RPI_MAX_MV_CMDS_C);
+        if (!s->unif_mv_cmds_c[job])
+            goto fail;
+        s->univ_pred_cmds[job] = av_mallocz(sizeof(HEVCPredCmd)*RPI_MAX_PRED_CMDS);
+        if (!s->univ_pred_cmds[job])
+            goto fail;
+    }
+
+#if RPI_INTER
+    // We divide the image into blocks 256 wide and 64 high
+    // We support up to 2048 widths
+    // We compute the number of chroma motion vector commands for 4:4:4 format and 4x4 chroma blocks - assuming all blocks are B predicted
+    // Also add space for the startup command for each stream.
+
+    for (job = 0; job < RPI_MAX_JOBS; job++) {
+        HEVCRpiJob * const jb = s->jobs + job;
+#if RPI_CACHE_UNIF_MVS
+        gpu_malloc_cached(QPU_N_UV * UV_COMMANDS_PER_QPU * sizeof(qpu_mc_pred_c_t), &jb->chroma_mvs_gptr);
+        gpu_malloc_cached(QPU_N_Y  * Y_COMMANDS_PER_QPU  * sizeof(qpu_mc_pred_y_t), &jb->luma_mvs_gptr);
+#else
+        gpu_malloc_uncached(QPU_N_UV * UV_COMMANDS_PER_QPU * sizeof(qpu_mc_pred_c_t), &jb->chroma_mvs_gptr);
+        gpu_malloc_uncached(QPU_N_Y  * Y_COMMANDS_PER_QPU  * sizeof(qpu_mc_pred_y_t), &jb->luma_mvs_gptr);
+#endif
+
+        {
+            qpu_mc_pred_c_t * p = (qpu_mc_pred_c_t *)jb->chroma_mvs_gptr.arm;
+            for(i = 0; i < QPU_N_UV; i++) {
+                jb->chroma_mvs[i].qpu_mc_base = p;
+                jb->chroma_mvs[i].qpu_mc_curr = p;
+                p += UV_COMMANDS_PER_QPU;
+            }
+        }
+        {
+            qpu_mc_pred_y_t * p = (qpu_mc_pred_y_t *)jb->luma_mvs_gptr.arm;
+            for(i = 0; i < QPU_N_Y; i++) {
+                jb->luma_mvs[i].qpu_mc_base = p;
+                jb->luma_mvs[i].qpu_mc_curr = p;
+                p += Y_COMMANDS_PER_QPU;
+            }
+        }
+    }
+    s->qpu_filter_uv = qpu_fn(mc_filter_uv);
+    s->qpu_filter_uv_b0 = qpu_fn(mc_filter_uv_b0);
+    s->qpu_dummy_frame = qpu_fn(mc_setup_c);  // Use our code as a dummy frame
+    s->qpu_filter = qpu_fn(mc_filter);
+    s->qpu_filter_b = qpu_fn(mc_filter_b);
+#endif
+    //gpu_malloc_uncached(2048*64,&s->dummy);
+
+    s->enable_rpi = 0;
+
+#ifdef RPI_WORKER
+    hevc_init_worker(s);
+#endif
+
+#endif
+
     s->cabac_state = av_malloc(HEVC_CONTEXTS);
     if (!s->cabac_state)
         goto fail;
@@ -3346,9 +5190,9 @@
     }
 
     if((avctx->active_thread_type & FF_THREAD_FRAME) && avctx->thread_count > 1)
-            s->threads_type = FF_THREAD_FRAME;
-        else
-            s->threads_type = FF_THREAD_SLICE;
+        s->threads_type = FF_THREAD_FRAME;
+    else
+        s->threads_type = FF_THREAD_SLICE;
 
     return 0;
 }
@@ -3407,6 +5251,8 @@
     .update_thread_context = hevc_update_thread_context,
     .init_thread_copy      = hevc_init_thread_copy,
     .capabilities          = AV_CODEC_CAP_DR1 | AV_CODEC_CAP_DELAY |
+//                             0,
+//                             AV_CODEC_CAP_FRAME_THREADS,
                              AV_CODEC_CAP_SLICE_THREADS | AV_CODEC_CAP_FRAME_THREADS,
     .profiles              = NULL_IF_CONFIG_SMALL(ff_hevc_profiles),
 };
diff -Naur ffmpeg-3.2.4/libavcodec/hevc_cabac.c ffmpeg-3.2.4.patch/libavcodec/hevc_cabac.c
--- ffmpeg-3.2.4/libavcodec/hevc_cabac.c	2017-02-10 14:25:26.000000000 +0100
+++ ffmpeg-3.2.4.patch/libavcodec/hevc_cabac.c	2017-05-28 20:42:45.738088666 +0200
@@ -21,14 +21,76 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
+#define UNCHECKED_BITSTREAM_READER 1
+
 #include "libavutil/attributes.h"
 #include "libavutil/common.h"
 
-#include "cabac_functions.h"
 #include "hevc.h"
+#include "cabac_functions.h"
+
+#ifdef RPI
+#include "rpi_zc.h"
+#endif
+
+// BY22 is probably faster than simple bypass if the processor has
+// either a fast 32-bit divide or a fast 32x32->64[63:32] instruction
+// x86 has fast int divide
+// Arm doesn't have divide or general fast 64 bit, but does have the multiply
+// * Beware: ARCH_xxx isn't set if configure --disable-asm is used
+#define USE_BY22 (HAVE_FAST_64BIT || ARCH_ARM || ARCH_X86)
+// Use native divide if we have a fast one - otherwise use mpy 1/x
+// x86 has a fast integer divide - arm doesn't - unsure about other
+// architectures
+#define USE_BY22_DIV  ARCH_X86
+
+// Special case blocks with a single significant ceoff
+// Decreases the complexity of the code for a common case but increases the
+// code size.
+#define USE_N_END_1 1
+
+#if ARCH_ARM
+#include "arm/hevc_cabac.h"
+#endif
 
 #define CABAC_MAX_BIN 31
 
+
+#if USE_BY22 && !USE_BY22_DIV
+#define I(x) (uint32_t)((0x10000000000ULL / (uint64_t)(x)) + 1ULL)
+
+static const uint32_t cabac_by22_inv_range[256] = {
+                                                    0,      I(257), I(258), I(259),
+    I(260), I(261), I(262), I(263), I(264), I(265), I(266), I(267), I(268), I(269),
+    I(270), I(271), I(272), I(273), I(274), I(275), I(276), I(277), I(278), I(279),
+    I(280), I(281), I(282), I(283), I(284), I(285), I(286), I(287), I(288), I(289),
+    I(290), I(291), I(292), I(293), I(294), I(295), I(296), I(297), I(298), I(299),
+    I(300), I(301), I(302), I(303), I(304), I(305), I(306), I(307), I(308), I(309),
+    I(310), I(311), I(312), I(313), I(314), I(315), I(316), I(317), I(318), I(319),
+    I(320), I(321), I(322), I(323), I(324), I(325), I(326), I(327), I(328), I(329),
+    I(330), I(331), I(332), I(333), I(334), I(335), I(336), I(337), I(338), I(339),
+    I(340), I(341), I(342), I(343), I(344), I(345), I(346), I(347), I(348), I(349),
+    I(350), I(351), I(352), I(353), I(354), I(355), I(356), I(357), I(358), I(359),
+    I(360), I(361), I(362), I(363), I(364), I(365), I(366), I(367), I(368), I(369),
+    I(370), I(371), I(372), I(373), I(374), I(375), I(376), I(377), I(378), I(379),
+    I(380), I(381), I(382), I(383), I(384), I(385), I(386), I(387), I(388), I(389),
+    I(390), I(391), I(392), I(393), I(394), I(395), I(396), I(397), I(398), I(399),
+    I(400), I(401), I(402), I(403), I(404), I(405), I(406), I(407), I(408), I(409),
+    I(410), I(411), I(412), I(413), I(414), I(415), I(416), I(417), I(418), I(419),
+    I(420), I(421), I(422), I(423), I(424), I(425), I(426), I(427), I(428), I(429),
+    I(430), I(431), I(432), I(433), I(434), I(435), I(436), I(437), I(438), I(439),
+    I(440), I(441), I(442), I(443), I(444), I(445), I(446), I(447), I(448), I(449),
+    I(450), I(451), I(452), I(453), I(454), I(455), I(456), I(457), I(458), I(459),
+    I(460), I(461), I(462), I(463), I(464), I(465), I(466), I(467), I(468), I(469),
+    I(470), I(471), I(472), I(473), I(474), I(475), I(476), I(477), I(478), I(479),
+    I(480), I(481), I(482), I(483), I(484), I(485), I(486), I(487), I(488), I(489),
+    I(490), I(491), I(492), I(493), I(494), I(495), I(496), I(497), I(498), I(499),
+    I(500), I(501), I(502), I(503), I(504), I(505), I(506), I(507), I(508), I(509),
+    I(510), I(511)
+};
+#undef I
+#endif  // USE_BY22
+
 /**
  * number of bin by SyntaxElement.
  */
@@ -445,6 +507,211 @@
     { 28, 36, 43, 49, 54, 58, 61, 63, },
 };
 
+
+typedef struct
+{
+    uint16_t coeff;
+    uint16_t scale;
+} xy_off_t;
+
+#define XYT_C(x,y,t) ((x) + ((y) << (t)))
+#define SCALE_TRAFO(t) ((t) > 3 ? 3 : (t))
+#define SCALE_SHR(t) ((t) - SCALE_TRAFO(t))
+#define XYT_S(x,y,t) (((x) >> SCALE_SHR(t)) + (((y) >> SCALE_SHR(t)) << SCALE_TRAFO(t)))
+
+#define XYT(x,y,t) {XYT_C(x,y,t), XYT_S(x,y,t)}
+
+#define OFF_DIAG(t) {\
+    XYT(0,0,t), XYT(0,1,t), XYT(1,0,t), XYT(0,2,t),\
+    XYT(1,1,t), XYT(2,0,t), XYT(0,3,t), XYT(1,2,t),\
+    XYT(2,1,t), XYT(3,0,t), XYT(1,3,t), XYT(2,2,t),\
+    XYT(3,1,t), XYT(2,3,t), XYT(3,2,t), XYT(3,3,t)\
+}
+
+#define OFF_HORIZ(t) {\
+    XYT(0,0,t), XYT(1,0,t), XYT(2,0,t), XYT(3,0,t),\
+    XYT(0,1,t), XYT(1,1,t), XYT(2,1,t), XYT(3,1,t),\
+    XYT(0,2,t), XYT(1,2,t), XYT(2,2,t), XYT(3,2,t),\
+    XYT(0,3,t), XYT(1,3,t), XYT(2,3,t), XYT(3,3,t)\
+}
+
+#define OFF_VERT(t) {\
+    XYT(0,0,t), XYT(0,1,t), XYT(0,2,t), XYT(0,3,t),\
+    XYT(1,0,t), XYT(1,1,t), XYT(1,2,t), XYT(1,3,t),\
+    XYT(2,0,t), XYT(2,1,t), XYT(2,2,t), XYT(2,3,t),\
+    XYT(3,0,t), XYT(3,1,t), XYT(3,2,t), XYT(3,3,t)\
+}
+
+static const xy_off_t off_xys[3][4][16] =
+{
+    {OFF_DIAG(2), OFF_DIAG(3), OFF_DIAG(4), OFF_DIAG(5)},
+    {OFF_HORIZ(2), OFF_HORIZ(3), OFF_HORIZ(4), OFF_HORIZ(5)},
+    {OFF_VERT(2), OFF_VERT(3), OFF_VERT(4), OFF_VERT(5)}
+};
+
+
+// Helper fns
+#ifndef hevc_mem_bits32
+static av_always_inline uint32_t hevc_mem_bits32(const void * buf, const unsigned int offset)
+{
+    return AV_RB32((const uint8_t *)buf + (offset >> 3)) << (offset & 7);
+}
+#endif
+
+#if AV_GCC_VERSION_AT_LEAST(3,4) && !defined(hevc_clz32)
+#define hevc_clz32 hevc_clz32_builtin
+static av_always_inline unsigned int hevc_clz32_builtin(const uint32_t x)
+{
+    // __builtin_clz says it works on ints - so adjust if int is >32 bits long
+    return __builtin_clz(x) - (sizeof(int) * 8 - 32);
+}
+#endif
+
+// It is unlikely that we will ever need this but include for completeness
+#ifndef hevc_clz32
+static inline unsigned int hevc_clz32(unsigned int x)
+{
+    unsigned int n = 1;
+    if ((x & 0xffff0000) == 0) {
+        n += 16;
+        x <<= 16;
+    }
+    if ((x & 0xff000000) == 0) {
+        n += 8;
+        x <<= 8;
+    }
+    if ((x & 0xf0000000) == 0) {
+        n += 4;
+        x <<= 4;
+    }
+    if ((x & 0xc0000000) == 0) {
+        n += 2;
+        x <<= 2;
+    }
+    return n - ((x >> 31) & 1);
+}
+#endif
+
+
+#if !USE_BY22
+// If no by22 then _by22 functions will revert to normal and so _peek/_flush
+// will no longer be called but the setup calls will still exist and we want
+// to null them out
+#define bypass_start(s)
+#define bypass_finish(s)
+#else
+// Use BY22 for residual bypass block
+
+#define bypass_start(s) get_cabac_by22_start(&s->HEVClc->cc)
+#define bypass_finish(s) get_cabac_by22_finish(&s->HEVClc->cc)
+
+// BY22 notes that bypass is simply a divide into the bitstream and so we
+// can peek out large quantities of bits at once and treat the result as if
+// it was VLC.  In many cases this will lead to O(1) processing rather than
+// O(n) though the setup and teardown is sufficiently expensive that it is
+// only worth using if we expect to be dealing with more than a few bits
+// The definition of "a few bits" will vary from platform to platform but
+// tests on ARM show that it probably isn't worth it for a single coded
+// residual, but is for >1 - it also seems likely that if there are
+// more residuals then they are likely to be bigger and this will make the
+// O(1) nature of the code more worthwhile.
+
+
+#if !USE_BY22_DIV
+// * 1/x @ 32 bits gets us 22 bits of accuracy
+#define CABAC_BY22_PEEK_BITS  22
+#else
+// A real 32-bit divide gets us another bit
+// If we have a 64 bit int & a unit time divider then we should get a lot
+// of bits (55)  but that is untested and it is unclear if it would give
+// us a large advantage
+#define CABAC_BY22_PEEK_BITS  23
+#endif
+
+// Bypass block start
+// Must be called before _by22_peek is used as it sets the CABAC environment
+// into the correct state.  _by22_finish must be called to return to 'normal'
+// (i.e. non-bypass) cabac decoding
+static inline void get_cabac_by22_start(CABACContext * const c)
+{
+    const unsigned int bits = __builtin_ctz(c->low);
+    const uint32_t m = hevc_mem_bits32(c->bytestream, 0);
+    uint32_t x = (c->low << (22 - CABAC_BITS)) ^ ((m ^ 0x80000000U) >> (9 + CABAC_BITS - bits));
+#if !USE_BY22_DIV
+    const uint32_t inv = cabac_by22_inv_range[c->range & 0xff];
+#endif
+
+    c->bytestream -= (CABAC_BITS / 8);
+    c->by22.bits = bits;
+#if !USE_BY22_DIV
+    c->by22.range = c->range;
+    c->range = inv;
+#endif
+    c->low = x;
+}
+
+// Bypass block finish
+// Must be called at the end of the bypass block to return to normal operation
+static inline void get_cabac_by22_finish(CABACContext * const c)
+{
+    unsigned int used = c->by22.bits;
+    unsigned int bytes_used = (used / CABAC_BITS) * (CABAC_BITS / 8);
+    unsigned int bits_used = used & (CABAC_BITS == 16 ? 15 : 7);
+
+    c->bytestream += bytes_used + (CABAC_BITS / 8);
+    c->low = (((uint32_t)c->low >> (22 - CABAC_BITS + bits_used)) | 1) << bits_used;
+#if !USE_BY22_DIV
+    c->range = c->by22.range;
+#endif
+}
+
+// Peek bypass bits
+// _by22_start must be called before _by22_peek is called and _by22_flush
+// must be called afterwards to flush any used bits
+// The actual number of valid bits returned is
+// min(<coded bypass block length>, CABAC_BY22_PEEK_BITS). CABAC_BY22_PEEK_BITS
+// will be at least 22 which should be long enough for any prefix or suffix
+// though probably not long enough for the worst case combination
+#ifndef get_cabac_by22_peek
+static inline uint32_t get_cabac_by22_peek(const CABACContext * const c)
+{
+#if USE_BY22_DIV
+    return ((unsigned int)c->low / (unsigned int)c->range) << 9;
+#else
+    uint32_t x = c->low & ~1U;
+    const uint32_t inv = c->range;
+
+    if (inv != 0)
+        x = (uint32_t)(((uint64_t)x * (uint64_t)inv) >> 32);
+
+    return x << 1;
+#endif
+}
+#endif
+
+// Flush bypass bits peeked by _by22_peek
+// Flush n bypass bits. n must be >= 1 to guarantee correct operation
+// val is an unmodified copy of whatever _by22_peek returned
+#ifndef get_cabac_by22_flush
+static inline void get_cabac_by22_flush(CABACContext * c, const unsigned int n, const uint32_t val)
+{
+    // Subtract the bits used & reshift up to the top of the word
+#if USE_BY22_DIV
+    const uint32_t low = (((unsigned int)c->low << n) - (((val >> (32 - n)) * (unsigned int)c->range) << 23));
+#else
+    const uint32_t low = (((uint32_t)c->low << n) - (((val >> (32 - n)) * c->by22.range) << 23));
+#endif
+
+    // and refill lower bits
+    // We will probably OR over some existing bits but that doesn't matter
+    c->by22.bits += n;
+    c->low = low | (hevc_mem_bits32(c->bytestream, c->by22.bits) >> 9);
+}
+#endif
+
+#endif  // USE_BY22
+
+
 void ff_hevc_save_states(HEVCContext *s, int ctb_addr_ts)
 {
     if (s->ps.pps->entropy_coding_sync_enabled_flag &&
@@ -863,19 +1130,19 @@
     return GET_CABAC(elem_offset[CBF_LUMA] + !trafo_depth);
 }
 
-static int hevc_transform_skip_flag_decode(HEVCContext *s, int c_idx)
+static int hevc_transform_skip_flag_decode(HEVCContext *s, int c_idx_nz)
 {
-    return GET_CABAC(elem_offset[TRANSFORM_SKIP_FLAG] + !!c_idx);
+    return GET_CABAC(elem_offset[TRANSFORM_SKIP_FLAG] + c_idx_nz);
 }
 
-static int explicit_rdpcm_flag_decode(HEVCContext *s, int c_idx)
+static int explicit_rdpcm_flag_decode(HEVCContext *s, int c_idx_nz)
 {
-    return GET_CABAC(elem_offset[EXPLICIT_RDPCM_FLAG] + !!c_idx);
+    return GET_CABAC(elem_offset[EXPLICIT_RDPCM_FLAG] + c_idx_nz);
 }
 
-static int explicit_rdpcm_dir_flag_decode(HEVCContext *s, int c_idx)
+static int explicit_rdpcm_dir_flag_decode(HEVCContext *s, int c_idx_nz)
 {
-    return GET_CABAC(elem_offset[EXPLICIT_RDPCM_DIR_FLAG] + !!c_idx);
+    return GET_CABAC(elem_offset[EXPLICIT_RDPCM_DIR_FLAG] + c_idx_nz);
 }
 
 int ff_hevc_log2_res_scale_abs(HEVCContext *s, int idx) {
@@ -891,14 +1158,14 @@
     return GET_CABAC(elem_offset[RES_SCALE_SIGN_FLAG] + idx);
 }
 
-static av_always_inline void last_significant_coeff_xy_prefix_decode(HEVCContext *s, int c_idx,
+static av_always_inline void last_significant_coeff_xy_prefix_decode(HEVCContext *s, int c_idx_nz,
                                                    int log2_size, int *last_scx_prefix, int *last_scy_prefix)
 {
     int i = 0;
     int max = (log2_size << 1) - 1;
     int ctx_offset, ctx_shift;
 
-    if (!c_idx) {
+    if (!c_idx_nz) {
         ctx_offset = 3 * (log2_size - 2)  + ((log2_size - 1) >> 2);
         ctx_shift = (log2_size + 1) >> 2;
     } else {
@@ -929,22 +1196,16 @@
     return value;
 }
 
-static av_always_inline int significant_coeff_group_flag_decode(HEVCContext *s, int c_idx, int ctx_cg)
+static av_always_inline int significant_coeff_group_flag_decode(HEVCContext *s, int c_idx_nz, int ctx_cg)
 {
     int inc;
 
-    inc = FFMIN(ctx_cg, 1) + (c_idx>0 ? 2 : 0);
+    inc = (ctx_cg != 0) + (c_idx_nz << 1);
 
     return GET_CABAC(elem_offset[SIGNIFICANT_COEFF_GROUP_FLAG] + inc);
 }
-static av_always_inline int significant_coeff_flag_decode(HEVCContext *s, int x_c, int y_c,
-                                           int offset, const uint8_t *ctx_idx_map)
-{
-    int inc = ctx_idx_map[(y_c << 2) + x_c] + offset;
-    return GET_CABAC(elem_offset[SIGNIFICANT_COEFF_FLAG] + inc);
-}
 
-static av_always_inline int significant_coeff_flag_decode_0(HEVCContext *s, int c_idx, int offset)
+static av_always_inline int significant_coeff_flag_decode_0(HEVCContext *s, int offset)
 {
     return GET_CABAC(elem_offset[SIGNIFICANT_COEFF_FLAG] + offset);
 }
@@ -966,90 +1227,378 @@
     return GET_CABAC(elem_offset[COEFF_ABS_LEVEL_GREATER2_FLAG] + inc);
 }
 
-static av_always_inline int coeff_abs_level_remaining_decode(HEVCContext *s, int rc_rice_param)
+
+#if !USE_BY22
+#define coeff_abs_level_remaining_decode_bypass(s,r) coeff_abs_level_remaining_decode(s, r)
+#endif
+
+
+#ifndef coeff_abs_level_remaining_decode_bypass
+static int coeff_abs_level_remaining_decode_bypass(HEVCContext * const s, const unsigned int rice_param)
+{
+    CABACContext * const c = &s->HEVClc->cc;
+    uint32_t y;
+    unsigned int prefix;
+    unsigned int last_coeff_abs_level_remaining;
+    unsigned int n;
+
+    y = get_cabac_by22_peek(c);
+    prefix = hevc_clz32(~y);
+    // y << prefix will always have top bit 0
+
+    if (prefix < 3) {
+        const unsigned int suffix = (y << prefix) >> (31 - rice_param);
+        last_coeff_abs_level_remaining = (prefix << rice_param) + suffix;
+        n = prefix + 1 + rice_param;
+    }
+    else if (prefix * 2 + rice_param <= CABAC_BY22_PEEK_BITS + 2)
+    {
+        const uint32_t suffix = ((y << prefix) | 0x80000000) >> (34 - (prefix + rice_param));
+
+        last_coeff_abs_level_remaining = (2 << rice_param) + suffix;
+        n = prefix * 2 + rice_param - 2;
+    }
+    else {
+        unsigned int suffix;
+
+        get_cabac_by22_flush(c, prefix, y);
+        y = get_cabac_by22_peek(c);
+
+        suffix = (y | 0x80000000) >> (34 - (prefix + rice_param));
+        last_coeff_abs_level_remaining = (2 << rice_param) + suffix;
+        n = prefix + rice_param - 2;
+    }
+
+    get_cabac_by22_flush(c, n, y);
+
+    return last_coeff_abs_level_remaining;
+}
+#endif
+
+static int coeff_abs_level_remaining_decode(HEVCContext * const s, int rc_rice_param)
 {
+    CABACContext * const c = &s->HEVClc->cc;
     int prefix = 0;
     int suffix = 0;
     int last_coeff_abs_level_remaining;
     int i;
 
-    while (prefix < CABAC_MAX_BIN && get_cabac_bypass(&s->HEVClc->cc))
+    while (prefix < CABAC_MAX_BIN && get_cabac_bypass(c))
         prefix++;
     if (prefix == CABAC_MAX_BIN) {
         av_log(s->avctx, AV_LOG_ERROR, "CABAC_MAX_BIN : %d\n", prefix);
         return 0;
     }
+
     if (prefix < 3) {
         for (i = 0; i < rc_rice_param; i++)
-            suffix = (suffix << 1) | get_cabac_bypass(&s->HEVClc->cc);
+            suffix = (suffix << 1) | get_cabac_bypass(c);
         last_coeff_abs_level_remaining = (prefix << rc_rice_param) + suffix;
     } else {
         int prefix_minus3 = prefix - 3;
         for (i = 0; i < prefix_minus3 + rc_rice_param; i++)
-            suffix = (suffix << 1) | get_cabac_bypass(&s->HEVClc->cc);
+            suffix = (suffix << 1) | get_cabac_bypass(c);
         last_coeff_abs_level_remaining = (((1 << prefix_minus3) + 3 - 1)
                                               << rc_rice_param) + suffix;
     }
+
     return last_coeff_abs_level_remaining;
 }
 
-static av_always_inline int coeff_sign_flag_decode(HEVCContext *s, uint8_t nb)
-{
-    int i;
-    int ret = 0;
+#if !USE_BY22
+#define coeff_sign_flag_decode_bypass coeff_sign_flag_decode
+static inline uint32_t coeff_sign_flag_decode(HEVCContext * const s, const unsigned int nb)
+{
+    CABACContext * const c = &s->HEVClc->cc;
+    unsigned int i;
+    uint32_t ret = 0;
 
     for (i = 0; i < nb; i++)
-        ret = (ret << 1) | get_cabac_bypass(&s->HEVClc->cc);
-    return ret;
+        ret = (ret << 1) | get_cabac_bypass(c);
+
+    return ret << (32 - nb);
+}
+#endif
+
+#ifndef coeff_sign_flag_decode_bypass
+static inline uint32_t coeff_sign_flag_decode_bypass(HEVCContext * const s, const unsigned int nb)
+{
+    CABACContext * const c = &s->HEVClc->cc;
+    uint32_t y;
+    y = get_cabac_by22_peek(c);
+    get_cabac_by22_flush(c, nb, y);
+    return y & ~(0xffffffffU >> nb);
+}
+#endif
+
+
+#ifndef get_cabac_greater1_bits
+static inline unsigned int get_cabac_greater1_bits(CABACContext * const c, const unsigned int n,
+    uint8_t * const state0)
+{
+    unsigned int i;
+    unsigned int rv = 0;
+    for (i = 0; i != n; ++i) {
+        const unsigned int idx = rv != 0 ? 0 : i < 3 ? i + 1 : 3;
+        const unsigned int b = get_cabac(c, state0 + idx);
+        rv = (rv << 1) | b;
+    }
+    return rv;
+}
+#endif
+
+
+// N.B. levels returned are the values assuming coeff_abs_level_remaining
+// is uncoded, so 1 must be added if it is coded.  sum_abs also reflects
+// this version of events.
+static inline uint32_t get_greaterx_bits(HEVCContext * const s, const unsigned int n_end, int * const levels,
+    int * const pprev_subset_coded, int * const psum,
+    const unsigned int idx0_gt1, const unsigned int idx_gt2)
+{
+    CABACContext * const c = &s->HEVClc->cc;
+    uint8_t * const state0 = s->HEVClc->cabac_state + idx0_gt1;
+    uint8_t * const state_gt2 = s->HEVClc->cabac_state + idx_gt2;
+    unsigned int rv;
+    unsigned int i;
+    const unsigned int n = FFMIN(n_end, 8);
+
+    // Really this is i != n but the simple unconditional loop is cheaper
+    // and faster
+    for (i = 0; i != 8; ++i)
+        levels[i] = 1;
+
+    rv = get_cabac_greater1_bits(c, n, state0);
+
+    *pprev_subset_coded = 0;
+    *psum = n;
+
+    rv <<= (32 - n);
+    if (rv != 0)
+    {
+        *pprev_subset_coded = 1;
+        *psum = n + 1;
+        i = hevc_clz32(rv);
+        levels[i] = 2;
+        if (get_cabac(c, state_gt2) == 0)
+        {
+            // Unset first coded bit
+            rv &= ~(0x80000000U >> i);
+        }
+    }
+
+    if (n_end > 8) {
+        const unsigned int g8 = n_end - 8;
+        rv |= ((1 << g8) - 1) << (24 - g8);
+        for (i = 0; i != g8; ++i) {
+            levels[i + 8] = 0;
+        }
+    }
+
+    return rv;
+}
+
+// extended_precision_processing_flag must be false given we are
+// putting the result into a 16-bit array
+// So trans_coeff_level must fit in 16 bits too (7.4.9.1 definition of coeff_abs_level_remaining)
+// scale_m is uint8_t
+//
+// scale is [40 - 72] << [0..12] based on qp- worst case is (45 << 12)
+//   or it can be 2 (if we have transquant_bypass)
+// shift is set to one less than we really want but would normally be
+//   s->ps.sps->bit_depth (max 16, min 8) + log2_trafo_size (max 5, min 2?) - 5 = max 16 min 5?
+// however the scale shift is substracted from shift to a min 0 so scale_m worst = 45 << 6
+// This can still theoretically lead to overflow but the coding would have to be very odd (& inefficient)
+// to achieve it
+
+#ifndef trans_scale_sat
+static inline int trans_scale_sat(const int level, const unsigned int scale, const unsigned int scale_m, const unsigned int shift)
+{
+    return av_clip_int16((((level * (int)(scale * scale_m)) >> shift) + 1) >> 1);
+}
+#endif
+
+
+#ifndef update_rice
+static inline void update_rice(uint8_t * const stat_coeff,
+    const unsigned int last_coeff_abs_level_remaining,
+    const unsigned int c_rice_param)
+{
+    const unsigned int x = (last_coeff_abs_level_remaining << 1) >> c_rice_param;
+    if (x >= 6)
+        (*stat_coeff)++;
+    else if (x == 0 && *stat_coeff > 0)
+        (*stat_coeff)--;
+}
+#endif
+
+
+// n must be > 0 on entry
+#ifndef get_cabac_sig_coeff_flag_idxs
+static inline uint8_t * get_cabac_sig_coeff_flag_idxs(CABACContext * const c, uint8_t * const state0,
+    unsigned int n,
+    const uint8_t const * ctx_map,
+    uint8_t * p)
+{
+    do {
+        if (get_cabac(c, state0 + ctx_map[n]))
+            *p++ = n;
+    } while (--n != 0);
+    return p;
+}
+#endif
+
+
+static int get_sig_coeff_flag_idxs(CABACContext * const c, uint8_t * const state0,
+    unsigned int n,
+    const uint8_t const * ctx_map,
+    uint8_t * const flag_idx)
+{
+    int rv;
+
+    rv = get_cabac_sig_coeff_flag_idxs(c, state0, n, ctx_map, flag_idx) - flag_idx;
+
+    return rv;
+}
+
+#define H4x4(x0, x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15) {\
+     x0,  x1,  x2,  x3,\
+     x4,  x5,  x6,  x7,\
+     x8,  x9, x10, x11,\
+    x12, x13, x14, x15}
+
+#define V4x4(x0, x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15) {\
+     x0,  x4,  x8, x12,\
+     x1,  x5,  x9, x13,\
+     x2,  x6, x10, x14,\
+     x3,  x7, x11, x15}
+
+#define D4x4(x0, x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15) {\
+     x0,  x4,  x1,  x8,\
+     x5,  x2, x12,  x9,\
+     x6,  x3, x13, x10,\
+     x7, x14, x11, x15}
+
+
+static inline int next_subset(HEVCContext * const s, int i, const int c_idx_nz,
+    uint8_t * const significant_coeff_group_flag,
+    const uint8_t * const scan_x_cg, const uint8_t * const scan_y_cg,
+    int * const pPrev_sig)
+{
+    while (--i >= 0) {
+        unsigned int x_cg = scan_x_cg[i];
+        unsigned int y_cg = scan_y_cg[i];
+
+        // For the flag decode we only care about Z/NZ but
+        // we use the full Right + Down * 2 when calculating
+        // significant coeff flags so we obtain it here
+        //.
+        // The group flag array is one longer than it needs to
+        // be so we don't need to check for y_cg limits
+        unsigned int prev_sig = ((significant_coeff_group_flag[y_cg] >> (x_cg + 1)) & 1) |
+            (((significant_coeff_group_flag[y_cg + 1] >> x_cg) & 1) << 1);
+
+        if (i == 0 ||
+            significant_coeff_group_flag_decode(s, c_idx_nz, prev_sig))
+        {
+            significant_coeff_group_flag[y_cg] |= (1 << x_cg);
+            *pPrev_sig = prev_sig;
+            break;
+        }
+    }
+
+    return i;
+}
+
+#ifdef RPI
+static void rpi_add_residual(HEVCContext * const s,
+    const unsigned int log2_trafo_size, const unsigned int c_idx,
+    const unsigned int x0, const unsigned int y0, const int16_t * const coeffs)
+{
+    const AVFrame * const frame = s->frame;
+    unsigned int stride = frame->linesize[c_idx];
+    unsigned int x = x0 >> s->ps.sps->hshift[c_idx];
+    unsigned int y = y0 >> s->ps.sps->vshift[c_idx];
+    const int is_sliced = rpi_sliced_frame(frame);
+    uint8_t * dst = !is_sliced ?
+            s->frame->data[c_idx] + y * stride + (x << s->ps.sps->pixel_shift) :
+        c_idx == 0 ?
+            rpi_sliced_frame_pos_y(frame, x, y) :
+            rpi_sliced_frame_pos_c(frame, x, y);
+
+//    if (c_idx != 0) {
+//        return;
+//    }
+    if (s->enable_rpi) {
+        HEVCPredCmd * const cmd = s->univ_pred_cmds[s->pass0_job] + s->num_pred_cmds[s->pass0_job]++;
+        cmd->type = RPI_PRED_ADD_RESIDUAL + (is_sliced ? c_idx : 0);
+        cmd->size = log2_trafo_size;
+        cmd->c_idx = c_idx;
+        cmd->ta.buf = coeffs;
+        cmd->ta.dst = dst;
+        cmd->ta.stride = stride;
+    }
+    else if (!is_sliced || c_idx == 0) {
+        s->hevcdsp.transform_add[log2_trafo_size-2](dst, (int16_t *)coeffs, stride);
+    }
+    else if (c_idx == 1) {
+        s->hevcdsp.add_residual_u[log2_trafo_size-2](dst, (int16_t *)coeffs, stride);
+    }
+    else {
+        s->hevcdsp.add_residual_v[log2_trafo_size-2](dst, (int16_t *)coeffs, stride);
+    }
 }
+#endif
 
 void ff_hevc_hls_residual_coding(HEVCContext *s, int x0, int y0,
                                 int log2_trafo_size, enum ScanType scan_idx,
                                 int c_idx)
 {
-#define GET_COORD(offset, n)                                    \
-    do {                                                        \
-        x_c = (x_cg << 2) + scan_x_off[n];                      \
-        y_c = (y_cg << 2) + scan_y_off[n];                      \
-    } while (0)
-    HEVCLocalContext *lc = s->HEVClc;
-    int transform_skip_flag = 0;
+    HEVCLocalContext * const lc = s->HEVClc;
+    int trans_skip_or_bypass = lc->cu.cu_transquant_bypass_flag;
 
     int last_significant_coeff_x, last_significant_coeff_y;
-    int last_scan_pos;
-    int n_end;
     int num_coeff = 0;
-    int greater1_ctx = 1;
+    int prev_subset_coded = 0;
 
     int num_last_subset;
     int x_cg_last_sig, y_cg_last_sig;
 
-    const uint8_t *scan_x_cg, *scan_y_cg, *scan_x_off, *scan_y_off;
+    const uint8_t *scan_x_cg, *scan_y_cg;
+    const xy_off_t * scan_xy_off;
 
+#ifndef RPI
     ptrdiff_t stride = s->frame->linesize[c_idx];
     int hshift = s->ps.sps->hshift[c_idx];
     int vshift = s->ps.sps->vshift[c_idx];
-    uint8_t *dst = &s->frame->data[c_idx][(y0 >> vshift) * stride +
+    uint8_t * const dst = &s->frame->data[c_idx][(y0 >> vshift) * stride +
                                           ((x0 >> hshift) << s->ps.sps->pixel_shift)];
-    int16_t *coeffs = (int16_t*)(c_idx ? lc->edge_emu_buffer2 : lc->edge_emu_buffer);
-    uint8_t significant_coeff_group_flag[8][8] = {{0}};
+#endif
+#ifdef RPI
+    int use_vpu;
+#endif
+    int16_t *coeffs;
+    uint8_t significant_coeff_group_flag[9] = {0};  // Allow 1 final byte that is always zero
     int explicit_rdpcm_flag = 0;
     int explicit_rdpcm_dir_flag;
 
     int trafo_size = 1 << log2_trafo_size;
     int i;
-    int qp,shift,add,scale,scale_m;
+    int qp,shift,scale;
     static const uint8_t level_scale[] = { 40, 45, 51, 57, 64, 72 };
     const uint8_t *scale_matrix = NULL;
     uint8_t dc_scale;
     int pred_mode_intra = (c_idx == 0) ? lc->tu.intra_pred_mode :
                                          lc->tu.intra_pred_mode_c;
 
-    memset(coeffs, 0, trafo_size * trafo_size * sizeof(int16_t));
+    int prev_sig = 0;
+    const int c_idx_nz = (c_idx != 0);
+
+    int may_hide_sign;
+
 
     // Derive QP for dequant
     if (!lc->cu.cu_transquant_bypass_flag) {
-        static const int qp_c[] = { 29, 30, 31, 32, 33, 33, 34, 34, 35, 35, 36, 36, 37, 37 };
+        static const uint8_t qp_c[] = { 29, 30, 31, 32, 33, 33, 34, 34, 35, 35, 36, 36, 37, 37 };
         static const uint8_t rem6[51 + 4 * 6 + 1] = {
             0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 0, 1, 2,
             3, 4, 5, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5,
@@ -1065,9 +1614,19 @@
         };
         int qp_y = lc->qp_y;
 
+        may_hide_sign = s->ps.pps->sign_data_hiding_flag;
+
         if (s->ps.pps->transform_skip_enabled_flag &&
             log2_trafo_size <= s->ps.pps->log2_max_transform_skip_block_size) {
-            transform_skip_flag = hevc_transform_skip_flag_decode(s, c_idx);
+            int transform_skip_flag = hevc_transform_skip_flag_decode(s, c_idx_nz);
+            if (transform_skip_flag) {
+                trans_skip_or_bypass = 1;
+                if (lc->cu.pred_mode ==  MODE_INTRA  &&
+                    s->ps.sps->implicit_rdpcm_enabled_flag &&
+                    (pred_mode_intra == 10 || pred_mode_intra == 26)) {
+                    may_hide_sign = 0;
+                }
+            }
         }
 
         if (c_idx == 0) {
@@ -1100,39 +1659,76 @@
             qp += s->ps.sps->qp_bd_offset;
         }
 
-        shift    = s->ps.sps->bit_depth + log2_trafo_size - 5;
-        add      = 1 << (shift-1);
-        scale    = level_scale[rem6[qp]] << (div6[qp]);
-        scale_m  = 16; // default when no custom scaling lists.
-        dc_scale = 16;
+        // Shift is set to one less than will actually occur as the scale
+        // and saturate step adds 1 and then shifts right again
+        shift = s->ps.sps->bit_depth + log2_trafo_size - 6;
+        scale = level_scale[rem6[qp]];
+        if (div6[qp] >= shift) {
+            scale <<= (div6[qp] - shift);
+            shift = 0;
+        } else {
+            shift -= div6[qp];
+        }
 
-        if (s->ps.sps->scaling_list_enable_flag && !(transform_skip_flag && log2_trafo_size > 2)) {
+        if (s->ps.sps->scaling_list_enable_flag && !(trans_skip_or_bypass && log2_trafo_size > 2)) {
             const ScalingList *sl = s->ps.pps->scaling_list_data_present_flag ?
-            &s->ps.pps->scaling_list : &s->ps.sps->scaling_list;
+                &s->ps.pps->scaling_list : &s->ps.sps->scaling_list;
             int matrix_id = lc->cu.pred_mode != MODE_INTRA;
 
             matrix_id = 3 * matrix_id + c_idx;
 
             scale_matrix = sl->sl[log2_trafo_size - 2][matrix_id];
+            dc_scale = scale_matrix[0];
             if (log2_trafo_size >= 4)
                 dc_scale = sl->sl_dc[log2_trafo_size - 4][matrix_id];
         }
+        else
+        {
+            static const uint8_t sixteen_scale[64] = {
+                16, 16, 16, 16, 16, 16, 16, 16,
+                16, 16, 16, 16, 16, 16, 16, 16,
+                16, 16, 16, 16, 16, 16, 16, 16,
+                16, 16, 16, 16, 16, 16, 16, 16,
+                16, 16, 16, 16, 16, 16, 16, 16,
+                16, 16, 16, 16, 16, 16, 16, 16,
+                16, 16, 16, 16, 16, 16, 16, 16,
+                16, 16, 16, 16, 16, 16, 16, 16
+            };
+            scale_matrix = sixteen_scale;
+            dc_scale = 16;
+        }
     } else {
+        static const uint8_t unit_scale[64] = {
+            1, 1, 1, 1, 1, 1, 1, 1,
+            1, 1, 1, 1, 1, 1, 1, 1,
+            1, 1, 1, 1, 1, 1, 1, 1,
+            1, 1, 1, 1, 1, 1, 1, 1,
+            1, 1, 1, 1, 1, 1, 1, 1,
+            1, 1, 1, 1, 1, 1, 1, 1,
+            1, 1, 1, 1, 1, 1, 1, 1,
+            1, 1, 1, 1, 1, 1, 1, 1,
+        };
+        scale_matrix = unit_scale;
         shift        = 0;
-        add          = 0;
-        scale        = 0;
-        dc_scale     = 0;
+        scale        = 2;  // We will shift right to kill this
+        dc_scale     = 1;
+
+        may_hide_sign = 0;
     }
 
+
+
+
     if (lc->cu.pred_mode == MODE_INTER && s->ps.sps->explicit_rdpcm_enabled_flag &&
-        (transform_skip_flag || lc->cu.cu_transquant_bypass_flag)) {
-        explicit_rdpcm_flag = explicit_rdpcm_flag_decode(s, c_idx);
+        trans_skip_or_bypass) {
+        explicit_rdpcm_flag = explicit_rdpcm_flag_decode(s, c_idx_nz);
         if (explicit_rdpcm_flag) {
-            explicit_rdpcm_dir_flag = explicit_rdpcm_dir_flag_decode(s, c_idx);
+            may_hide_sign = 0;
+            explicit_rdpcm_dir_flag = explicit_rdpcm_dir_flag_decode(s, c_idx_nz);
         }
     }
 
-    last_significant_coeff_xy_prefix_decode(s, c_idx, log2_trafo_size,
+    last_significant_coeff_xy_prefix_decode(s, c_idx_nz, log2_trafo_size,
                                            &last_significant_coeff_x, &last_significant_coeff_y);
 
     if (last_significant_coeff_x > 3) {
@@ -1160,119 +1756,134 @@
         int last_x_c = last_significant_coeff_x & 3;
         int last_y_c = last_significant_coeff_y & 3;
 
-        scan_x_off = ff_hevc_diag_scan4x4_x;
-        scan_y_off = ff_hevc_diag_scan4x4_y;
         num_coeff = diag_scan4x4_inv[last_y_c][last_x_c];
-        if (trafo_size == 4) {
+
+        switch (log2_trafo_size) {
+        case 2:
             scan_x_cg = scan_1x1;
             scan_y_cg = scan_1x1;
-        } else if (trafo_size == 8) {
+            break;
+        case 3:
             num_coeff += diag_scan2x2_inv[y_cg_last_sig][x_cg_last_sig] << 4;
             scan_x_cg = diag_scan2x2_x;
             scan_y_cg = diag_scan2x2_y;
-        } else if (trafo_size == 16) {
+            break;
+        case 4:
             num_coeff += diag_scan4x4_inv[y_cg_last_sig][x_cg_last_sig] << 4;
             scan_x_cg = ff_hevc_diag_scan4x4_x;
             scan_y_cg = ff_hevc_diag_scan4x4_y;
-        } else { // trafo_size == 32
+            break;
+        case 5:
+        default:
             num_coeff += diag_scan8x8_inv[y_cg_last_sig][x_cg_last_sig] << 4;
             scan_x_cg = ff_hevc_diag_scan8x8_x;
             scan_y_cg = ff_hevc_diag_scan8x8_y;
+            break;
         }
         break;
     }
     case SCAN_HORIZ:
         scan_x_cg = horiz_scan2x2_x;
         scan_y_cg = horiz_scan2x2_y;
-        scan_x_off = horiz_scan4x4_x;
-        scan_y_off = horiz_scan4x4_y;
         num_coeff = horiz_scan8x8_inv[last_significant_coeff_y][last_significant_coeff_x];
         break;
     default: //SCAN_VERT
         scan_x_cg = horiz_scan2x2_y;
         scan_y_cg = horiz_scan2x2_x;
-        scan_x_off = horiz_scan4x4_y;
-        scan_y_off = horiz_scan4x4_x;
         num_coeff = horiz_scan8x8_inv[last_significant_coeff_x][last_significant_coeff_y];
         break;
     }
     num_coeff++;
     num_last_subset = (num_coeff - 1) >> 4;
 
-    for (i = num_last_subset; i >= 0; i--) {
-        int n, m;
-        int x_cg, y_cg, x_c, y_c, pos;
-        int implicit_non_zero_coeff = 0;
-        int64_t trans_coeff_level;
-        int prev_sig = 0;
-        int offset = i << 4;
-        int rice_init = 0;
-
-        uint8_t significant_coeff_flag_idx[16];
-        uint8_t nb_significant_coeff_flag = 0;
+    significant_coeff_group_flag[y_cg_last_sig] = 1 << x_cg_last_sig; // 1st subset always significant
 
-        x_cg = scan_x_cg[i];
-        y_cg = scan_y_cg[i];
+    scan_xy_off = off_xys[scan_idx][log2_trafo_size - 2];
 
-        if ((i < num_last_subset) && (i > 0)) {
-            int ctx_cg = 0;
-            if (x_cg < (1 << (log2_trafo_size - 2)) - 1)
-                ctx_cg += significant_coeff_group_flag[x_cg + 1][y_cg];
-            if (y_cg < (1 << (log2_trafo_size - 2)) - 1)
-                ctx_cg += significant_coeff_group_flag[x_cg][y_cg + 1];
-
-            significant_coeff_group_flag[x_cg][y_cg] =
-                significant_coeff_group_flag_decode(s, c_idx, ctx_cg);
-            implicit_non_zero_coeff = 1;
-        } else {
-            significant_coeff_group_flag[x_cg][y_cg] =
-            ((x_cg == x_cg_last_sig && y_cg == y_cg_last_sig) ||
-             (x_cg == 0 && y_cg == 0));
+    {
+        const unsigned int ccount = 1 << (log2_trafo_size * 2);
+#ifdef RPI
+        use_vpu = 0;
+        if (s->enable_rpi) {
+            use_vpu = !trans_skip_or_bypass && !lc->tu.cross_pf && log2_trafo_size>=4;
+            coeffs = rpi_alloc_coeff_buf(s, !use_vpu ? 0 : log2_trafo_size - 2, ccount);
+#if HAVE_NEON
+            rpi_zap_coeff_vals_neon(coeffs, log2_trafo_size - 2);
+#else
+            memset(coeffs, 0, ccount * sizeof(int16_t));
+#endif
+        }
+        else
+#endif
+        {
+            coeffs = (int16_t*)(c_idx_nz ? lc->edge_emu_buffer2 : lc->edge_emu_buffer);
+            memset(coeffs, 0, ccount * sizeof(int16_t));
         }
+    }
 
-        last_scan_pos = num_coeff - offset - 1;
+    i = num_last_subset;
+    do {
+        int implicit_non_zero_coeff = 0;
+        int n_end;
+
+        uint8_t significant_coeff_flag_idx[16];
+        unsigned int nb_significant_coeff_flag = 0;
 
         if (i == num_last_subset) {
+            // First time through
+            int last_scan_pos = num_coeff - (i << 4) - 1;
             n_end = last_scan_pos - 1;
             significant_coeff_flag_idx[0] = last_scan_pos;
             nb_significant_coeff_flag = 1;
         } else {
             n_end = 15;
+            implicit_non_zero_coeff = (i != 0);
         }
 
-        if (x_cg < ((1 << log2_trafo_size) - 1) >> 2)
-            prev_sig = !!significant_coeff_group_flag[x_cg + 1][y_cg];
-        if (y_cg < ((1 << log2_trafo_size) - 1) >> 2)
-            prev_sig += (!!significant_coeff_group_flag[x_cg][y_cg + 1] << 1);
-
-        if (significant_coeff_group_flag[x_cg][y_cg] && n_end >= 0) {
-            static const uint8_t ctx_idx_map[] = {
-                0, 1, 4, 5, 2, 3, 4, 5, 6, 6, 8, 8, 7, 7, 8, 8, // log2_trafo_size == 2
-                1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, // prev_sig == 0
-                2, 2, 2, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, // prev_sig == 1
-                2, 1, 0, 0, 2, 1, 0, 0, 2, 1, 0, 0, 2, 1, 0, 0, // prev_sig == 2
-                2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2  // default
+        if (n_end >= 0) {
+            static const uint8_t ctx_idx_maps_ts2[3][16] = {
+                D4x4(0, 1, 4, 5, 2, 3, 4, 5, 6, 6, 8, 8, 7, 7, 8, 8), // log2_trafo_size == 2
+                H4x4(0, 1, 4, 5, 2, 3, 4, 5, 6, 6, 8, 8, 7, 7, 8, 8), // log2_trafo_size == 2
+                V4x4(0, 1, 4, 5, 2, 3, 4, 5, 6, 6, 8, 8, 7, 7, 8, 8)  // log2_trafo_size == 2
+            };
+            static const uint8_t ctx_idx_maps[3][4][16] = {
+                {
+                    D4x4(1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0), // prev_sig == 0
+                    D4x4(2, 2, 2, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0), // prev_sig == 1
+                    D4x4(2, 1, 0, 0, 2, 1, 0, 0, 2, 1, 0, 0, 2, 1, 0, 0), // prev_sig == 2
+                    D4x4(2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2)  // prev_sig == 3, default
+                },
+                {
+                    H4x4(1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0), // prev_sig == 0
+                    H4x4(2, 2, 2, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0), // prev_sig == 1
+                    H4x4(2, 1, 0, 0, 2, 1, 0, 0, 2, 1, 0, 0, 2, 1, 0, 0), // prev_sig == 2
+                    H4x4(2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2)  // prev_sig == 3, default
+                },
+                {
+                    V4x4(1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0), // prev_sig == 0
+                    V4x4(2, 2, 2, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0), // prev_sig == 1
+                    V4x4(2, 1, 0, 0, 2, 1, 0, 0, 2, 1, 0, 0, 2, 1, 0, 0), // prev_sig == 2
+                    V4x4(2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2)  // prev_sig == 3, default
+                }
             };
             const uint8_t *ctx_idx_map_p;
             int scf_offset = 0;
-            if (s->ps.sps->transform_skip_context_enabled_flag &&
-                (transform_skip_flag || lc->cu.cu_transquant_bypass_flag)) {
-                ctx_idx_map_p = (uint8_t*) &ctx_idx_map[4 * 16];
-                if (c_idx == 0) {
-                    scf_offset = 40;
-                } else {
-                    scf_offset = 14 + 27;
-                }
+
+            if (s->ps.sps->transform_skip_context_enabled_flag && trans_skip_or_bypass) {
+                ctx_idx_map_p = ctx_idx_maps[0][3];
+                scf_offset = 40 + c_idx_nz;
             } else {
-                if (c_idx != 0)
+                if (c_idx_nz != 0)
                     scf_offset = 27;
+
                 if (log2_trafo_size == 2) {
-                    ctx_idx_map_p = (uint8_t*) &ctx_idx_map[0];
+                    ctx_idx_map_p = ctx_idx_maps_ts2[scan_idx];
                 } else {
-                    ctx_idx_map_p = (uint8_t*) &ctx_idx_map[(prev_sig + 1) << 4];
-                    if (c_idx == 0) {
-                        if ((x_cg > 0 || y_cg > 0))
+                    ctx_idx_map_p = ctx_idx_maps[scan_idx][prev_sig];
+                    if (!c_idx_nz) {
+                        if (i != 0)
                             scf_offset += 3;
+
                         if (log2_trafo_size == 3) {
                             scf_offset += (scan_idx == SCAN_DIAG) ? 9 : 15;
                         } else {
@@ -1286,34 +1897,30 @@
                     }
                 }
             }
-            for (n = n_end; n > 0; n--) {
-                x_c = scan_x_off[n];
-                y_c = scan_y_off[n];
-                if (significant_coeff_flag_decode(s, x_c, y_c, scf_offset, ctx_idx_map_p)) {
-                    significant_coeff_flag_idx[nb_significant_coeff_flag] = n;
-                    nb_significant_coeff_flag++;
+
+            if (n_end > 0) {
+                int cnt = get_sig_coeff_flag_idxs(&s->HEVClc->cc,
+                    s->HEVClc->cabac_state + elem_offset[SIGNIFICANT_COEFF_FLAG] + scf_offset,
+                    n_end, ctx_idx_map_p,
+                    significant_coeff_flag_idx + nb_significant_coeff_flag);
+
+                nb_significant_coeff_flag += cnt;
+                if (cnt != 0) {
                     implicit_non_zero_coeff = 0;
                 }
             }
+
             if (implicit_non_zero_coeff == 0) {
-                if (s->ps.sps->transform_skip_context_enabled_flag &&
-                    (transform_skip_flag || lc->cu.cu_transquant_bypass_flag)) {
-                    if (c_idx == 0) {
-                        scf_offset = 42;
-                    } else {
-                        scf_offset = 16 + 27;
-                    }
+                if (s->ps.sps->transform_skip_context_enabled_flag && trans_skip_or_bypass) {
+                    scf_offset = 42 + c_idx_nz;
                 } else {
                     if (i == 0) {
-                        if (c_idx == 0)
-                            scf_offset = 0;
-                        else
-                            scf_offset = 27;
+                        scf_offset = c_idx_nz ? 27 : 0;
                     } else {
                         scf_offset = 2 + scf_offset;
                     }
                 }
-                if (significant_coeff_flag_decode_0(s, c_idx, scf_offset) == 1) {
+                if (significant_coeff_flag_decode_0(s, scf_offset) == 1) {
                     significant_coeff_flag_idx[nb_significant_coeff_flag] = 0;
                     nb_significant_coeff_flag++;
                 }
@@ -1323,141 +1930,185 @@
             }
         }
 
-        n_end = nb_significant_coeff_flag;
+        if (nb_significant_coeff_flag != 0) {
+            const unsigned int gt1_idx_delta = (c_idx_nz << 2) |
+                ((i != 0 && !c_idx_nz) ? 2 : 0) |
+                prev_subset_coded;
+            const unsigned int idx0_gt1 = elem_offset[COEFF_ABS_LEVEL_GREATER1_FLAG] +
+                (gt1_idx_delta << 2);
+            const unsigned int idx_gt2 = elem_offset[COEFF_ABS_LEVEL_GREATER2_FLAG] +
+                gt1_idx_delta;
+
+            const unsigned int x_cg = scan_x_cg[i];
+            const unsigned int y_cg = scan_y_cg[i];
+            int16_t * const blk_coeffs = coeffs +
+                ((x_cg + (y_cg << log2_trafo_size)) << 2);
+            // This calculation is 'wrong' for log2_traffo_size == 2
+            // but that doesn't mattor as in this case x_cg & y_cg
+            // are always 0 so result is correct (0) anyway
+            const uint8_t * const blk_scale = scale_matrix +
+                (((x_cg + (y_cg << 3)) << (5 - log2_trafo_size)));
+
+            // * The following code block doesn't deal with these flags:
+            //   (nor did the one it replaces)
+            //
+            // cabac_bypass_alignment_enabled_flag
+            //    This should be easy but I can't find a test case
+            // extended_precision_processing_flag
+            //    This can extend the required precision past 16bits
+            //    so is probably tricky - also no example found yet
+
+#if USE_N_END_1
+            if (nb_significant_coeff_flag == 1) {
+                // There is a small gain to be had from special casing the single
+                // transform coefficient case.  The reduction in complexity
+                // makes up for the code duplicatioon.
+
+                int trans_coeff_level = 1;
+                int coeff_sign_flag;
+                int coded_val = 0;
+
+                // initialize first elem of coeff_bas_level_greater1_flag
+                prev_subset_coded = 0;
+
+                if (get_cabac(&s->HEVClc->cc, s->HEVClc->cabac_state + idx0_gt1 + 1)) {
+                    trans_coeff_level = 2;
+                    prev_subset_coded = 1;
+                    coded_val = get_cabac(&s->HEVClc->cc, s->HEVClc->cabac_state + idx_gt2);
+                }
 
+                // Probably not worth the overhead of starting by22 for just one value
+                coeff_sign_flag = get_cabac_bypass(&s->HEVClc->cc);
 
-        if (n_end) {
-            int first_nz_pos_in_cg;
-            int last_nz_pos_in_cg;
-            int c_rice_param = 0;
-            int first_greater1_coeff_idx = -1;
-            uint8_t coeff_abs_level_greater1_flag[8];
-            uint16_t coeff_sign_flag;
-            int sum_abs = 0;
-            int sign_hidden;
-            int sb_type;
-
-
-            // initialize first elem of coeff_bas_level_greater1_flag
-            int ctx_set = (i > 0 && c_idx == 0) ? 2 : 0;
-
-            if (s->ps.sps->persistent_rice_adaptation_enabled_flag) {
-                if (!transform_skip_flag && !lc->cu.cu_transquant_bypass_flag)
-                    sb_type = 2 * (c_idx == 0 ? 1 : 0);
-                else
-                    sb_type = 2 * (c_idx == 0 ? 1 : 0) + 1;
-                c_rice_param = lc->stat_coeff[sb_type] / 4;
-            }
+                if (coded_val)
+                {
+                    if (!s->ps.sps->persistent_rice_adaptation_enabled_flag) {
+                        trans_coeff_level = 3 + coeff_abs_level_remaining_decode(s, 0);
+                    } else {
+                        uint8_t * const stat_coeff =
+                            lc->stat_coeff + trans_skip_or_bypass + 2 - ((c_idx_nz) << 1);
+                        const unsigned int c_rice_param = *stat_coeff >> 2;
+                        const int last_coeff_abs_level_remaining = coeff_abs_level_remaining_decode(s, c_rice_param);
 
-            if (!(i == num_last_subset) && greater1_ctx == 0)
-                ctx_set++;
-            greater1_ctx = 1;
-            last_nz_pos_in_cg = significant_coeff_flag_idx[0];
-
-            for (m = 0; m < (n_end > 8 ? 8 : n_end); m++) {
-                int inc = (ctx_set << 2) + greater1_ctx;
-                coeff_abs_level_greater1_flag[m] =
-                    coeff_abs_level_greater1_flag_decode(s, c_idx, inc);
-                if (coeff_abs_level_greater1_flag[m]) {
-                    greater1_ctx = 0;
-                    if (first_greater1_coeff_idx == -1)
-                        first_greater1_coeff_idx = m;
-                } else if (greater1_ctx > 0 && greater1_ctx < 3) {
-                    greater1_ctx++;
+                        trans_coeff_level = 3 + last_coeff_abs_level_remaining;
+                        update_rice(stat_coeff, last_coeff_abs_level_remaining, c_rice_param);
+                    }
                 }
-            }
-            first_nz_pos_in_cg = significant_coeff_flag_idx[n_end - 1];
-
-            if (lc->cu.cu_transquant_bypass_flag ||
-                (lc->cu.pred_mode ==  MODE_INTRA  &&
-                 s->ps.sps->implicit_rdpcm_enabled_flag  &&  transform_skip_flag  &&
-                 (pred_mode_intra == 10 || pred_mode_intra  ==  26 )) ||
-                 explicit_rdpcm_flag)
-                sign_hidden = 0;
-            else
-                sign_hidden = (last_nz_pos_in_cg - first_nz_pos_in_cg >= 4);
 
-            if (first_greater1_coeff_idx != -1) {
-                coeff_abs_level_greater1_flag[first_greater1_coeff_idx] += coeff_abs_level_greater2_flag_decode(s, c_idx, ctx_set);
-            }
-            if (!s->ps.pps->sign_data_hiding_flag || !sign_hidden ) {
-                coeff_sign_flag = coeff_sign_flag_decode(s, nb_significant_coeff_flag) << (16 - nb_significant_coeff_flag);
-            } else {
-                coeff_sign_flag = coeff_sign_flag_decode(s, nb_significant_coeff_flag - 1) << (16 - (nb_significant_coeff_flag - 1));
+                {
+                    const xy_off_t * const xy_off = scan_xy_off + significant_coeff_flag_idx[0];
+                    const int k = (int32_t)(coeff_sign_flag << 31) >> 31;
+                    const unsigned int scale_m = blk_scale[xy_off->scale];
+
+                    blk_coeffs[xy_off->coeff] = trans_scale_sat(
+                        (trans_coeff_level ^ k) - k,  // Apply sign
+                        scale,
+                        i == 0 && xy_off->coeff == 0 ? dc_scale : scale_m,
+                        shift);
+                }
             }
-
-            for (m = 0; m < n_end; m++) {
-                n = significant_coeff_flag_idx[m];
-                GET_COORD(offset, n);
-                if (m < 8) {
-                    trans_coeff_level = 1 + coeff_abs_level_greater1_flag[m];
-                    if (trans_coeff_level == ((m == first_greater1_coeff_idx) ? 3 : 2)) {
-                        int last_coeff_abs_level_remaining = coeff_abs_level_remaining_decode(s, c_rice_param);
-
-                        trans_coeff_level += last_coeff_abs_level_remaining;
-                        if (trans_coeff_level > (3 << c_rice_param))
-                            c_rice_param = s->ps.sps->persistent_rice_adaptation_enabled_flag ? c_rice_param + 1 : FFMIN(c_rice_param + 1, 4);
-                        if (s->ps.sps->persistent_rice_adaptation_enabled_flag && !rice_init) {
-                            int c_rice_p_init = lc->stat_coeff[sb_type] / 4;
-                            if (last_coeff_abs_level_remaining >= (3 << c_rice_p_init))
-                                lc->stat_coeff[sb_type]++;
-                            else if (2 * last_coeff_abs_level_remaining < (1 << c_rice_p_init))
-                                if (lc->stat_coeff[sb_type] > 0)
-                                    lc->stat_coeff[sb_type]--;
-                            rice_init = 1;
+            else
+#endif
+            {
+                int sign_hidden = may_hide_sign;
+                int levels[16]; // Should be able to get away with int16_t but that fails some tests
+                uint32_t coeff_sign_flags;
+                uint32_t coded_vals = 0;
+                // Sum(abs(level[]))
+                // In fact we only need the bottom bit and in some future
+                // version that may be all we calculate
+                unsigned int sum_abs;
+
+                coded_vals = get_greaterx_bits(s, nb_significant_coeff_flag, levels,
+                    &prev_subset_coded, &sum_abs, idx0_gt1, idx_gt2);
+
+                if (significant_coeff_flag_idx[0] - significant_coeff_flag_idx[nb_significant_coeff_flag - 1] <= 3)
+                    sign_hidden = 0;
+
+                // -- Start bypass block
+
+                bypass_start(s);
+
+                coeff_sign_flags = coeff_sign_flag_decode_bypass(s, nb_significant_coeff_flag - sign_hidden);
+
+                if (coded_vals != 0)
+                {
+                    const int rice_adaptation_enabled = s->ps.sps->persistent_rice_adaptation_enabled_flag;
+                    uint8_t * stat_coeff = !rice_adaptation_enabled ? NULL :
+                        lc->stat_coeff + trans_skip_or_bypass + 2 - ((c_idx_nz) << 1);
+                    int c_rice_param = !rice_adaptation_enabled ? 0 : *stat_coeff >> 2;
+                    int * level = levels - 1;
+
+                    do {
+                        {
+                            const unsigned int z = hevc_clz32(coded_vals) + 1;
+                            level += z;
+                            coded_vals <<= z;
                         }
-                    }
-                } else {
-                    int last_coeff_abs_level_remaining = coeff_abs_level_remaining_decode(s, c_rice_param);
 
-                    trans_coeff_level = 1 + last_coeff_abs_level_remaining;
-                    if (trans_coeff_level > (3 << c_rice_param))
-                        c_rice_param = s->ps.sps->persistent_rice_adaptation_enabled_flag ? c_rice_param + 1 : FFMIN(c_rice_param + 1, 4);
-                    if (s->ps.sps->persistent_rice_adaptation_enabled_flag && !rice_init) {
-                        int c_rice_p_init = lc->stat_coeff[sb_type] / 4;
-                        if (last_coeff_abs_level_remaining >= (3 << c_rice_p_init))
-                            lc->stat_coeff[sb_type]++;
-                        else if (2 * last_coeff_abs_level_remaining < (1 << c_rice_p_init))
-                            if (lc->stat_coeff[sb_type] > 0)
-                                lc->stat_coeff[sb_type]--;
-                        rice_init = 1;
-                    }
+                        {
+                            const int last_coeff_abs_level_remaining = coeff_abs_level_remaining_decode_bypass(s, c_rice_param);
+                            const int trans_coeff_level = *level + last_coeff_abs_level_remaining + 1;
+
+                            sum_abs += last_coeff_abs_level_remaining + 1;
+                            *level = trans_coeff_level;
+
+                            if (stat_coeff != NULL)
+                                update_rice(stat_coeff, last_coeff_abs_level_remaining, c_rice_param);
+                            stat_coeff = NULL;
+
+                            if (trans_coeff_level > (3 << c_rice_param) &&
+                                (c_rice_param < 4 || rice_adaptation_enabled))
+                                ++c_rice_param;
+                        }
+                    } while (coded_vals != 0);
                 }
-                if (s->ps.pps->sign_data_hiding_flag && sign_hidden) {
-                    sum_abs += trans_coeff_level;
-                    if (n == first_nz_pos_in_cg && (sum_abs&1))
-                        trans_coeff_level = -trans_coeff_level;
+
+                // sign_hidden = 0 or 1 so we can combine the tests
+                if ((sign_hidden & sum_abs) != 0) {
+                    levels[nb_significant_coeff_flag - 1] = -levels[nb_significant_coeff_flag - 1];
                 }
-                if (coeff_sign_flag >> 15)
-                    trans_coeff_level = -trans_coeff_level;
-                coeff_sign_flag <<= 1;
-                if(!lc->cu.cu_transquant_bypass_flag) {
-                    if (s->ps.sps->scaling_list_enable_flag && !(transform_skip_flag && log2_trafo_size > 2)) {
-                        if(y_c || x_c || log2_trafo_size < 4) {
-                            switch(log2_trafo_size) {
-                                case 3: pos = (y_c << 3) + x_c; break;
-                                case 4: pos = ((y_c >> 1) << 3) + (x_c >> 1); break;
-                                case 5: pos = ((y_c >> 2) << 3) + (x_c >> 2); break;
-                                default: pos = (y_c << 2) + x_c; break;
-                            }
-                            scale_m = scale_matrix[pos];
-                        } else {
-                            scale_m = dc_scale;
-                        }
+
+                bypass_finish(s);
+
+                // -- Finish bypass block
+
+                // Scale loop
+                {
+                    int m = nb_significant_coeff_flag - 1;
+
+                    // Deal with DC component (if any) first
+                    if (i == 0 && significant_coeff_flag_idx[m] == 0)
+                    {
+                        const int k = (int32_t)(coeff_sign_flags << m) >> 31;
+                        blk_coeffs[0] = trans_scale_sat(
+                            (levels[m] ^ k) - k, scale, dc_scale, shift);
+                        --m;
                     }
-                    trans_coeff_level = (trans_coeff_level * (int64_t)scale * (int64_t)scale_m + add) >> shift;
-                    if(trans_coeff_level < 0) {
-                        if((~trans_coeff_level) & 0xFffffffffff8000)
-                            trans_coeff_level = -32768;
-                    } else {
-                        if(trans_coeff_level & 0xffffffffffff8000)
-                            trans_coeff_level = 32767;
+
+#if !USE_N_END_1
+                    // If N_END_1 set then m was at least 1 initially
+                    if (m >= 0)
+#endif
+                    {
+                        do {
+                            const xy_off_t * const xy_off = scan_xy_off +
+                                significant_coeff_flag_idx[m];
+                            const int k = (int32_t)(coeff_sign_flags << m) >> 31;
+
+                            blk_coeffs[xy_off->coeff] = trans_scale_sat(
+                                (levels[m] ^ k) - k,
+                                scale,
+                                blk_scale[xy_off->scale],
+                                shift);
+                        } while (--m >= 0);
                     }
                 }
-                coeffs[y_c * trafo_size + x_c] = trans_coeff_level;
+
             }
         }
-    }
+    } while ((i = next_subset(s, i, c_idx_nz,
+        significant_coeff_group_flag, scan_x_cg, scan_y_cg, &prev_sig)) >= 0);
 
     if (lc->cu.cu_transquant_bypass_flag) {
         if (explicit_rdpcm_flag || (s->ps.sps->implicit_rdpcm_enabled_flag &&
@@ -1467,7 +2118,7 @@
             s->hevcdsp.transform_rdpcm(coeffs, log2_trafo_size, mode);
         }
     } else {
-        if (transform_skip_flag) {
+        if (trans_skip_or_bypass) { // Must be trans_skip as we've already dealt with bypass
             int rot = s->ps.sps->transform_skip_rotation_enabled_flag &&
                       log2_trafo_size == 2 &&
                       lc->cu.pred_mode == MODE_INTRA;
@@ -1475,7 +2126,6 @@
                 for (i = 0; i < 8; i++)
                     FFSWAP(int16_t, coeffs[i], coeffs[16 - i - 1]);
             }
-
             s->hevcdsp.transform_skip(coeffs, log2_trafo_size);
 
             if (explicit_rdpcm_flag || (s->ps.sps->implicit_rdpcm_enabled_flag &&
@@ -1486,8 +2136,26 @@
                 s->hevcdsp.transform_rdpcm(coeffs, log2_trafo_size, mode);
             }
         } else if (lc->cu.pred_mode == MODE_INTRA && c_idx == 0 && log2_trafo_size == 2) {
-            s->hevcdsp.idct_4x4_luma(coeffs);
+           s->hevcdsp.idct_4x4_luma(coeffs);
         } else {
+#ifdef RPI
+            if (!use_vpu) {
+              int max_xy = FFMAX(last_significant_coeff_x, last_significant_coeff_y);
+              if (max_xy == 0) {
+                  s->hevcdsp.idct_dc[log2_trafo_size-2](coeffs);
+              } else {
+                  int col_limit = last_significant_coeff_x + last_significant_coeff_y + 4;
+                  if (max_xy < 4)
+                      col_limit = FFMIN(4, col_limit);
+                  else if (max_xy < 8)
+                      col_limit = FFMIN(8, col_limit);
+                  else if (max_xy < 12)
+                      col_limit = FFMIN(24, col_limit);
+
+                  s->hevcdsp.idct[log2_trafo_size-2](coeffs, col_limit);
+              }
+            }
+#else
             int max_xy = FFMAX(last_significant_coeff_x, last_significant_coeff_y);
             if (max_xy == 0)
                 s->hevcdsp.idct_dc[log2_trafo_size-2](coeffs);
@@ -1501,6 +2169,7 @@
                     col_limit = FFMIN(24, col_limit);
                 s->hevcdsp.idct[log2_trafo_size-2](coeffs, col_limit);
             }
+#endif
         }
     }
     if (lc->tu.cross_pf) {
@@ -1510,7 +2179,11 @@
             coeffs[i] = coeffs[i] + ((lc->tu.res_scale_val * coeffs_y[i]) >> 3);
         }
     }
+#ifdef RPI
+    rpi_add_residual(s, log2_trafo_size, c_idx, x0, y0, coeffs);
+#else
     s->hevcdsp.transform_add[log2_trafo_size-2](dst, coeffs, stride);
+#endif
 }
 
 void ff_hevc_hls_mvd_coding(HEVCContext *s, int x0, int y0, int log2_cb_size)
diff -Naur ffmpeg-3.2.4/libavcodec/hevcdsp.c ffmpeg-3.2.4.patch/libavcodec/hevcdsp.c
--- ffmpeg-3.2.4/libavcodec/hevcdsp.c	2016-06-27 01:54:29.000000000 +0200
+++ ffmpeg-3.2.4.patch/libavcodec/hevcdsp.c	2017-05-28 20:42:45.742088680 +0200
@@ -123,6 +123,120 @@
 #include "hevcdsp_template.c"
 #undef BIT_DEPTH
 
+static void hevc_deblocking_boundary_strengths(int pus, int dup, int in_inc, int out_inc,
+                                               int *curr_rpl0, int *curr_rpl1, int *neigh_rpl0, int *neigh_rpl1,
+                                               MvField *curr, MvField *neigh, uint8_t *bs)
+{
+    for (; pus > 0; pus--) {
+        int strength, out;
+        int curr_refL0 = curr_rpl0[curr->ref_idx[0]];
+        int curr_refL1 = curr_rpl1[curr->ref_idx[1]];
+        int neigh_refL0 = neigh_rpl0[neigh->ref_idx[0]];
+        int neigh_refL1 = neigh_rpl1[neigh->ref_idx[1]];
+
+#if 1 // This more directly matches the original implementation
+        if (curr->pred_flag == PF_BI &&  neigh->pred_flag == PF_BI) {
+            // same L0 and L1
+            if (curr_refL0 == neigh_refL0 &&
+                curr_refL0 == curr_refL1 &&
+                neigh_refL0 == neigh_refL1) {
+                if ((FFABS(neigh->mv[0].x - curr->mv[0].x) >= 4 || FFABS(neigh->mv[0].y - curr->mv[0].y) >= 4 ||
+                     FFABS(neigh->mv[1].x - curr->mv[1].x) >= 4 || FFABS(neigh->mv[1].y - curr->mv[1].y) >= 4) &&
+                    (FFABS(neigh->mv[1].x - curr->mv[0].x) >= 4 || FFABS(neigh->mv[1].y - curr->mv[0].y) >= 4 ||
+                     FFABS(neigh->mv[0].x - curr->mv[1].x) >= 4 || FFABS(neigh->mv[0].y - curr->mv[1].y) >= 4))
+                    strength = 1;
+                else
+                    strength = 0;
+            } else if (neigh_refL0 == curr_refL0 &&
+                       neigh_refL1 == curr_refL1) {
+                if (FFABS(neigh->mv[0].x - curr->mv[0].x) >= 4 || FFABS(neigh->mv[0].y - curr->mv[0].y) >= 4 ||
+                    FFABS(neigh->mv[1].x - curr->mv[1].x) >= 4 || FFABS(neigh->mv[1].y - curr->mv[1].y) >= 4)
+                    strength = 1;
+                else
+                    strength = 0;
+            } else if (neigh_refL1 == curr_refL0 &&
+                       neigh_refL0 == curr_refL1) {
+                if (FFABS(neigh->mv[1].x - curr->mv[0].x) >= 4 || FFABS(neigh->mv[1].y - curr->mv[0].y) >= 4 ||
+                    FFABS(neigh->mv[0].x - curr->mv[1].x) >= 4 || FFABS(neigh->mv[0].y - curr->mv[1].y) >= 4)
+                    strength = 1;
+                else
+                    strength = 0;
+            } else {
+                strength = 1;
+            }
+        } else if ((curr->pred_flag != PF_BI) && (neigh->pred_flag != PF_BI)){ // 1 MV
+            Mv curr_mv0, neigh_mv0;
+
+            if (curr->pred_flag & 1) {
+                curr_mv0   = curr->mv[0];
+            } else {
+                curr_mv0   = curr->mv[1];
+                curr_refL0 = curr_refL1;
+            }
+
+            if (neigh->pred_flag & 1) {
+                neigh_mv0   = neigh->mv[0];
+            } else {
+                neigh_mv0   = neigh->mv[1];
+                neigh_refL0 = neigh_refL1;
+            }
+
+            if (curr_refL0 == neigh_refL0) {
+                if (FFABS(curr_mv0.x - neigh_mv0.x) >= 4 || FFABS(curr_mv0.y - neigh_mv0.y) >= 4)
+                    strength = 1;
+                else
+                    strength = 0;
+            } else
+                strength = 1;
+        } else
+            strength = 1;
+#else // This has exactly the same effect, but is more suitable for vectorisation
+        Mv curr_mv[2];
+        Mv neigh_mv[2];
+        memcpy(curr_mv, curr->mv, sizeof curr_mv);
+        memcpy(neigh_mv, neigh->mv, sizeof neigh_mv);
+
+        if (!(curr->pred_flag & 2)) {
+            curr_mv[1] = curr_mv[0];
+            curr_refL1 = curr_refL0;
+        }
+        if (!(neigh->pred_flag & 2)) {
+            neigh_mv[1] = neigh_mv[0];
+            neigh_refL1 = neigh_refL0;
+        }
+        if (!(curr->pred_flag & 1)) {
+            curr_mv[0] = curr_mv[1];
+            curr_refL0 = curr_refL1;
+        }
+        if (!(neigh->pred_flag & 1)) {
+            neigh_mv[0] = neigh_mv[1];
+            neigh_refL0 = neigh_refL1;
+        }
+
+        strength = 1;
+
+        strength &= (neigh_refL0 != curr_refL0) | (neigh_refL1 != curr_refL1) |
+                (FFABS(neigh_mv[0].x - curr_mv[0].x) >= 4) | (FFABS(neigh_mv[0].y - curr_mv[0].y) >= 4) |
+                (FFABS(neigh_mv[1].x - curr_mv[1].x) >= 4) | (FFABS(neigh_mv[1].y - curr_mv[1].y) >= 4);
+
+        strength &= (neigh_refL1 != curr_refL0) | (neigh_refL0 != curr_refL1) |
+                (FFABS(neigh_mv[1].x - curr_mv[0].x) >= 4) | (FFABS(neigh_mv[1].y - curr_mv[0].y) >= 4) |
+                (FFABS(neigh_mv[0].x - curr_mv[1].x) >= 4) | (FFABS(neigh_mv[0].y - curr_mv[1].y) >= 4);
+
+        strength |= (((curr->pred_flag + 1) ^ (neigh->pred_flag + 1)) >> 2);
+#endif
+
+        curr += in_inc / sizeof (MvField);
+        neigh += in_inc / sizeof (MvField);
+
+        for (out = dup; out > 0; out--)
+        {
+            *bs = strength;
+            bs += out_inc;
+        }
+    }
+}
+
 void ff_hevc_dsp_init(HEVCDSPContext *hevcdsp, int bit_depth)
 {
 #undef FUNC
@@ -193,6 +307,16 @@
     PEL_FUNC(put_hevc_qpel_bi_w, 1, 0, put_hevc_qpel_bi_w_v, depth);          \
     PEL_FUNC(put_hevc_qpel_bi_w, 1, 1, put_hevc_qpel_bi_w_hv, depth)
 
+#ifndef RPI
+#define SLICED_LOOP_FILTERS(depth)
+#else
+#define SLICED_LOOP_FILTERS(depth)\
+    hevcdsp->hevc_v_loop_filter_luma2 = FUNC(hevc_v_loop_filter_luma2, depth); \
+    hevcdsp->hevc_h_loop_filter_uv    = FUNC(hevc_h_loop_filter_uv, depth);    \
+    hevcdsp->hevc_v_loop_filter_uv2   = FUNC(hevc_v_loop_filter_uv2, depth)
+#endif
+
+
 #define HEVC_DSP(depth)                                                     \
     hevcdsp->put_pcm                = FUNC(put_pcm, depth);                 \
     hevcdsp->transform_add[0]       = FUNC(transform_add4x4, depth);        \
@@ -200,6 +324,15 @@
     hevcdsp->transform_add[2]       = FUNC(transform_add16x16, depth);      \
     hevcdsp->transform_add[3]       = FUNC(transform_add32x32, depth);      \
     hevcdsp->transform_skip         = FUNC(transform_skip, depth);          \
+    hevcdsp->put_pcm_c              = FUNC(put_pcm_c, depth);                 \
+    hevcdsp->add_residual_u[0]      = FUNC(add_residual4x4_u, depth);         \
+    hevcdsp->add_residual_u[1]      = FUNC(add_residual8x8_u, depth);         \
+    hevcdsp->add_residual_u[2]      = FUNC(add_residual16x16_u, depth);       \
+    hevcdsp->add_residual_u[3]      = FUNC(add_residual32x32_u, depth);       \
+    hevcdsp->add_residual_v[0]      = FUNC(add_residual4x4_v, depth);         \
+    hevcdsp->add_residual_v[1]      = FUNC(add_residual8x8_v, depth);         \
+    hevcdsp->add_residual_v[2]      = FUNC(add_residual16x16_v, depth);       \
+    hevcdsp->add_residual_v[3]      = FUNC(add_residual32x32_v, depth);       \
     hevcdsp->transform_rdpcm        = FUNC(transform_rdpcm, depth);         \
     hevcdsp->idct_4x4_luma          = FUNC(transform_4x4_luma, depth);      \
     hevcdsp->idct[0]                = FUNC(idct_4x4, depth);                \
@@ -225,6 +358,19 @@
     hevcdsp->sao_edge_restore[0] = FUNC(sao_edge_restore_0, depth);            \
     hevcdsp->sao_edge_restore[1] = FUNC(sao_edge_restore_1, depth);            \
                                                                                \
+    hevcdsp->sao_band_filter_c[0] =                                            \
+    hevcdsp->sao_band_filter_c[1] =                                            \
+    hevcdsp->sao_band_filter_c[2] =                                            \
+    hevcdsp->sao_band_filter_c[3] =                                            \
+    hevcdsp->sao_band_filter_c[4] = FUNC(sao_band_filter_c, depth);            \
+    hevcdsp->sao_edge_filter_c[0] =                                            \
+    hevcdsp->sao_edge_filter_c[1] =                                            \
+    hevcdsp->sao_edge_filter_c[2] =                                            \
+    hevcdsp->sao_edge_filter_c[3] =                                            \
+    hevcdsp->sao_edge_filter_c[4] = FUNC(sao_edge_filter_c, depth);            \
+    hevcdsp->sao_edge_restore_c[0] = FUNC(sao_edge_restore_c_0, depth);        \
+    hevcdsp->sao_edge_restore_c[1] = FUNC(sao_edge_restore_c_1, depth);        \
+                                                                               \
     QPEL_FUNCS(depth);                                                         \
     QPEL_UNI_FUNCS(depth);                                                     \
     QPEL_BI_FUNCS(depth);                                                      \
@@ -232,6 +378,7 @@
     EPEL_UNI_FUNCS(depth);                                                     \
     EPEL_BI_FUNCS(depth);                                                      \
                                                                                \
+    SLICED_LOOP_FILTERS(depth);                                                \
     hevcdsp->hevc_h_loop_filter_luma     = FUNC(hevc_h_loop_filter_luma, depth);   \
     hevcdsp->hevc_v_loop_filter_luma     = FUNC(hevc_v_loop_filter_luma, depth);   \
     hevcdsp->hevc_h_loop_filter_chroma   = FUNC(hevc_h_loop_filter_chroma, depth); \
@@ -257,6 +404,8 @@
         break;
     }
 
+    hevcdsp->hevc_deblocking_boundary_strengths = hevc_deblocking_boundary_strengths;
+
     if (ARCH_X86)
         ff_hevc_dsp_init_x86(hevcdsp, bit_depth);
     if (ARCH_ARM)
diff -Naur ffmpeg-3.2.4/libavcodec/hevcdsp.h ffmpeg-3.2.4.patch/libavcodec/hevcdsp.h
--- ffmpeg-3.2.4/libavcodec/hevcdsp.h	2016-06-27 01:54:29.000000000 +0200
+++ ffmpeg-3.2.4.patch/libavcodec/hevcdsp.h	2017-05-28 20:42:45.743088684 +0200
@@ -42,11 +42,26 @@
     uint8_t type_idx[3];    ///< sao_type_idx
 } SAOParams;
 
+typedef struct Mv {
+    int16_t x;  ///< horizontal component of motion vector
+    int16_t y;  ///< vertical component of motion vector
+} Mv;
+
+typedef struct MvField {
+    DECLARE_ALIGNED(4, Mv, mv)[2];
+    int8_t ref_idx[2];
+    int8_t pred_flag;
+} MvField;
+
 typedef struct HEVCDSPContext {
     void (*put_pcm)(uint8_t *_dst, ptrdiff_t _stride, int width, int height,
                     struct GetBitContext *gb, int pcm_bit_depth);
+    void (*put_pcm_c)(uint8_t *_dst, ptrdiff_t _stride, int width, int height,
+                    struct GetBitContext *gb, int pcm_bit_depth);
 
-    void (*transform_add[4])(uint8_t *_dst, int16_t *coeffs, ptrdiff_t _stride);
+    void (*transform_add[4])(uint8_t *dst, int16_t *res, ptrdiff_t stride);
+    void (*add_residual_u[4])(uint8_t *dst, int16_t *res, ptrdiff_t stride);
+    void (*add_residual_v[4])(uint8_t *dst, int16_t *res, ptrdiff_t stride);
 
     void (*transform_skip)(int16_t *coeffs, int16_t log2_size);
 
@@ -60,14 +75,23 @@
 
     void (*sao_band_filter[5])(uint8_t *_dst, uint8_t *_src, ptrdiff_t _stride_dst, ptrdiff_t _stride_src,
                                int16_t *sao_offset_val, int sao_left_class, int width, int height);
+    void (*sao_band_filter_c[5])(uint8_t *_dst, const uint8_t *_src, ptrdiff_t _stride_dst, ptrdiff_t _stride_src,
+                               const int16_t *sao_offset_val_u, int sao_left_class_u,
+                               const int16_t *sao_offset_val_v, int sao_left_class_v,
+                               int width, int height);
 
     /* implicit stride_src parameter has value of 2 * MAX_PB_SIZE + AV_INPUT_BUFFER_PADDING_SIZE */
     void (*sao_edge_filter[5])(uint8_t *_dst /* align 16 */, uint8_t *_src /* align 32 */, ptrdiff_t stride_dst,
                                int16_t *sao_offset_val, int sao_eo_class, int width, int height);
+    void (*sao_edge_filter_c[5])(uint8_t *_dst /* align 16 */, const uint8_t *_src /* align 32 */, ptrdiff_t stride_dst,
+                               const int16_t *sao_offset_val_u, const int16_t *sao_offset_val_v, int sao_eo_class, int width, int height);
 
     void (*sao_edge_restore[2])(uint8_t *_dst, uint8_t *_src, ptrdiff_t _stride_dst, ptrdiff_t _stride_src,
                                 struct SAOParams *sao, int *borders, int _width, int _height, int c_idx,
                                 uint8_t *vert_edge, uint8_t *horiz_edge, uint8_t *diag_edge);
+    void (*sao_edge_restore_c[2])(uint8_t *_dst, uint8_t *_src, ptrdiff_t _stride_dst, ptrdiff_t _stride_src,
+                                struct SAOParams *sao, int *borders, int _width, int _height, int c_idx,
+                                uint8_t *vert_edge, uint8_t *horiz_edge, uint8_t *diag_edge);
 
     void (*put_hevc_qpel[10][2][2])(int16_t *dst, uint8_t *src, ptrdiff_t srcstride,
                                     int height, intptr_t mx, intptr_t my, int width);
@@ -120,6 +144,22 @@
     void (*hevc_v_loop_filter_chroma_c)(uint8_t *pix, ptrdiff_t stride,
                                         int32_t *tc, uint8_t *no_p,
                                         uint8_t *no_q);
+#ifdef RPI
+    void (*hevc_v_loop_filter_luma2)(uint8_t * _pix_r,
+                                 unsigned int _stride, unsigned int beta, const int32_t tc[2],
+                                 const uint8_t no_p[2], const uint8_t no_q[2],
+                                 uint8_t * _pix_l);
+    void (*hevc_h_loop_filter_uv)(uint8_t * src, unsigned int stride, uint32_t tc4,
+                                 unsigned int no_f);
+    void (*hevc_v_loop_filter_uv2)(uint8_t * src_r, unsigned int stride, uint32_t tc4,
+                                 uint8_t * src_l,
+                                 unsigned int no_f);
+
+#endif
+
+    void (*hevc_deblocking_boundary_strengths)(int pus, int dup, int in_inc, int out_inc,
+                                               int *curr_rpl0, int *curr_rpl1, int *neigh_rpl0, int *neigh_rpl1,
+                                               MvField *curr, MvField *neigh, uint8_t *bs);
 } HEVCDSPContext;
 
 void ff_hevc_dsp_init(HEVCDSPContext *hpc, int bit_depth);
diff -Naur ffmpeg-3.2.4/libavcodec/hevcdsp_template.c ffmpeg-3.2.4.patch/libavcodec/hevcdsp_template.c
--- ffmpeg-3.2.4/libavcodec/hevcdsp_template.c	2016-06-27 01:54:29.000000000 +0200
+++ ffmpeg-3.2.4.patch/libavcodec/hevcdsp_template.c	2017-05-28 20:42:45.744088687 +0200
@@ -26,6 +26,9 @@
 #include "bit_depth_template.c"
 #include "hevcdsp.h"
 
+#ifdef RPI
+#include "rpi_zc.h"
+#endif
 
 static void FUNC(put_pcm)(uint8_t *_dst, ptrdiff_t stride, int width, int height,
                           GetBitContext *gb, int pcm_bit_depth)
@@ -42,6 +45,29 @@
     }
 }
 
+static void FUNC(put_pcm_c)(uint8_t *_dst, ptrdiff_t stride, int width, int height,
+                          GetBitContext *gb, int pcm_bit_depth)
+{
+    int x, y;
+    pixel *dst = (pixel *)_dst;
+
+    stride /= sizeof(pixel);
+
+    for (y = 0; y < height; y++) {
+        for (x = 0; x < width; x++)
+            dst[x*2] = get_bits(gb, pcm_bit_depth) << (BIT_DEPTH - pcm_bit_depth);
+        dst += stride;
+    }
+
+    dst = (pixel *)_dst + 1;
+    for (y = 0; y < height; y++) {
+        for (x = 0; x < width; x++)
+            dst[x*2] = get_bits(gb, pcm_bit_depth) << (BIT_DEPTH - pcm_bit_depth);
+        dst += stride;
+    }
+}
+
+
 static av_always_inline void FUNC(transquant_bypass)(uint8_t *_dst, int16_t *coeffs,
                                                      ptrdiff_t stride, int size)
 {
@@ -59,6 +85,23 @@
     }
 }
 
+static av_always_inline void FUNC(add_residual_uv)(uint8_t *_dst, int16_t *res,
+                                                ptrdiff_t stride, int size)
+{
+    int x, y;
+    pixel *dst = (pixel *)_dst;
+
+    stride /= sizeof(pixel);
+
+    for (y = 0; y < size; y++) {
+        for (x = 0; x < size * 2; x += 2) {
+            dst[x] = av_clip_pixel(dst[x] + *res);
+            res++;
+        }
+        dst += stride;
+    }
+}
+
 static void FUNC(transform_add4x4)(uint8_t *_dst, int16_t *coeffs,
                                        ptrdiff_t stride)
 {
@@ -83,6 +126,58 @@
     FUNC(transquant_bypass)(_dst, coeffs, stride, 32);
 }
 
+// -- U -- (plaited)
+
+static void FUNC(add_residual4x4_u)(uint8_t *_dst, int16_t *res,
+                                  ptrdiff_t stride)
+{
+    FUNC(add_residual_uv)(_dst, res, stride, 4);
+}
+
+static void FUNC(add_residual8x8_u)(uint8_t *_dst, int16_t *res,
+                                  ptrdiff_t stride)
+{
+    FUNC(add_residual_uv)(_dst, res, stride, 8);
+}
+
+static void FUNC(add_residual16x16_u)(uint8_t *_dst, int16_t *res,
+                                    ptrdiff_t stride)
+{
+    FUNC(add_residual_uv)(_dst, res, stride, 16);
+}
+
+static void FUNC(add_residual32x32_u)(uint8_t *_dst, int16_t *res,
+                                    ptrdiff_t stride)
+{
+    FUNC(add_residual_uv)(_dst, res, stride, 32);
+}
+
+// -- V -- (plaited)
+
+static void FUNC(add_residual4x4_v)(uint8_t *_dst, int16_t *res,
+                                  ptrdiff_t stride)
+{
+    FUNC(add_residual_uv)(_dst + 1, res, stride, 4);
+}
+
+static void FUNC(add_residual8x8_v)(uint8_t *_dst, int16_t *res,
+                                  ptrdiff_t stride)
+{
+    FUNC(add_residual_uv)(_dst + 1, res, stride, 8);
+}
+
+static void FUNC(add_residual16x16_v)(uint8_t *_dst, int16_t *res,
+                                    ptrdiff_t stride)
+{
+    FUNC(add_residual_uv)(_dst + 1, res, stride, 16);
+}
+
+static void FUNC(add_residual32x32_v)(uint8_t *_dst, int16_t *res,
+                                    ptrdiff_t stride)
+{
+    FUNC(add_residual_uv)(_dst + 1, res, stride, 32);
+}
+
 
 static void FUNC(transform_rdpcm)(int16_t *_coeffs, int16_t log2_size, int mode)
 {
@@ -367,7 +462,6 @@
     int x, y;
     pixel *dst = (pixel *)_dst;
     pixel *src = (pixel *)_src;
-    int16_t *sao_offset_val = sao->offset_val[c_idx];
     int sao_eo_class    = sao->eo_class[c_idx];
     int init_x = 0, width = _width, height = _height;
 
@@ -376,33 +470,29 @@
 
     if (sao_eo_class != SAO_EO_VERT) {
         if (borders[0]) {
-            int offset_val = sao_offset_val[0];
             for (y = 0; y < height; y++) {
-                dst[y * stride_dst] = av_clip_pixel(src[y * stride_src] + offset_val);
+                dst[y * stride_dst] = src[y * stride_src];
             }
             init_x = 1;
         }
         if (borders[2]) {
-            int offset_val = sao_offset_val[0];
             int offset     = width - 1;
             for (x = 0; x < height; x++) {
-                dst[x * stride_dst + offset] = av_clip_pixel(src[x * stride_src + offset] + offset_val);
+                dst[x * stride_dst + offset] = src[x * stride_src + offset];
             }
             width--;
         }
     }
     if (sao_eo_class != SAO_EO_HORIZ) {
         if (borders[1]) {
-            int offset_val = sao_offset_val[0];
             for (x = init_x; x < width; x++)
-                dst[x] = av_clip_pixel(src[x] + offset_val);
+                dst[x] = src[x];
         }
         if (borders[3]) {
-            int offset_val   = sao_offset_val[0];
-            int y_stride_dst = stride_dst * (height - 1);
-            int y_stride_src = stride_src * (height - 1);
+            ptrdiff_t y_stride_dst = stride_dst * (height - 1);
+            ptrdiff_t y_stride_src = stride_src * (height - 1);
             for (x = init_x; x < width; x++)
-                dst[x + y_stride_dst] = av_clip_pixel(src[x + y_stride_src] + offset_val);
+                dst[x + y_stride_dst] = src[x + y_stride_src];
             height--;
         }
     }
@@ -417,7 +507,6 @@
     int x, y;
     pixel *dst = (pixel *)_dst;
     pixel *src = (pixel *)_src;
-    int16_t *sao_offset_val = sao->offset_val[c_idx];
     int sao_eo_class    = sao->eo_class[c_idx];
     int init_x = 0, init_y = 0, width = _width, height = _height;
 
@@ -426,34 +515,30 @@
 
     if (sao_eo_class != SAO_EO_VERT) {
         if (borders[0]) {
-            int offset_val = sao_offset_val[0];
             for (y = 0; y < height; y++) {
-                dst[y * stride_dst] = av_clip_pixel(src[y * stride_src] + offset_val);
+                dst[y * stride_dst] = src[y * stride_src];
             }
             init_x = 1;
         }
         if (borders[2]) {
-            int offset_val = sao_offset_val[0];
             int offset     = width - 1;
             for (x = 0; x < height; x++) {
-                dst[x * stride_dst + offset] = av_clip_pixel(src[x * stride_src + offset] + offset_val);
+                dst[x * stride_dst + offset] = src[x * stride_src + offset];
             }
             width--;
         }
     }
     if (sao_eo_class != SAO_EO_HORIZ) {
         if (borders[1]) {
-            int offset_val = sao_offset_val[0];
             for (x = init_x; x < width; x++)
-                dst[x] = av_clip_pixel(src[x] + offset_val);
+                dst[x] = src[x];
             init_y = 1;
         }
         if (borders[3]) {
-            int offset_val   = sao_offset_val[0];
-            int y_stride_dst = stride_dst * (height - 1);
-            int y_stride_src = stride_src * (height - 1);
+            ptrdiff_t y_stride_dst = stride_dst * (height - 1);
+            ptrdiff_t y_stride_src = stride_src * (height - 1);
             for (x = init_x; x < width; x++)
-                dst[x + y_stride_dst] = av_clip_pixel(src[x + y_stride_src] + offset_val);
+                dst[x + y_stride_dst] = src[x + y_stride_src];
             height--;
         }
     }
@@ -494,6 +579,127 @@
     }
 }
 
+
+// --- Plaited chroma versions
+
+#if BIT_DEPTH != 8
+static void FUNC(sao_band_filter_c)(uint8_t *_dst, const uint8_t *_src,
+                                  ptrdiff_t stride_dst, ptrdiff_t stride_src,
+                                  const int16_t *sao_offset_val_u, int sao_left_class_u,
+                                  const int16_t *sao_offset_val_v, int sao_left_class_v,
+                                  int width, int height)
+{
+    av_log(NULL, AV_LOG_PANIC, "%s: NIF\n", __func__);                              \
+    abort();                                                                        \
+}
+#else
+static void FUNC(sao_band_filter_c)(uint8_t *_dst, const uint8_t *_src,
+                                  ptrdiff_t stride_dst, ptrdiff_t stride_src,
+                                  const int16_t *sao_offset_val_u, int sao_left_class_u,
+                                  const int16_t *sao_offset_val_v, int sao_left_class_v,
+                                  int width, int height)
+{
+    pixel *dst = (pixel *)_dst;
+    pixel *src = (pixel *)_src;
+    int offset_table_u[32] = { 0 };
+    int offset_table_v[32] = { 0 };
+    int k, y, x;
+    int shift  = BIT_DEPTH - 5;
+
+    stride_dst /= sizeof(pixel);
+    stride_src /= sizeof(pixel);
+    width *= 2;
+
+    for (k = 0; k < 4; k++)
+    {
+        offset_table_u[(k + sao_left_class_u) & 31] = sao_offset_val_u[k + 1];
+        offset_table_v[(k + sao_left_class_v) & 31] = sao_offset_val_v[k + 1];
+    }
+    for (y = 0; y < height; y++) {
+        for (x = 0; x < width; x += 2)
+        {
+            dst[x + 0] = av_clip_pixel(src[x + 0] + offset_table_u[src[x + 0] >> shift]);
+            dst[x + 1] = av_clip_pixel(src[x + 1] + offset_table_v[src[x + 1] >> shift]);
+        }
+        dst += stride_dst;
+        src += stride_src;
+    }
+}
+#endif
+
+#if BIT_DEPTH != 8
+static void FUNC(sao_edge_filter_c)(uint8_t *_dst, const uint8_t *_src, ptrdiff_t stride_dst,
+                                  const int16_t *sao_offset_val_u, const int16_t *sao_offset_val_v,
+                                  int eo, int width, int height) {
+    av_log(NULL, AV_LOG_PANIC, "%s: NIF\n", __func__);                              \
+    abort();                                                                        \
+}
+#else
+
+static void FUNC(sao_edge_filter_c)(uint8_t *_dst, const uint8_t *_src, ptrdiff_t stride_dst,
+                                  const int16_t *sao_offset_val_u, const int16_t *sao_offset_val_v,
+                                  int eo, int width, int height) {
+
+    static const uint8_t edge_idx[] = { 1, 2, 0, 3, 4 };
+    static const int8_t pos[4][2][2] = {
+        { { -1,  0 }, {  1, 0 } }, // horizontal
+        { {  0, -1 }, {  0, 1 } }, // vertical
+        { { -1, -1 }, {  1, 1 } }, // 45 degree
+        { {  1, -1 }, { -1, 1 } }, // 135 degree
+    };
+    pixel *dst = (pixel *)_dst;
+    pixel *src = (pixel *)_src;
+    int a_stride, b_stride;
+    int x, y;
+    ptrdiff_t stride_src = (2*MAX_PB_SIZE + AV_INPUT_BUFFER_PADDING_SIZE) / sizeof(pixel);
+    stride_dst /= sizeof(pixel);
+    width *= 2;
+
+    a_stride = pos[eo][0][0] * 2 + pos[eo][0][1] * stride_src;
+    b_stride = pos[eo][1][0] * 2 + pos[eo][1][1] * stride_src;
+    for (y = 0; y < height; y++) {
+        for (x = 0; x < width; x += 2) {
+            int diff0u = CMP(src[x], src[x + a_stride]);
+            int diff1u = CMP(src[x], src[x + b_stride]);
+            int offset_valu        = edge_idx[2 + diff0u + diff1u];
+            int diff0v = CMP(src[x+1], src[x+1 + a_stride]);
+            int diff1v = CMP(src[x+1], src[x+1 + b_stride]);
+            int offset_valv        = edge_idx[2 + diff0v + diff1v];
+            dst[x] = av_clip_pixel(src[x] + sao_offset_val_u[offset_valu]);
+            dst[x+1] = av_clip_pixel(src[x+1] + sao_offset_val_v[offset_valv]);
+        }
+        src += stride_src;
+        dst += stride_dst;
+    }
+}
+#endif
+
+#if BIT_DEPTH != 8
+static void FUNC(sao_edge_restore_c_0)(uint8_t *_dst, uint8_t *_src,
+                                    ptrdiff_t stride_dst, ptrdiff_t stride_src, SAOParams *sao,
+                                    int *borders, int _width, int _height,
+                                    int c_idx, uint8_t *vert_edge,
+                                    uint8_t *horiz_edge, uint8_t *diag_edge)
+{
+    av_log(NULL, AV_LOG_PANIC, "%s: NIF\n", __func__);                              \
+    abort();                                                                        \
+}
+static void FUNC(sao_edge_restore_c_1)(uint8_t *_dst, uint8_t *_src,
+                                    ptrdiff_t stride_dst, ptrdiff_t stride_src, SAOParams *sao,
+                                    int *borders, int _width, int _height,
+                                    int c_idx, uint8_t *vert_edge,
+                                    uint8_t *horiz_edge, uint8_t *diag_edge)
+{
+    av_log(NULL, AV_LOG_PANIC, "%s: NIF\n", __func__);                              \
+    abort();                                                                        \
+}
+#else
+// Any old 2 byte 'normal' restore will work for these
+#define sao_edge_restore_c_0_8 sao_edge_restore_0_10
+#define sao_edge_restore_c_1_8 sao_edge_restore_1_10
+#endif
+
+
 #undef CMP
 
 ////////////////////////////////////////////////////////////////////////////////
@@ -1694,3 +1900,217 @@
 #undef TQ1
 #undef TQ2
 #undef TQ3
+
+#ifdef RPI
+
+// line zero
+#define P3 pix_l[0 * xstride]
+#define P2 pix_l[1 * xstride]
+#define P1 pix_l[2 * xstride]
+#define P0 pix_l[3 * xstride]
+#define Q0 pix_r[0 * xstride]
+#define Q1 pix_r[1 * xstride]
+#define Q2 pix_r[2 * xstride]
+#define Q3 pix_r[3 * xstride]
+
+// line three. used only for deblocking decision
+#define TP3 pix_l[0 * xstride + 3 * ystride]
+#define TP2 pix_l[1 * xstride + 3 * ystride]
+#define TP1 pix_l[2 * xstride + 3 * ystride]
+#define TP0 pix_l[3 * xstride + 3 * ystride]
+#define TQ0 pix_r[0 * xstride + 3 * ystride]
+#define TQ1 pix_r[1 * xstride + 3 * ystride]
+#define TQ2 pix_r[2 * xstride + 3 * ystride]
+#define TQ3 pix_r[3 * xstride + 3 * ystride]
+
+// This is identical to hevc_loop_filter_luma except that the P/Q
+// components are on separate pointers
+static void FUNC(hevc_v_loop_filter_luma2)(uint8_t * _pix_r,
+                                 unsigned int _stride, unsigned int beta, const int32_t _tc[2],
+                                 const uint8_t _no_p[2], const uint8_t _no_q[2],
+                                 uint8_t * _pix_l)
+{
+    int d, j;
+    pixel *pix_l        = (pixel *)_pix_l;
+    pixel *pix_r        = (pixel *)_pix_r;
+    const ptrdiff_t xstride = 1;
+    const ptrdiff_t ystride = _stride / sizeof(pixel);
+
+    beta <<= BIT_DEPTH - 8;
+
+    for (j = 0; j < 2; j++) {
+        const int dp0  = abs(P2  - 2 * P1  + P0);
+        const int dq0  = abs(Q2  - 2 * Q1  + Q0);
+        const int dp3  = abs(TP2 - 2 * TP1 + TP0);
+        const int dq3  = abs(TQ2 - 2 * TQ1 + TQ0);
+        const int d0   = dp0 + dq0;
+        const int d3   = dp3 + dq3;
+        const int tc   = _tc[j]   << (BIT_DEPTH - 8);
+        const int no_p = _no_p[j];
+        const int no_q = _no_q[j];
+
+        if (d0 + d3 >= beta) {
+            pix_l += 4 * ystride;
+            pix_r += 4 * ystride;
+            continue;
+        } else {
+            const int beta_3 = beta >> 3;
+            const int beta_2 = beta >> 2;
+            const int tc25   = ((tc * 5 + 1) >> 1);
+
+            if (abs(P3  -  P0) + abs(Q3  -  Q0) < beta_3 && abs(P0  -  Q0) < tc25 &&
+                abs(TP3 - TP0) + abs(TQ3 - TQ0) < beta_3 && abs(TP0 - TQ0) < tc25 &&
+                                      (d0 << 1) < beta_2 &&      (d3 << 1) < beta_2) {
+                // strong filtering
+                const int tc2 = tc << 1;
+                for (d = 0; d < 4; d++) {
+                    const int p3 = P3;
+                    const int p2 = P2;
+                    const int p1 = P1;
+                    const int p0 = P0;
+                    const int q0 = Q0;
+                    const int q1 = Q1;
+                    const int q2 = Q2;
+                    const int q3 = Q3;
+                    if (!no_p) {
+                        P0 = p0 + av_clip(((p2 + 2 * p1 + 2 * p0 + 2 * q0 + q1 + 4) >> 3) - p0, -tc2, tc2);
+                        P1 = p1 + av_clip(((p2 + p1 + p0 + q0 + 2) >> 2) - p1, -tc2, tc2);
+                        P2 = p2 + av_clip(((2 * p3 + 3 * p2 + p1 + p0 + q0 + 4) >> 3) - p2, -tc2, tc2);
+                    }
+                    if (!no_q) {
+                        Q0 = q0 + av_clip(((p1 + 2 * p0 + 2 * q0 + 2 * q1 + q2 + 4) >> 3) - q0, -tc2, tc2);
+                        Q1 = q1 + av_clip(((p0 + q0 + q1 + q2 + 2) >> 2) - q1, -tc2, tc2);
+                        Q2 = q2 + av_clip(((2 * q3 + 3 * q2 + q1 + q0 + p0 + 4) >> 3) - q2, -tc2, tc2);
+                    }
+                    pix_l += ystride;
+                    pix_r += ystride;
+                }
+            } else { // normal filtering
+                int nd_p = 1;
+                int nd_q = 1;
+                const int tc_2 = tc >> 1;
+                if (dp0 + dp3 < ((beta + (beta >> 1)) >> 3))
+                    nd_p = 2;
+                if (dq0 + dq3 < ((beta + (beta >> 1)) >> 3))
+                    nd_q = 2;
+
+                for (d = 0; d < 4; d++) {
+                    const int p2 = P2;
+                    const int p1 = P1;
+                    const int p0 = P0;
+                    const int q0 = Q0;
+                    const int q1 = Q1;
+                    const int q2 = Q2;
+                    int delta0   = (9 * (q0 - p0) - 3 * (q1 - p1) + 8) >> 4;
+                    if (abs(delta0) < 10 * tc) {
+                        delta0 = av_clip(delta0, -tc, tc);
+                        if (!no_p)
+                            P0 = av_clip_pixel(p0 + delta0);
+                        if (!no_q)
+                            Q0 = av_clip_pixel(q0 - delta0);
+                        if (!no_p && nd_p > 1) {
+                            const int deltap1 = av_clip((((p2 + p0 + 1) >> 1) - p1 + delta0) >> 1, -tc_2, tc_2);
+                            P1 = av_clip_pixel(p1 + deltap1);
+                        }
+                        if (!no_q && nd_q > 1) {
+                            const int deltaq1 = av_clip((((q2 + q0 + 1) >> 1) - q1 - delta0) >> 1, -tc_2, tc_2);
+                            Q1 = av_clip_pixel(q1 + deltaq1);
+                        }
+                    }
+                    pix_l += ystride;
+                    pix_r += ystride;
+                }
+            }
+        }
+    }
+}
+
+#undef TP3
+#undef TP2
+#undef TP1
+#undef TP0
+#undef TQ0
+#undef TQ1
+#undef TQ2
+#undef TQ3
+
+#undef P3
+#undef P2
+#undef P1
+#undef P0
+#undef Q0
+#undef Q1
+#undef Q2
+#undef Q3
+
+#define P1 pix_l[0 * xstride]
+#define P0 pix_l[1 * xstride]
+#define Q0 pix_r[0 * xstride]
+#define Q1 pix_r[1 * xstride]
+
+static void FUNC(hevc_loop_filter_uv2)(uint8_t *_pix_l, ptrdiff_t _xstride,
+                                          ptrdiff_t _ystride, const int32_t *_tc,
+                                          const uint8_t *_no_p, const uint8_t *_no_q, uint8_t *_pix_r)
+{
+    int d, j, no_p, no_q;
+    pixel *pix_l        = (pixel *)_pix_l;
+    pixel *pix_r        = (pixel *)_pix_r;
+    ptrdiff_t xstride = _xstride / sizeof(pixel);
+    ptrdiff_t ystride = _ystride / sizeof(pixel);
+
+    for (j = 0; j < 2; j++) {
+        const int tc = _tc[j] << (BIT_DEPTH - 8);
+        if (tc <= 0) {
+            pix_l += 4 * ystride;
+            pix_r += 4 * ystride;
+            continue;
+        }
+        no_p = _no_p[j];
+        no_q = _no_q[j];
+
+        for (d = 0; d < 4; d++) {
+            int delta0;
+            const int p1 = P1;
+            const int p0 = P0;
+            const int q0 = Q0;
+            const int q1 = Q1;
+            delta0 = av_clip((((q0 - p0) * 4) + p1 - q1 + 4) >> 3, -tc, tc);
+            if (!no_p)
+                P0 = av_clip_pixel(p0 + delta0);
+            if (!no_q)
+                Q0 = av_clip_pixel(q0 - delta0);
+            pix_l += ystride;
+            pix_r += ystride;
+        }
+    }
+}
+
+static void FUNC(hevc_h_loop_filter_uv)(uint8_t * pix, unsigned int stride, uint32_t tc4,
+                                 unsigned int no_f)
+{
+    uint8_t no_p[2] = {no_f & 1, no_f & 2};
+    uint8_t no_q[2] = {no_f & 4, no_f & 8};
+    int32_t tc[4] = {tc4 & 0xff, (tc4 >> 8) & 0xff, (tc4 >> 16) & 0xff, tc4 >> 24};
+    FUNC(hevc_loop_filter_chroma)(pix, stride, sizeof(pixel) * 2, tc, no_p, no_q);
+    FUNC(hevc_loop_filter_chroma)(pix + sizeof(pixel), stride, sizeof(pixel) * 2, tc + 2, no_p, no_q);
+}
+
+static void FUNC(hevc_v_loop_filter_uv2)(uint8_t * src_r, unsigned int stride, uint32_t tc4,
+                                 uint8_t * src_l,
+                                 unsigned int no_f)
+{
+    uint8_t no_p[2] = {no_f & 1, no_f & 2};
+    uint8_t no_q[2] = {no_f & 4, no_f & 8};
+    int32_t tc[4] = {tc4 & 0xff, (tc4 >> 8) & 0xff, (tc4 >> 16) & 0xff, tc4 >> 24};
+    FUNC(hevc_loop_filter_uv2)(src_l, sizeof(pixel) * 2, stride, tc, no_p, no_q, src_r);
+    FUNC(hevc_loop_filter_uv2)(src_l + sizeof(pixel), sizeof(pixel) * 2, stride, tc + 2, no_p, no_q, src_r + sizeof(pixel));
+}
+
+#undef P1
+#undef P0
+#undef Q0
+#undef Q1
+
+
+#endif
+
diff -Naur ffmpeg-3.2.4/libavcodec/hevc_filter.c ffmpeg-3.2.4.patch/libavcodec/hevc_filter.c
--- ffmpeg-3.2.4/libavcodec/hevc_filter.c	2017-02-10 14:25:26.000000000 +0100
+++ ffmpeg-3.2.4.patch/libavcodec/hevc_filter.c	2017-05-28 20:42:45.741088677 +0200
@@ -22,6 +22,12 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
+//#define DISABLE_SAO
+//#define DISABLE_DEBLOCK
+//#define DISABLE_STRENGTHS
+// define DISABLE_DEBLOCK_NONREF for a 6% speed boost (by skipping deblocking on unimportant frames)
+//#define DISABLE_DEBLOCK_NONREF
+
 #include "libavutil/common.h"
 #include "libavutil/internal.h"
 
@@ -30,6 +36,11 @@
 
 #include "bit_depth_template.c"
 
+#ifdef RPI
+#include "rpi_qpu.h"
+#include "rpi_zc.h"
+#endif
+
 #define LUMA 0
 #define CB 1
 #define CR 2
@@ -138,6 +149,15 @@
     return s->qp_y_tab[x + y * s->ps.sps->min_cb_width];
 }
 
+static inline unsigned int pixel_shift(const HEVCContext * const s, const unsigned int c_idx)
+{
+#ifdef RPI
+    return c_idx != 0 && rpi_sliced_frame(s->frame) ? 1 : s->ps.sps->pixel_shift;
+#else
+    return s->ps.sps->pixel_shift;
+#endif
+}
+
 static void copy_CTB(uint8_t *dst, const uint8_t *src, int width, int height,
                      intptr_t stride_dst, intptr_t stride_src)
 {
@@ -192,7 +212,7 @@
                            int stride_src, int x, int y, int width, int height,
                            int c_idx, int x_ctb, int y_ctb)
 {
-    int sh = s->ps.sps->pixel_shift;
+    const unsigned int sh = pixel_shift(s, c_idx);
     int w = s->ps.sps->width >> s->ps.sps->hshift[c_idx];
     int h = s->ps.sps->height >> s->ps.sps->vshift[c_idx];
 
@@ -223,13 +243,14 @@
         int y_min        = ((y0         ) >> s->ps.sps->log2_min_pu_size);
         int x_max        = ((x0 + width ) >> s->ps.sps->log2_min_pu_size);
         int y_max        = ((y0 + height) >> s->ps.sps->log2_min_pu_size);
-        int len          = (min_pu_size >> hshift) << s->ps.sps->pixel_shift;
+        const unsigned int sh = pixel_shift(s, c_idx);
+        int len          = (min_pu_size >> hshift) << sh;
         for (y = y_min; y < y_max; y++) {
             for (x = x_min; x < x_max; x++) {
                 if (s->is_pcm[y * s->ps.sps->min_pu_width + x]) {
                     int n;
-                    uint8_t *src = src1 + (((y << s->ps.sps->log2_min_pu_size) - y0) >> vshift) * stride_src + ((((x << s->ps.sps->log2_min_pu_size) - x0) >> hshift) << s->ps.sps->pixel_shift);
-                    const uint8_t *dst = dst1 + (((y << s->ps.sps->log2_min_pu_size) - y0) >> vshift) * stride_dst + ((((x << s->ps.sps->log2_min_pu_size) - x0) >> hshift) << s->ps.sps->pixel_shift);
+                    uint8_t *src = src1 + (((y << s->ps.sps->log2_min_pu_size) - y0) >> vshift) * stride_src + ((((x << s->ps.sps->log2_min_pu_size) - x0) >> hshift) << sh);
+                    const uint8_t *dst = dst1 + (((y << s->ps.sps->log2_min_pu_size) - y0) >> vshift) * stride_dst + ((((x << s->ps.sps->log2_min_pu_size) - x0) >> hshift) << sh);
                     for (n = 0; n < (min_pu_size >> vshift); n++) {
                         memcpy(src, dst, len);
                         src += stride_src;
@@ -245,7 +266,7 @@
 
 static void sao_filter_CTB(HEVCContext *s, int x, int y)
 {
-    static const uint8_t sao_tab[8] = { 0, 1, 2, 2, 3, 3, 4, 4 };
+    static const uint8_t sao_tab[8] = { 0 /* 8 */, 1 /* 16 */, 2 /* 24 */, 2 /* 32 */, 3, 3 /* 48 */, 4, 4 /* 64 */};
     HEVCLocalContext *lc = s->HEVClc;
     int c_idx;
     int edges[4];  // 0 left 1 top 2 right 3 bottom
@@ -266,12 +287,22 @@
     uint8_t right_tile_edge  = 0;
     uint8_t up_tile_edge     = 0;
     uint8_t bottom_tile_edge = 0;
+#ifdef RPI
+    const int sliced = rpi_sliced_frame(s->frame);
+    const int plane_count = sliced ? 2 : (s->ps.sps->chroma_format_idc ? 3 : 1);
+#else
+    const int plane_count = (s->ps.sps->chroma_format_idc ? 3 : 1);
+#endif
 
     edges[0]   = x_ctb == 0;
     edges[1]   = y_ctb == 0;
     edges[2]   = x_ctb == s->ps.sps->ctb_width  - 1;
     edges[3]   = y_ctb == s->ps.sps->ctb_height - 1;
 
+#ifdef DISABLE_SAO
+    return;
+#endif
+
     if (restore) {
         if (!edges[0]) {
             left_tile_edge  = no_tile_filter && s->ps.pps->tile_id[ctb_addr_ts] != s->ps.pps->tile_id[s->ps.pps->ctb_addr_rs_to_ts[ctb_addr_rs-1]];
@@ -303,7 +334,7 @@
         }
     }
 
-    for (c_idx = 0; c_idx < (s->ps.sps->chroma_format_idc ? 3 : 1); c_idx++) {
+    for (c_idx = 0; c_idx < plane_count; c_idx++) {
         int x0       = x >> s->ps.sps->hshift[c_idx];
         int y0       = y >> s->ps.sps->vshift[c_idx];
         int stride_src = s->frame->linesize[c_idx];
@@ -312,28 +343,82 @@
         int width    = FFMIN(ctb_size_h, (s->ps.sps->width  >> s->ps.sps->hshift[c_idx]) - x0);
         int height   = FFMIN(ctb_size_v, (s->ps.sps->height >> s->ps.sps->vshift[c_idx]) - y0);
         int tab      = sao_tab[(FFALIGN(width, 8) >> 3) - 1];
-        uint8_t *src = &s->frame->data[c_idx][y0 * stride_src + (x0 << s->ps.sps->pixel_shift)];
-        int stride_dst;
+        ptrdiff_t stride_dst;
         uint8_t *dst;
 
+#ifdef RPI
+        const unsigned int sh = (sliced && c_idx != 0) ? 1 : s->ps.sps->pixel_shift;
+        const int wants_lr = sao->type_idx[c_idx] == SAO_EDGE && sao->eo_class[c_idx] != 1 /* Vertical */;
+        uint8_t * const src = !sliced ?
+                &s->frame->data[c_idx][y0 * stride_src + (x0 << s->ps.sps->pixel_shift)] :
+            c_idx == 0 ?
+                rpi_sliced_frame_pos_y(s->frame, x0, y0) :
+                rpi_sliced_frame_pos_c(s->frame, x0, y0);
+        const uint8_t * const src_l = edges[0] || !wants_lr ? NULL :
+            !sliced ? src - (1 << sh) :
+            c_idx == 0 ?
+                rpi_sliced_frame_pos_y(s->frame, x0 - 1, y0) :
+                rpi_sliced_frame_pos_c(s->frame, x0 - 1, y0);
+        const uint8_t * const src_r = edges[2] || !wants_lr ? NULL :
+            !sliced ? src + (width << sh) :
+            c_idx == 0 ?
+                rpi_sliced_frame_pos_y(s->frame, x0 + width, y0) :
+                rpi_sliced_frame_pos_c(s->frame, x0 + width, y0);
+
+
+        if (sliced && c_idx > 1) {
+            break;
+        }
+#else
+        const unsigned int sh = s->ps.sps->pixel_shift;
+        const int wants_lr = sao->type_idx[c_idx] == SAO_EDGE && sao->eo_class[c_idx] != 1 /* Vertical */;
+        uint8_t * const src = &s->frame->data[c_idx][y0 * stride_src + (x0 << s->ps.sps->pixel_shift)];
+        const uint8_t * const src_l = edges[0] || !wants_lr ? NULL : src - (1 << sh);
+        const uint8_t * const src_r = edges[2] || !wants_lr ? NULL : src + (width << sh);
+#endif
+
         switch (sao->type_idx[c_idx]) {
         case SAO_BAND:
             copy_CTB_to_hv(s, src, stride_src, x0, y0, width, height, c_idx,
                            x_ctb, y_ctb);
             if (s->ps.pps->transquant_bypass_enable_flag ||
                 (s->ps.sps->pcm.loop_filter_disable_flag && s->ps.sps->pcm_enabled_flag)) {
-            dst = lc->edge_emu_buffer;
-            stride_dst = 2*MAX_PB_SIZE;
-            copy_CTB(dst, src, width << s->ps.sps->pixel_shift, height, stride_dst, stride_src);
-            s->hevcdsp.sao_band_filter[tab](src, dst, stride_src, stride_dst,
-                                            sao->offset_val[c_idx], sao->band_position[c_idx],
-                                            width, height);
-            restore_tqb_pixels(s, src, dst, stride_src, stride_dst,
-                               x, y, width, height, c_idx);
+                dst = lc->edge_emu_buffer;
+                stride_dst = 2*MAX_PB_SIZE;
+                copy_CTB(dst, src, width << sh, height, stride_dst, stride_src);
+#ifdef RPI
+                if (sliced && c_idx != 0)
+                {
+                    s->hevcdsp.sao_band_filter_c[tab](src, dst, stride_src, stride_dst,
+                                                    sao->offset_val[1], sao->band_position[1],
+                                                    sao->offset_val[2], sao->band_position[2],
+                                                    width, height);
+                }
+                else
+#endif
+                {
+                    s->hevcdsp.sao_band_filter[tab](src, dst, stride_src, stride_dst,
+                                                    sao->offset_val[c_idx], sao->band_position[c_idx],
+                                                    width, height);
+                }
+                restore_tqb_pixels(s, src, dst, stride_src, stride_dst,
+                                   x, y, width, height, c_idx);
             } else {
-            s->hevcdsp.sao_band_filter[tab](src, src, stride_src, stride_src,
-                                            sao->offset_val[c_idx], sao->band_position[c_idx],
-                                            width, height);
+#ifdef RPI
+                if (sliced && c_idx != 0)
+                {
+                    s->hevcdsp.sao_band_filter_c[tab](src, src, stride_src, stride_src,
+                                                    sao->offset_val[1], sao->band_position[1],
+                                                    sao->offset_val[2], sao->band_position[2],
+                                                    width, height);
+                }
+                else
+#endif
+                {
+                    s->hevcdsp.sao_band_filter[tab](src, src, stride_src, stride_src,
+                                                    sao->offset_val[c_idx], sao->band_position[c_idx],
+                                                    width, height);
+                }
             }
             sao->type_idx[c_idx] = SAO_APPLIED;
             break;
@@ -341,108 +426,117 @@
         {
             int w = s->ps.sps->width >> s->ps.sps->hshift[c_idx];
             int h = s->ps.sps->height >> s->ps.sps->vshift[c_idx];
-            int left_edge = edges[0];
             int top_edge = edges[1];
-            int right_edge = edges[2];
             int bottom_edge = edges[3];
-            int sh = s->ps.sps->pixel_shift;
-            int left_pixels, right_pixels;
 
             stride_dst = 2*MAX_PB_SIZE + AV_INPUT_BUFFER_PADDING_SIZE;
             dst = lc->edge_emu_buffer + stride_dst + AV_INPUT_BUFFER_PADDING_SIZE;
 
             if (!top_edge) {
-                int left = 1 - left_edge;
-                int right = 1 - right_edge;
-                const uint8_t *src1[2];
                 uint8_t *dst1;
-                int src_idx, pos;
+                int src_idx;
+                const uint8_t * const src_spb = s->sao_pixel_buffer_h[c_idx] + (((2 * y_ctb - 1) * w + x0) << sh);
+
+                dst1 = dst - stride_dst;
 
-                dst1 = dst - stride_dst - (left << sh);
-                src1[0] = src - stride_src - (left << sh);
-                src1[1] = s->sao_pixel_buffer_h[c_idx] + (((2 * y_ctb - 1) * w + x0 - left) << sh);
-                pos = 0;
-                if (left) {
+                if (src_l != NULL) {
                     src_idx = (CTB(s->sao, x_ctb-1, y_ctb-1).type_idx[c_idx] ==
                                SAO_APPLIED);
-                    copy_pixel(dst1, src1[src_idx], sh);
-                    pos += (1 << sh);
+                    copy_pixel(dst1 - (1 << sh), src_idx ? src_spb - (1 << sh) : src_l - stride_src, sh);
                 }
+
                 src_idx = (CTB(s->sao, x_ctb, y_ctb-1).type_idx[c_idx] ==
                            SAO_APPLIED);
-                memcpy(dst1 + pos, src1[src_idx] + pos, width << sh);
-                if (right) {
-                    pos += width << sh;
+                memcpy(dst1, src_idx ? src_spb : src - stride_src, width << sh);
+
+                if (src_r != NULL) {
                     src_idx = (CTB(s->sao, x_ctb+1, y_ctb-1).type_idx[c_idx] ==
                                SAO_APPLIED);
-                    copy_pixel(dst1 + pos, src1[src_idx] + pos, sh);
+                    copy_pixel(dst1 + (width << sh), src_idx ? src_spb + (width << sh) : src_r - stride_src, sh);
                 }
             }
             if (!bottom_edge) {
-                int left = 1 - left_edge;
-                int right = 1 - right_edge;
-                const uint8_t *src1[2];
-                uint8_t *dst1;
-                int src_idx, pos;
+                uint8_t * const dst1 = dst + height * stride_dst;
+                int src_idx;
+                const uint8_t * const src_spb = s->sao_pixel_buffer_h[c_idx] + (((2 * y_ctb + 2) * w + x0) << sh);
+                const unsigned int hoff = height * stride_src;
 
-                dst1 = dst + height * stride_dst - (left << sh);
-                src1[0] = src + height * stride_src - (left << sh);
-                src1[1] = s->sao_pixel_buffer_h[c_idx] + (((2 * y_ctb + 2) * w + x0 - left) << sh);
-                pos = 0;
-                if (left) {
+                if (src_l != NULL) {
                     src_idx = (CTB(s->sao, x_ctb-1, y_ctb+1).type_idx[c_idx] ==
                                SAO_APPLIED);
-                    copy_pixel(dst1, src1[src_idx], sh);
-                    pos += (1 << sh);
+                    copy_pixel(dst1 - (1 << sh), src_idx ? src_spb - (1 << sh) : src_l + hoff, sh);
                 }
+
                 src_idx = (CTB(s->sao, x_ctb, y_ctb+1).type_idx[c_idx] ==
                            SAO_APPLIED);
-                memcpy(dst1 + pos, src1[src_idx] + pos, width << sh);
-                if (right) {
-                    pos += width << sh;
+                memcpy(dst1, src_idx ? src_spb : src + hoff, width << sh);
+
+                if (src_r != NULL) {
                     src_idx = (CTB(s->sao, x_ctb+1, y_ctb+1).type_idx[c_idx] ==
                                SAO_APPLIED);
-                    copy_pixel(dst1 + pos, src1[src_idx] + pos, sh);
+                    copy_pixel(dst1 + (width << sh), src_idx ? src_spb + (width << sh) : src_r + hoff, sh);
                 }
             }
-            left_pixels = 0;
-            if (!left_edge) {
+            if (src_l != NULL) {
                 if (CTB(s->sao, x_ctb-1, y_ctb).type_idx[c_idx] == SAO_APPLIED) {
                     copy_vert(dst - (1 << sh),
                               s->sao_pixel_buffer_v[c_idx] + (((2 * x_ctb - 1) * h + y0) << sh),
                               sh, height, stride_dst, 1 << sh);
                 } else {
-                    left_pixels = 1;
+                    copy_vert(dst - (1 << sh),
+                              src_l,
+                              sh, height, stride_dst, stride_src);
                 }
             }
-            right_pixels = 0;
-            if (!right_edge) {
+            if (src_r != NULL) {
                 if (CTB(s->sao, x_ctb+1, y_ctb).type_idx[c_idx] == SAO_APPLIED) {
                     copy_vert(dst + (width << sh),
                               s->sao_pixel_buffer_v[c_idx] + (((2 * x_ctb + 2) * h + y0) << sh),
                               sh, height, stride_dst, 1 << sh);
                 } else {
-                    right_pixels = 1;
+                    copy_vert(dst + (width << sh),
+                              src_r,
+                              sh, height, stride_dst, stride_src);
                 }
             }
 
-            copy_CTB(dst - (left_pixels << sh),
-                     src - (left_pixels << sh),
-                     (width + left_pixels + right_pixels) << sh,
+            copy_CTB(dst,
+                     src,
+                     width << sh,
                      height, stride_dst, stride_src);
 
             copy_CTB_to_hv(s, src, stride_src, x0, y0, width, height, c_idx,
                            x_ctb, y_ctb);
-            s->hevcdsp.sao_edge_filter[tab](src, dst, stride_src, sao->offset_val[c_idx],
-                                            sao->eo_class[c_idx], width, height);
-            s->hevcdsp.sao_edge_restore[restore](src, dst,
-                                                stride_src, stride_dst,
-                                                sao,
-                                                edges, width,
-                                                height, c_idx,
-                                                vert_edge,
-                                                horiz_edge,
-                                                diag_edge);
+#ifdef RPI
+            if (sliced && c_idx != 0)
+            {
+                // Class always the same for both U & V (which is just as well :-))
+                s->hevcdsp.sao_edge_filter_c[tab](src, dst, stride_src,
+                                                sao->offset_val[1], sao->offset_val[2], sao->eo_class[1],
+                                                width, height);
+                s->hevcdsp.sao_edge_restore_c[restore](src, dst,
+                                                    stride_src, stride_dst,
+                                                    sao,
+                                                    edges, width,
+                                                    height, c_idx,
+                                                    vert_edge,
+                                                    horiz_edge,
+                                                    diag_edge);
+            }
+            else
+#endif
+            {
+                s->hevcdsp.sao_edge_filter[tab](src, dst, stride_src, sao->offset_val[c_idx],
+                                                sao->eo_class[c_idx], width, height);
+                s->hevcdsp.sao_edge_restore[restore](src, dst,
+                                                    stride_src, stride_dst,
+                                                    sao,
+                                                    edges, width,
+                                                    height, c_idx,
+                                                    vert_edge,
+                                                    horiz_edge,
+                                                    diag_edge);
+            }
             restore_tqb_pixels(s, src, dst, stride_src, stride_dst,
                                x, y, width, height, c_idx);
             sao->type_idx[c_idx] = SAO_APPLIED;
@@ -452,6 +546,7 @@
     }
 }
 
+// Returns 2 or 0.
 static int get_pcm(HEVCContext *s, int x, int y)
 {
     int log2_min_pu_size = s->ps.sps->log2_min_pu_size;
@@ -478,7 +573,7 @@
     uint8_t *src;
     int x, y;
     int chroma, beta;
-    int32_t c_tc[2], tc[2];
+    int32_t c_tc[4], tc[2];
     uint8_t no_p[2] = { 0 };
     uint8_t no_q[2] = { 0 };
 
@@ -495,6 +590,15 @@
                 s->ps.sps->pcm.loop_filter_disable_flag) ||
                s->ps.pps->transquant_bypass_enable_flag;
 
+#ifdef DISABLE_DEBLOCK_NONREF
+    if (!s->used_for_ref)
+      return; // Don't deblock non-reference frames
+#endif
+#ifdef DISABLE_DEBLOCK
+    return;
+#endif
+    if (!s->used_for_ref && s->avctx->skip_loop_filter >= AVDISCARD_NONREF)
+        return;
     if (x0) {
         left_tc_offset   = s->deblock[ctb - 1].tc_offset;
         left_beta_offset = s->deblock[ctb - 1].beta_offset;
@@ -528,19 +632,51 @@
 
                 tc[0]   = bs0 ? TC_CALC(qp, bs0) : 0;
                 tc[1]   = bs1 ? TC_CALC(qp, bs1) : 0;
-                src     = &s->frame->data[LUMA][y * s->frame->linesize[LUMA] + (x << s->ps.sps->pixel_shift)];
                 if (pcmf) {
                     no_p[0] = get_pcm(s, x - 1, y);
                     no_p[1] = get_pcm(s, x - 1, y + 4);
                     no_q[0] = get_pcm(s, x, y);
                     no_q[1] = get_pcm(s, x, y + 4);
-                    s->hevcdsp.hevc_v_loop_filter_luma_c(src,
-                                                         s->frame->linesize[LUMA],
-                                                         beta, tc, no_p, no_q);
-                } else
-                    s->hevcdsp.hevc_v_loop_filter_luma(src,
-                                                       s->frame->linesize[LUMA],
-                                                       beta, tc, no_p, no_q);
+                }
+#ifdef RPI
+                if (rpi_sliced_frame(s->frame)) {
+
+                    // This copes properly with no_p/no_q
+                    s->hevcdsp.hevc_v_loop_filter_luma2(rpi_sliced_frame_pos_y(s->frame, x, y),
+                                                     s->frame->linesize[LUMA],
+                                                     beta, tc, no_p, no_q,
+                                                     rpi_sliced_frame_pos_y(s->frame, x - 4, y));
+                }
+                else
+#endif
+                {
+                    src = &s->frame->data[LUMA][y * s->frame->linesize[LUMA] + (x << s->ps.sps->pixel_shift)];
+                    if (pcmf) {
+                        // Standard DSP code is broken if no_p / no_q is set
+                        s->hevcdsp.hevc_v_loop_filter_luma_c(src,
+                                                           s->frame->linesize[LUMA],
+                                                           beta, tc, no_p, no_q);
+                    }
+                    else
+#ifdef RPI_DEBLOCK_VPU
+                    if (s->enable_rpi_deblock) {
+                        uint8_t (*setup)[2][2][4];
+                        int num16 = (y>>4)*s->setup_width + (x>>4);
+                        int a = ((y>>3) & 1) << 1;
+                        int b = (x>>3) & 1;
+                        setup = s->dvq->y_setup_arm[num16];
+                        setup[0][b][0][a] = beta;
+                        setup[0][b][0][a + 1] = beta;
+                        setup[0][b][1][a] = tc[0];
+                        setup[0][b][1][a + 1] = tc[1];
+                    } else
+#endif
+                    {
+                        s->hevcdsp.hevc_v_loop_filter_luma(src,
+                                                           s->frame->linesize[LUMA],
+                                                           beta, tc, no_p, no_q);
+                    }
+                }
             }
         }
 
@@ -560,7 +696,12 @@
                 beta = betatable[av_clip(qp + beta_offset, 0, MAX_QP)];
                 tc[0]   = bs0 ? TC_CALC(qp, bs0) : 0;
                 tc[1]   = bs1 ? TC_CALC(qp, bs1) : 0;
-                src     = &s->frame->data[LUMA][y * s->frame->linesize[LUMA] + (x << s->ps.sps->pixel_shift)];
+                src =
+#ifdef RPI
+                    rpi_sliced_frame(s->frame) ?
+                        rpi_sliced_frame_pos_y(s->frame, x, y) :
+#endif
+                        &s->frame->data[LUMA][y * s->frame->linesize[LUMA] + (x << s->ps.sps->pixel_shift)];
                 if (pcmf) {
                     no_p[0] = get_pcm(s, x, y - 1);
                     no_p[1] = get_pcm(s, x + 4, y - 1);
@@ -570,6 +711,19 @@
                                                          s->frame->linesize[LUMA],
                                                          beta, tc, no_p, no_q);
                 } else
+#ifdef RPI_DEBLOCK_VPU
+                if (s->enable_rpi_deblock) {
+                    uint8_t (*setup)[2][2][4];
+                    int num16 = (y>>4)*s->setup_width + (x>>4);
+                    int a = ((x>>3) & 1) << 1;
+                    int b = (y>>3) & 1;
+                    setup = s->dvq->y_setup_arm[num16];
+                    setup[1][b][0][a] = beta;
+                    setup[1][b][0][a + 1] = beta;
+                    setup[1][b][1][a] = tc[0];
+                    setup[1][b][1][a + 1] = tc[1];
+                } else
+#endif
                     s->hevcdsp.hevc_h_loop_filter_luma(src,
                                                        s->frame->linesize[LUMA],
                                                        beta, tc, no_p, no_q);
@@ -578,6 +732,91 @@
     }
 
     if (s->ps.sps->chroma_format_idc) {
+#ifdef RPI
+        if (rpi_sliced_frame(s->frame)) {
+            const int v = 2;
+            const int h = 2;
+
+            // vertical filtering chroma
+            for (y = y0; y < y_end; y += 8 * v) {
+                for (x = x0 ? x0 : 8 * h; x < x_end; x += 8 * h) {
+                    const int bs0 = s->vertical_bs[(x +  y          * s->bs_width) >> 2];
+                    const int bs1 = s->vertical_bs[(x + (y + 4 * v) * s->bs_width) >> 2];
+
+                    if ((bs0 == 2) || (bs1 == 2)) {
+                        const int qp0 = (get_qPy(s, x - 1, y)         + get_qPy(s, x, y)         + 1) >> 1;
+                        const int qp1 = (get_qPy(s, x - 1, y + 4 * v) + get_qPy(s, x, y + 4 * v) + 1) >> 1;
+                        unsigned int no_f = 0;
+
+                        // tc_offset here should be set to cur_tc_offset I think
+                        const uint32_t tc4 =
+                            ((bs0 != 2) ? 0 : chroma_tc(s, qp0, 1, cur_tc_offset) | (chroma_tc(s, qp0, 2, cur_tc_offset) << 16)) |
+                            ((bs1 != 2) ? 0 : ((chroma_tc(s, qp1, 1, cur_tc_offset) | (chroma_tc(s, qp1, 2, cur_tc_offset) << 16)) << 8));
+
+                        if (tc4 == 0)
+                            continue;
+
+                        if (pcmf) {
+                            no_f =
+                                (get_pcm(s, x - 1, y) ? 1 : 0) |
+                                (get_pcm(s, x - 1, y + 4 * v) ? 2 : 0) |
+                                (get_pcm(s, x, y) ? 4 : 0) |
+                                (get_pcm(s, x, y + 4 * v) ? 8 : 0);
+                            if (no_f == 0xf)
+                                continue;
+                        }
+
+                        s->hevcdsp.hevc_v_loop_filter_uv2(rpi_sliced_frame_pos_c(s->frame, x >> 1, y >> 1),
+                                                       s->frame->linesize[1],
+                                                       tc4,
+                                                       rpi_sliced_frame_pos_c(s->frame, (x >> 1) - 2, y >> 1),
+                                                       no_f);
+                    }
+                }
+
+                if (y == 0)
+                    continue;
+
+                // horizontal filtering chroma
+                tc_offset = x0 ? left_tc_offset : cur_tc_offset;
+                x_end2 = x_end;
+                if (x_end != s->ps.sps->width)
+                    x_end2 = x_end - 8 * h;
+
+                for (x = x0 ? x0 - 8 * h: 0; x < x_end2; x += 8 * h) {
+                    const int bs0 = s->horizontal_bs[( x          + y * s->bs_width) >> 2];
+                    const int bs1 = s->horizontal_bs[((x + 4 * h) + y * s->bs_width) >> 2];
+                    if ((bs0 == 2) || (bs1 == 2)) {
+                        const int qp0 = bs0 == 2 ? (get_qPy(s, x,         y - 1) + get_qPy(s, x,         y) + 1) >> 1 : 0;
+                        const int qp1 = bs1 == 2 ? (get_qPy(s, x + 4 * h, y - 1) + get_qPy(s, x + 4 * h, y) + 1) >> 1 : 0;
+                        const uint32_t tc4 =
+                            ((bs0 != 2) ? 0 : chroma_tc(s, qp0, 1, tc_offset) | (chroma_tc(s, qp0, 2, tc_offset) << 16)) |
+                            ((bs1 != 2) ? 0 : ((chroma_tc(s, qp1, 1, cur_tc_offset) | (chroma_tc(s, qp1, 2, cur_tc_offset) << 16)) << 8));
+                        unsigned int no_f = 0;
+
+                        if (tc4 == 0)
+                            continue;
+
+                        if (pcmf) {
+                            no_f =
+                                (get_pcm(s, x,         y - 1) ? 1 : 0) |
+                                (get_pcm(s, x + 4 * h, y - 1) ? 2 : 0) |
+                                (get_pcm(s, x,         y)     ? 4 : 0) |
+                                (get_pcm(s, x + 4 * h, y)     ? 8 : 0);
+
+                            if (no_f == 0xf)
+                                continue;
+                        }
+
+                        s->hevcdsp.hevc_h_loop_filter_uv(rpi_sliced_frame_pos_c(s->frame, x >> 1, y >> 1),
+                                                             s->frame->linesize[1],
+                                                             tc4, no_f);
+                    }
+                }
+            }
+        }
+        else
+#endif
         for (chroma = 1; chroma <= 2; chroma++) {
             int h = 1 << s->ps.sps->hshift[chroma];
             int v = 1 << s->ps.sps->vshift[chroma];
@@ -594,7 +833,12 @@
 
                         c_tc[0] = (bs0 == 2) ? chroma_tc(s, qp0, chroma, tc_offset) : 0;
                         c_tc[1] = (bs1 == 2) ? chroma_tc(s, qp1, chroma, tc_offset) : 0;
-                        src       = &s->frame->data[chroma][(y >> s->ps.sps->vshift[chroma]) * s->frame->linesize[chroma] + ((x >> s->ps.sps->hshift[chroma]) << s->ps.sps->pixel_shift)];
+                        src =
+#ifdef RPI
+                            rpi_sliced_frame(s->frame) ?
+                                rpi_sliced_frame_pos_c(s->frame, x >> s->ps.sps->hshift[chroma], y >> s->ps.sps->vshift[chroma]) :
+#endif
+                                &s->frame->data[chroma][(y >> s->ps.sps->vshift[chroma]) * s->frame->linesize[chroma] + ((x >> s->ps.sps->hshift[chroma]) << s->ps.sps->pixel_shift)];
                         if (pcmf) {
                             no_p[0] = get_pcm(s, x - 1, y);
                             no_p[1] = get_pcm(s, x - 1, y + (4 * v));
@@ -604,9 +848,23 @@
                                                                    s->frame->linesize[chroma],
                                                                    c_tc, no_p, no_q);
                         } else
+#ifdef RPI_DEBLOCK_VPU
+                        if (s->enable_rpi_deblock) {
+                            uint8_t (*setup)[2][2][4];
+                            int xc = x>>s->ps.sps->hshift[chroma];
+                            int yc = y>>s->ps.sps->vshift[chroma];
+                            int num16 = (yc>>4)*s->uv_setup_width + (xc>>4);
+                            int a = ((yc>>3) & 1) << 1;
+                            int b = (xc>>3) & 1;
+                            setup = s->dvq->uv_setup_arm[num16];
+                            setup[0][b][0][a] = c_tc[0];
+                            setup[0][b][0][a + 1] = c_tc[1];
+                        } else
+#endif
                             s->hevcdsp.hevc_v_loop_filter_chroma(src,
                                                                  s->frame->linesize[chroma],
                                                                  c_tc, no_p, no_q);
+
                     }
                 }
 
@@ -627,7 +885,12 @@
 
                         c_tc[0]   = bs0 == 2 ? chroma_tc(s, qp0, chroma, tc_offset)     : 0;
                         c_tc[1]   = bs1 == 2 ? chroma_tc(s, qp1, chroma, cur_tc_offset) : 0;
-                        src       = &s->frame->data[chroma][(y >> s->ps.sps->vshift[1]) * s->frame->linesize[chroma] + ((x >> s->ps.sps->hshift[1]) << s->ps.sps->pixel_shift)];
+                        src =
+#ifdef RPI
+                            rpi_sliced_frame(s->frame) ?
+                                rpi_sliced_frame_pos_c(s->frame, x >> s->ps.sps->hshift[chroma], y >> s->ps.sps->vshift[chroma]) :
+#endif
+                                &s->frame->data[chroma][(y >> s->ps.sps->vshift[1]) * s->frame->linesize[chroma] + ((x >> s->ps.sps->hshift[1]) << s->ps.sps->pixel_shift)];
                         if (pcmf) {
                             no_p[0] = get_pcm(s, x,           y - 1);
                             no_p[1] = get_pcm(s, x + (4 * h), y - 1);
@@ -637,6 +900,19 @@
                                                                    s->frame->linesize[chroma],
                                                                    c_tc, no_p, no_q);
                         } else
+#ifdef RPI_DEBLOCK_VPU
+                        if (s->enable_rpi_deblock) {
+                            uint8_t (*setup)[2][2][4];
+                            int xc = x>>s->ps.sps->hshift[chroma];
+                            int yc = y>>s->ps.sps->vshift[chroma];
+                            int num16 = (yc>>4)*s->uv_setup_width + (xc>>4);
+                            int a = ((xc>>3) & 1) << 1;
+                            int b = (yc>>3) & 1;
+                            setup = s->dvq->uv_setup_arm[num16];
+                            setup[1][b][0][a] = c_tc[0];
+                            setup[1][b][0][a + 1] = c_tc[1];
+                        } else
+#endif
                             s->hevcdsp.hevc_h_loop_filter_chroma(src,
                                                                  s->frame->linesize[chroma],
                                                                  c_tc, no_p, no_q);
@@ -647,69 +923,6 @@
     }
 }
 
-static int boundary_strength(HEVCContext *s, MvField *curr, MvField *neigh,
-                             RefPicList *neigh_refPicList)
-{
-    if (curr->pred_flag == PF_BI &&  neigh->pred_flag == PF_BI) {
-        // same L0 and L1
-        if (s->ref->refPicList[0].list[curr->ref_idx[0]] == neigh_refPicList[0].list[neigh->ref_idx[0]]  &&
-            s->ref->refPicList[0].list[curr->ref_idx[0]] == s->ref->refPicList[1].list[curr->ref_idx[1]] &&
-            neigh_refPicList[0].list[neigh->ref_idx[0]] == neigh_refPicList[1].list[neigh->ref_idx[1]]) {
-            if ((FFABS(neigh->mv[0].x - curr->mv[0].x) >= 4 || FFABS(neigh->mv[0].y - curr->mv[0].y) >= 4 ||
-                 FFABS(neigh->mv[1].x - curr->mv[1].x) >= 4 || FFABS(neigh->mv[1].y - curr->mv[1].y) >= 4) &&
-                (FFABS(neigh->mv[1].x - curr->mv[0].x) >= 4 || FFABS(neigh->mv[1].y - curr->mv[0].y) >= 4 ||
-                 FFABS(neigh->mv[0].x - curr->mv[1].x) >= 4 || FFABS(neigh->mv[0].y - curr->mv[1].y) >= 4))
-                return 1;
-            else
-                return 0;
-        } else if (neigh_refPicList[0].list[neigh->ref_idx[0]] == s->ref->refPicList[0].list[curr->ref_idx[0]] &&
-                   neigh_refPicList[1].list[neigh->ref_idx[1]] == s->ref->refPicList[1].list[curr->ref_idx[1]]) {
-            if (FFABS(neigh->mv[0].x - curr->mv[0].x) >= 4 || FFABS(neigh->mv[0].y - curr->mv[0].y) >= 4 ||
-                FFABS(neigh->mv[1].x - curr->mv[1].x) >= 4 || FFABS(neigh->mv[1].y - curr->mv[1].y) >= 4)
-                return 1;
-            else
-                return 0;
-        } else if (neigh_refPicList[1].list[neigh->ref_idx[1]] == s->ref->refPicList[0].list[curr->ref_idx[0]] &&
-                   neigh_refPicList[0].list[neigh->ref_idx[0]] == s->ref->refPicList[1].list[curr->ref_idx[1]]) {
-            if (FFABS(neigh->mv[1].x - curr->mv[0].x) >= 4 || FFABS(neigh->mv[1].y - curr->mv[0].y) >= 4 ||
-                FFABS(neigh->mv[0].x - curr->mv[1].x) >= 4 || FFABS(neigh->mv[0].y - curr->mv[1].y) >= 4)
-                return 1;
-            else
-                return 0;
-        } else {
-            return 1;
-        }
-    } else if ((curr->pred_flag != PF_BI) && (neigh->pred_flag != PF_BI)){ // 1 MV
-        Mv A, B;
-        int ref_A, ref_B;
-
-        if (curr->pred_flag & 1) {
-            A     = curr->mv[0];
-            ref_A = s->ref->refPicList[0].list[curr->ref_idx[0]];
-        } else {
-            A     = curr->mv[1];
-            ref_A = s->ref->refPicList[1].list[curr->ref_idx[1]];
-        }
-
-        if (neigh->pred_flag & 1) {
-            B     = neigh->mv[0];
-            ref_B = neigh_refPicList[0].list[neigh->ref_idx[0]];
-        } else {
-            B     = neigh->mv[1];
-            ref_B = neigh_refPicList[1].list[neigh->ref_idx[1]];
-        }
-
-        if (ref_A == ref_B) {
-            if (FFABS(A.x - B.x) >= 4 || FFABS(A.y - B.y) >= 4)
-                return 1;
-            else
-                return 0;
-        } else
-            return 1;
-    }
-
-    return 1;
-}
 
 void ff_hevc_deblocking_boundary_strengths(HEVCContext *s, int x0, int y0,
                                            int log2_trafo_size)
@@ -720,10 +933,22 @@
     int log2_min_tu_size = s->ps.sps->log2_min_tb_size;
     int min_pu_width     = s->ps.sps->min_pu_width;
     int min_tu_width     = s->ps.sps->min_tb_width;
-    int is_intra = tab_mvf[(y0 >> log2_min_pu_size) * min_pu_width +
-                           (x0 >> log2_min_pu_size)].pred_flag == PF_INTRA;
     int boundary_upper, boundary_left;
-    int i, j, bs;
+    int i, j;
+    RefPicList *rpl      = s->ref->refPicList;
+    const unsigned int log2_dup = FFMIN(log2_min_pu_size, log2_trafo_size);
+    const unsigned int min_pu_in_4pix = 1 << (log2_dup - 2);  // Dup
+    const unsigned int trafo_in_min_pus = 1 << (log2_trafo_size - log2_dup); // Rep
+    int y_pu             = y0 >> log2_min_pu_size;
+    int x_pu             = x0 >> log2_min_pu_size;
+    MvField *curr        = &tab_mvf[y_pu * min_pu_width + x_pu];
+    int is_intra         = curr->pred_flag == PF_INTRA;
+    int inc              = log2_min_pu_size == 2 ? 2 : 1;
+    uint8_t *bs;
+
+#ifdef DISABLE_STRENGTHS
+    return;
+#endif
 
     boundary_upper = y0 > 0 && !(y0 & 7);
     if (boundary_upper &&
@@ -735,34 +960,56 @@
           (y0 % (1 << s->ps.sps->log2_ctb_size)) == 0)))
         boundary_upper = 0;
 
+    bs = &s->horizontal_bs[(x0 + y0 * s->bs_width) >> 2];
+
     if (boundary_upper) {
         RefPicList *rpl_top = (lc->boundary_flags & BOUNDARY_UPPER_SLICE) ?
                               ff_hevc_get_ref_list(s, s->ref, x0, y0 - 1) :
-                              s->ref->refPicList;
-        int yp_pu = (y0 - 1) >> log2_min_pu_size;
-        int yq_pu =  y0      >> log2_min_pu_size;
-        int yp_tu = (y0 - 1) >> log2_min_tu_size;
-        int yq_tu =  y0      >> log2_min_tu_size;
+                              rpl;
+        MvField *top = curr - min_pu_width;
+
+        if (is_intra) {
+            for (i = 0; i < (1 << log2_trafo_size); i += 4)
+                bs[i >> 2] = 2;
+
+        } else {
+            int y_tu = y0 >> log2_min_tu_size;
+            int x_tu = x0 >> log2_min_tu_size;
+            uint8_t *curr_cbf_luma = &s->cbf_luma[y_tu * min_tu_width + x_tu];
+            uint8_t *top_cbf_luma = curr_cbf_luma - min_tu_width;
+
+            s->hevcdsp.hevc_deblocking_boundary_strengths(trafo_in_min_pus,
+                    min_pu_in_4pix, sizeof (MvField), 4 >> 2,
+                    rpl[0].list, rpl[1].list, rpl_top[0].list, rpl_top[1].list,
+                    curr, top, bs);
 
             for (i = 0; i < (1 << log2_trafo_size); i += 4) {
-                int x_pu = (x0 + i) >> log2_min_pu_size;
-                int x_tu = (x0 + i) >> log2_min_tu_size;
-                MvField *top  = &tab_mvf[yp_pu * min_pu_width + x_pu];
-                MvField *curr = &tab_mvf[yq_pu * min_pu_width + x_pu];
-                uint8_t top_cbf_luma  = s->cbf_luma[yp_tu * min_tu_width + x_tu];
-                uint8_t curr_cbf_luma = s->cbf_luma[yq_tu * min_tu_width + x_tu];
-
-                if (curr->pred_flag == PF_INTRA || top->pred_flag == PF_INTRA)
-                    bs = 2;
-                else if (curr_cbf_luma || top_cbf_luma)
-                    bs = 1;
-                else
-                    bs = boundary_strength(s, curr, top, rpl_top);
-                s->horizontal_bs[((x0 + i) + y0 * s->bs_width) >> 2] = bs;
+                int i_pu = i >> log2_min_pu_size;
+                int i_tu = i >> log2_min_tu_size;
+
+                if (top[i_pu].pred_flag == PF_INTRA)
+                    bs[i >> 2] = 2;
+                else if (curr_cbf_luma[i_tu] || top_cbf_luma[i_tu])
+                    bs[i >> 2] = 1;
             }
+        }
+    }
+
+    if (!is_intra) {
+        for (j = inc; j < trafo_in_min_pus; j += inc) {
+            MvField *top;
+
+            curr += min_pu_width * inc;
+            top = curr - min_pu_width;
+            bs += s->bs_width * inc << log2_min_pu_size >> 2;
+
+            s->hevcdsp.hevc_deblocking_boundary_strengths(trafo_in_min_pus,
+                    min_pu_in_4pix, sizeof (MvField), 4 >> 2,
+                    rpl[0].list, rpl[1].list, rpl[0].list, rpl[1].list,
+                    curr, top, bs);
+        }
     }
 
-    // bs for vertical TU boundaries
     boundary_left = x0 > 0 && !(x0 & 7);
     if (boundary_left &&
         ((!s->sh.slice_loop_filter_across_slices_enabled_flag &&
@@ -773,77 +1020,160 @@
           (x0 % (1 << s->ps.sps->log2_ctb_size)) == 0)))
         boundary_left = 0;
 
+    curr = &tab_mvf[y_pu * min_pu_width + x_pu];
+    bs = &s->vertical_bs[(x0 + y0 * s->bs_width) >> 2];
+
     if (boundary_left) {
         RefPicList *rpl_left = (lc->boundary_flags & BOUNDARY_LEFT_SLICE) ?
                                ff_hevc_get_ref_list(s, s->ref, x0 - 1, y0) :
-                               s->ref->refPicList;
-        int xp_pu = (x0 - 1) >> log2_min_pu_size;
-        int xq_pu =  x0      >> log2_min_pu_size;
-        int xp_tu = (x0 - 1) >> log2_min_tu_size;
-        int xq_tu =  x0      >> log2_min_tu_size;
+                               rpl;
+        MvField *left = curr - 1;
 
-            for (i = 0; i < (1 << log2_trafo_size); i += 4) {
-                int y_pu      = (y0 + i) >> log2_min_pu_size;
-                int y_tu      = (y0 + i) >> log2_min_tu_size;
-                MvField *left = &tab_mvf[y_pu * min_pu_width + xp_pu];
-                MvField *curr = &tab_mvf[y_pu * min_pu_width + xq_pu];
-                uint8_t left_cbf_luma = s->cbf_luma[y_tu * min_tu_width + xp_tu];
-                uint8_t curr_cbf_luma = s->cbf_luma[y_tu * min_tu_width + xq_tu];
-
-                if (curr->pred_flag == PF_INTRA || left->pred_flag == PF_INTRA)
-                    bs = 2;
-                else if (curr_cbf_luma || left_cbf_luma)
-                    bs = 1;
-                else
-                    bs = boundary_strength(s, curr, left, rpl_left);
-                s->vertical_bs[(x0 + (y0 + i) * s->bs_width) >> 2] = bs;
+        if (is_intra) {
+            for (j = 0; j < (1 << log2_trafo_size); j += 4)
+                bs[j * s->bs_width >> 2] = 2;
+
+        } else {
+            int y_tu = y0 >> log2_min_tu_size;
+            int x_tu = x0 >> log2_min_tu_size;
+            uint8_t *curr_cbf_luma = &s->cbf_luma[y_tu * min_tu_width + x_tu];
+            uint8_t *left_cbf_luma = curr_cbf_luma - 1;
+
+            s->hevcdsp.hevc_deblocking_boundary_strengths(trafo_in_min_pus,
+                    min_pu_in_4pix, min_pu_width * sizeof (MvField), 4 * s->bs_width >> 2,
+                    rpl[0].list, rpl[1].list, rpl_left[0].list, rpl_left[1].list,
+                    curr, left, bs);
+
+            for (j = 0; j < (1 << log2_trafo_size); j += 4) {
+                int j_pu = j >> log2_min_pu_size;
+                int j_tu = j >> log2_min_tu_size;
+
+                if (left[j_pu * min_pu_width].pred_flag == PF_INTRA)
+                    bs[j * s->bs_width >> 2] = 2;
+                else if (curr_cbf_luma[j_tu * min_tu_width] || left_cbf_luma[j_tu * min_tu_width])
+                    bs[j * s->bs_width >> 2] = 1;
             }
+        }
     }
 
-    if (log2_trafo_size > log2_min_pu_size && !is_intra) {
-        RefPicList *rpl = s->ref->refPicList;
+    if (!is_intra) {
+        for (i = inc; i < trafo_in_min_pus; i += inc) {
+            MvField *left;
+
+            curr += inc;
+            left = curr - 1;
+            bs += inc << log2_min_pu_size >> 2;
+
+            s->hevcdsp.hevc_deblocking_boundary_strengths(trafo_in_min_pus,
+                    min_pu_in_4pix, min_pu_width * sizeof (MvField), 4 * s->bs_width >> 2,
+                    rpl[0].list, rpl[1].list, rpl[0].list, rpl[1].list,
+                    curr, left, bs);
+        }
+    }
+}
 
-        // bs for TU internal horizontal PU boundaries
-        for (j = 8; j < (1 << log2_trafo_size); j += 8) {
-            int yp_pu = (y0 + j - 1) >> log2_min_pu_size;
-            int yq_pu = (y0 + j)     >> log2_min_pu_size;
+#undef LUMA
+#undef CB
+#undef CR
 
-            for (i = 0; i < (1 << log2_trafo_size); i += 4) {
-                int x_pu = (x0 + i) >> log2_min_pu_size;
-                MvField *top  = &tab_mvf[yp_pu * min_pu_width + x_pu];
-                MvField *curr = &tab_mvf[yq_pu * min_pu_width + x_pu];
+#ifdef RPI_DEBLOCK_VPU
+// ff_hevc_flush_buffer_lines
+// flushes and invalidates all pixel rows in [start,end-1]
+static void ff_hevc_flush_buffer_lines(HEVCContext *s, int start, int end, int flush_luma, int flush_chroma)
+{
+    rpi_cache_flush_env_t * const rfe = rpi_cache_flush_init();
+    rpi_cache_flush_add_frame_lines(rfe, s->frame, RPI_CACHE_FLUSH_MODE_WB_INVALIDATE,
+      start, end - start, s->ps.sps->vshift[1], flush_luma, flush_chroma);
+    rpi_cache_flush_finish(rfe);
+}
+#endif
 
-                bs = boundary_strength(s, curr, top, rpl);
-                s->horizontal_bs[((x0 + i) + (y0 + j) * s->bs_width) >> 2] = bs;
-            }
+#if RPI_INTER
+
+// Flush some lines of a reference frames
+void rpi_flush_ref_frame_progress(HEVCContext * const s, ThreadFrame * const f, const unsigned int n)
+{
+    if (s->enable_rpi && s->used_for_ref) {
+        const int d0 = ((int *)f->progress->data)[0];
+        const unsigned int curr_y = d0 == -1 ? 0 : d0;  // At start of time progress is -1
+
+        if (curr_y < (unsigned int)f->f->height) {
+            rpi_cache_flush_env_t * const rfe = rpi_cache_flush_init();
+            rpi_cache_flush_add_frame_lines(rfe, s->frame, RPI_CACHE_FLUSH_MODE_WB_INVALIDATE,
+              curr_y, FFMIN(n, (unsigned int)f->f->height) - curr_y, s->ps.sps->vshift[1], 1, 1);
+            rpi_cache_flush_finish(rfe);
         }
+    }
+}
+#endif
 
-        // bs for TU internal vertical PU boundaries
-        for (j = 0; j < (1 << log2_trafo_size); j += 4) {
-            int y_pu = (y0 + j) >> log2_min_pu_size;
+#ifdef RPI_DEBLOCK_VPU
+/* rpi_deblock deblocks an entire row of ctbs using the VPU */
+static void rpi_deblock(HEVCContext *s, int y, int ctb_size)
+{
+  // Flush image, 4 lines above to bottom of ctb stripe
+  ff_hevc_flush_buffer_lines(s, FFMAX(y-4,0), y+ctb_size, 1, 1);
+  // TODO flush buffer of beta/tc setup when it becomes cached
+
+  // Prepare three commands at once to avoid calling overhead
+  s->dvq->vpu_cmds_arm[0][0] = get_vc_address_y(s->frame) + s->frame->linesize[0] * y;
+  s->dvq->vpu_cmds_arm[0][1] = s->frame->linesize[0];
+  s->dvq->vpu_cmds_arm[0][2] = s->setup_width;
+  s->dvq->vpu_cmds_arm[0][3] = (int) ( s->dvq->y_setup_vc + s->setup_width * (y>>4) );
+  s->dvq->vpu_cmds_arm[0][4] = ctb_size>>4;
+  s->dvq->vpu_cmds_arm[0][5] = 2;
+
+  s->dvq->vpu_cmds_arm[1][0] = get_vc_address_u(s->frame) + s->frame->linesize[1] * (y>> s->ps.sps->vshift[1]);
+  s->dvq->vpu_cmds_arm[1][1] = s->frame->linesize[1];
+  s->dvq->vpu_cmds_arm[1][2] = s->uv_setup_width;
+  s->dvq->vpu_cmds_arm[1][3] = (int) ( s->dvq->uv_setup_vc + s->uv_setup_width * ((y>>4)>> s->ps.sps->vshift[1]) );
+  s->dvq->vpu_cmds_arm[1][4] = (ctb_size>>4)>> s->ps.sps->vshift[1];
+  s->dvq->vpu_cmds_arm[1][5] = 3;
+
+  s->dvq->vpu_cmds_arm[2][0] = get_vc_address_v(s->frame) + s->frame->linesize[2] * (y>> s->ps.sps->vshift[2]);
+  s->dvq->vpu_cmds_arm[2][1] = s->frame->linesize[2];
+  s->dvq->vpu_cmds_arm[2][2] = s->uv_setup_width;
+  s->dvq->vpu_cmds_arm[2][3] = (int) ( s->dvq->uv_setup_vc + s->uv_setup_width * ((y>>4)>> s->ps.sps->vshift[1]) );
+  s->dvq->vpu_cmds_arm[2][4] = (ctb_size>>4)>> s->ps.sps->vshift[1];
+  s->dvq->vpu_cmds_arm[2][5] = 4;
+
+  // Call VPU
+  {
+      const vpu_qpu_job_h vqj = vpu_qpu_job_new();
+      vpu_qpu_job_add_vpu(vqj, vpu_get_fn(), s->dvq->vpu_cmds_vc, 3, 0, 0, 0, 5);  // 5 means to do all the commands
+      vpu_qpu_job_add_sync_this(vqj, &s->dvq->cmd_id);
+      vpu_qpu_job_finish(vqj);
+  }
 
-            for (i = 8; i < (1 << log2_trafo_size); i += 8) {
-                int xp_pu = (x0 + i - 1) >> log2_min_pu_size;
-                int xq_pu = (x0 + i)     >> log2_min_pu_size;
-                MvField *left = &tab_mvf[y_pu * min_pu_width + xp_pu];
-                MvField *curr = &tab_mvf[y_pu * min_pu_width + xq_pu];
+  s->dvq_n = (s->dvq_n + 1) & (RPI_DEBLOCK_VPU_Q_COUNT - 1);
+  s->dvq = s->dvq_ents + s->dvq_n;
 
-                bs = boundary_strength(s, curr, left, rpl);
-                s->vertical_bs[((x0 + i) + (y0 + j) * s->bs_width) >> 2] = bs;
-            }
-        }
-    }
+  vpu_qpu_wait(&s->dvq->cmd_id);
 }
 
-#undef LUMA
-#undef CB
-#undef CR
+#endif
 
 void ff_hevc_hls_filter(HEVCContext *s, int x, int y, int ctb_size)
 {
     int x_end = x >= s->ps.sps->width  - ctb_size;
+#ifdef RPI_DEBLOCK_VPU
+    int done_deblock = 0;
+#endif
     if (s->avctx->skip_loop_filter < AVDISCARD_ALL)
         deblocking_filter_CTB(s, x, y);
+#ifdef RPI_DEBLOCK_VPU
+    if (s->enable_rpi_deblock && x_end)
+    {
+      int y_at_end = y >= s->ps.sps->height - ctb_size;
+      int height = 64;  // Deblock in units 64 high to avoid too many VPU calls
+      int y_start = y&~63;
+      if (y_at_end) height = s->ps.sps->height - y_start;
+      if ((((y+ctb_size)&63)==0) || y_at_end) {
+        done_deblock = 1;
+        rpi_deblock(s, y_start, height);
+      }
+    }
+#endif
     if (s->ps.sps->sao_enabled) {
         int y_end = y >= s->ps.sps->height - ctb_size;
         if (y && x)
@@ -852,16 +1182,46 @@
             sao_filter_CTB(s, x - ctb_size, y);
         if (y && x_end) {
             sao_filter_CTB(s, x, y - ctb_size);
-            if (s->threads_type & FF_THREAD_FRAME )
+            if (s->threads_type == FF_THREAD_FRAME ) {
+#if RPI_INTER
+                rpi_flush_ref_frame_progress(s,&s->ref->tf, y);
+#endif
                 ff_thread_report_progress(&s->ref->tf, y, 0);
+            }
         }
         if (x_end && y_end) {
             sao_filter_CTB(s, x , y);
-            if (s->threads_type & FF_THREAD_FRAME )
+            if (s->threads_type == FF_THREAD_FRAME ) {
+#if RPI_INTER
+                rpi_flush_ref_frame_progress(s, &s->ref->tf, y + ctb_size);
+#endif
                 ff_thread_report_progress(&s->ref->tf, y + ctb_size, 0);
+            }
         }
-    } else if (s->threads_type & FF_THREAD_FRAME && x_end)
+    } else if (s->threads_type == FF_THREAD_FRAME && x_end) {
+        //int newh = y + ctb_size - 4;
+        //int currh = s->ref->tf.progress->data[0];
+        //if (((y + ctb_size)&63)==0)
+#ifdef RPI_DEBLOCK_VPU
+        if (s->enable_rpi_deblock) {
+          // we no longer need to flush the luma buffer as it is in GPU memory when using deblocking on the rpi
+          if (done_deblock) {
+            ff_thread_report_progress(&s->ref->tf, y + ctb_size - 4, 0);
+          }
+        } else {
+#if RPI_INTER
+          rpi_flush_ref_frame_progress(s, &s->ref->tf, y + ctb_size - 4);
+#endif
+          ff_thread_report_progress(&s->ref->tf, y + ctb_size - 4, 0);
+        }
+#else
+#if RPI_INTER
+        rpi_flush_ref_frame_progress(s, &s->ref->tf, y + ctb_size - 4);
+        // we no longer need to flush the luma buffer as it is in GPU memory when using deblocking on the rpi
+#endif
         ff_thread_report_progress(&s->ref->tf, y + ctb_size - 4, 0);
+#endif
+    }
 }
 
 void ff_hevc_hls_filters(HEVCContext *s, int x_ctb, int y_ctb, int ctb_size)
diff -Naur ffmpeg-3.2.4/libavcodec/hevc.h ffmpeg-3.2.4.patch/libavcodec/hevc.h
--- ffmpeg-3.2.4/libavcodec/hevc.h	2017-02-10 14:25:26.000000000 +0100
+++ ffmpeg-3.2.4.patch/libavcodec/hevc.h	2017-05-28 20:42:45.736088659 +0200
@@ -23,6 +23,9 @@
 #ifndef AVCODEC_HEVC_H
 #define AVCODEC_HEVC_H
 
+// define RPI to split the CABAC/prediction/transform into separate stages
+#include "config.h"
+
 #include "libavutil/buffer.h"
 #include "libavutil/md5.h"
 
@@ -37,6 +40,45 @@
 #include "thread.h"
 #include "videodsp.h"
 
+// define RPI to split the CABAC/prediction/transform into separate stages
+#ifndef RPI
+
+  #define RPI_INTER          0
+  #define RPI_TSTATS         0
+  #define RPI_HEVC_SAND      0
+
+#else
+
+  #include "rpi_qpu.h"
+  #define RPI_INTER          1          // 0 use ARM for UV inter-pred, 1 use QPU
+
+  // Define RPI_WORKER to launch a worker thread for pixel processing tasks
+  #define RPI_WORKER
+  // By passing jobs to a worker thread we hope to be able to catch up during slow frames
+  // This has no effect unless RPI_WORKER is defined
+  // N.B. The extra thread count is effectively RPI_MAX_JOBS - 1 as
+  // RPI_MAX_JOBS defines the number of worker parameter sets and we must have one
+  // free for the foreground to fill in.
+  #define RPI_MAX_JOBS 2
+
+  // Define RPI_DEBLOCK_VPU to perform deblocking on the VPUs
+  // As it stands there is something mildy broken in VPU deblock - looks mostly OK
+  // but reliably fails some conformance tests (e.g. DBLK_A/B/C_)
+  // With VPU luma & chroma pred it is much the same speed to deblock on the ARM
+//  #define RPI_DEBLOCK_VPU
+
+  #define RPI_VPU_DEBLOCK_CACHED 1
+
+  #if HAVE_NEON
+  #define RPI_HEVC_SAND      1
+  #else
+  // Sand bust on Pi1 currently - reasons unknown
+  #define RPI_HEVC_SAND      0
+  #endif
+
+  #define RPI_TSTATS 0
+#endif
+
 #define MAX_DPB_SIZE 16 // A.4.1
 #define MAX_REFS 16
 
@@ -669,17 +711,6 @@
     uint8_t cu_transquant_bypass_flag;
 } CodingUnit;
 
-typedef struct Mv {
-    int16_t x;  ///< horizontal component of motion vector
-    int16_t y;  ///< vertical component of motion vector
-} Mv;
-
-typedef struct MvField {
-    DECLARE_ALIGNED(4, Mv, mv)[2];
-    int8_t ref_idx[2];
-    int8_t pred_flag;
-} MvField;
-
 typedef struct NeighbourAvailable {
     int cand_bottom_left;
     int cand_left;
@@ -756,7 +787,17 @@
     uint8_t flags;
 } HEVCFrame;
 
+#ifdef RPI_WORKER
+typedef struct HEVCLocalContextIntra {
+    TransformUnit tu;
+    NeighbourAvailable na;
+} HEVCLocalContextIntra;
+#endif
+
 typedef struct HEVCLocalContext {
+    TransformUnit tu;
+    NeighbourAvailable na;  // WARNING tu and na must be the first two fields to match HEVCLocalContextIntra
+
     uint8_t cabac_state[HEVC_CONTEXTS];
 
     uint8_t stat_coeff[4];
@@ -771,7 +812,6 @@
 
     int qPy_pred;
 
-    TransformUnit tu;
 
     uint8_t ctb_left_flag;
     uint8_t ctb_up_flag;
@@ -788,7 +828,6 @@
     int ct_depth;
     CodingUnit cu;
     PredictionUnit pu;
-    NeighbourAvailable na;
 
 #define BOUNDARY_LEFT_SLICE     (1 << 0)
 #define BOUNDARY_LEFT_TILE      (1 << 1)
@@ -799,6 +838,147 @@
     int boundary_flags;
 } HEVCLocalContext;
 
+
+#ifdef RPI
+
+// The processing is done in chunks
+// Each chunk corresponds to 24 64x64 luma blocks (24 so it is divisible by 8 for chroma and 12 for luma)
+// This is a distance of 1536 pixels across the screen
+// Increasing RPI_NUM_CHUNKS will reduce time spent activating QPUs and cache flushing,
+// but allocate more memory and increase the latency before data in the next frame can be processed
+#define RPI_NUM_CHUNKS 4
+#define RPI_CHUNK_SIZE 12
+
+// RPI_MAX_WIDTH is maximum width in pixels supported by the accelerated code
+#define RPI_MAX_WIDTH (RPI_NUM_CHUNKS*64*RPI_CHUNK_SIZE)
+
+// Worst case is for 4:4:4 4x4 blocks with 64 high coding tree blocks, so 16 MV cmds per 4 pixels across for each colour plane, * 2 for bi
+#define RPI_MAX_MV_CMDS_Y   (2*16*1*(RPI_MAX_WIDTH/4))
+#define RPI_MAX_MV_CMDS_C   (2*16*2*(RPI_MAX_WIDTH/4))
+// Each block can have an intra prediction and a transform_add command
+#define RPI_MAX_PRED_CMDS (2*16*3*(RPI_MAX_WIDTH/4))
+// Worst case is 16x16 CTUs
+#define RPI_MAX_DEBLOCK_CMDS (RPI_MAX_WIDTH*4/16)
+
+#define RPI_CMD_LUMA_UNI 0
+#define RPI_CMD_CHROMA_UNI 1
+#define RPI_CMD_LUMA_BI 2
+#define RPI_CMD_CHROMA_BI 3
+#define RPI_CMD_V_BI 4
+
+// RPI_PRECLEAR is not working yet - perhaps clearing on VPUs is flawed?
+// #define RPI_PRECLEAR
+
+// Command for inter prediction
+typedef struct HEVCMvCmd {
+    uint8_t cmd;
+    uint8_t block_w;
+    uint8_t block_h;
+    int8_t ref_idx[2];
+    uint16_t dststride;
+    uint16_t srcstride;
+    uint16_t srcstride1;
+    int16_t weight;
+    int16_t offset;
+    int16_t x_off;
+    int16_t y_off;
+    uint8_t *src;
+    uint8_t *src1;
+    uint8_t *dst;
+    Mv mv;
+    Mv mv1;
+} HEVCMvCmd;
+
+
+// Command for intra prediction and transform_add of predictions to coefficients
+enum rpi_pred_cmd_e
+{
+    RPI_PRED_ADD_RESIDUAL,
+    RPI_PRED_ADD_RESIDUAL_U, // = RPI_PRED_TRANSFORM_ADD + c_idx
+    RPI_PRED_ADD_RESIDUAL_V, // = RPI_PRED_TRANSFORM_ADD + c_idx
+    RPI_PRED_INTRA,
+    RPI_PRED_I_PCM,
+    RPI_PRED_CMD_MAX
+};
+
+typedef struct HEVCPredCmd {
+    uint8_t type;
+    uint8_t size;  // log2 "size" used by all variants
+    uint8_t na;    // i_pred - but left here as they pack well
+    uint8_t c_idx; // i_pred
+    union {
+        struct {  // TRANSFORM_ADD
+            uint8_t * dst;
+            const int16_t * buf;
+            uint32_t stride;
+        } ta;
+        struct {  // INTRA
+            uint16_t x;
+            uint16_t y;
+            enum IntraPredMode mode;
+        } i_pred;
+        struct {  // I_PCM
+            uint16_t x;
+            uint16_t y;
+            const void * src;
+            uint32_t src_len;
+        } i_pcm;
+    };
+} HEVCPredCmd;
+
+#endif
+
+#ifdef RPI
+
+struct qpu_mc_pred_c_s;
+struct qpu_mc_pred_y_s;
+
+typedef struct HEVCRpiLumaPred
+{
+    struct qpu_mc_pred_y_s *qpu_mc_base;
+    struct qpu_mc_pred_y_s *qpu_mc_curr;
+    struct qpu_mc_pred_y_s *last_lx;
+    unsigned int load;
+} HEVCRpiLumaPred;
+
+typedef struct HEVCRpiChromaPred
+{
+    struct qpu_mc_pred_c_s *qpu_mc_base;
+    struct qpu_mc_pred_c_s *qpu_mc_curr;
+    struct qpu_mc_pred_c_s *last_l0;
+    struct qpu_mc_pred_c_s *last_l1;
+    unsigned int load;
+} HEVCRpiChromaPred;
+
+typedef struct HEVCRpiJob {
+    GPU_MEM_PTR_T chroma_mvs_gptr;
+    GPU_MEM_PTR_T luma_mvs_gptr;
+    HEVCRpiChromaPred chroma_mvs[QPU_N_UV];
+    HEVCRpiLumaPred luma_mvs[QPU_N_Y];
+} HEVCRpiJob;
+
+#if RPI_TSTATS
+typedef struct HEVCRpiStats {
+    int y_pred1_y8_merge;
+    int y_pred1_xy;
+    int y_pred1_x0;
+    int y_pred1_y0;
+    int y_pred1_x0y0;
+    int y_pred1_wle8;
+    int y_pred1_wgt8;
+    int y_pred1_hle16;
+    int y_pred1_hgt16;
+    int y_pred2_xy;
+    int y_pred2_x0;
+    int y_pred2_y0;
+    int y_pred2_x0y0;
+    int y_pred2_hle16;
+    int y_pred2_hgt16;
+} HEVCRpiStats;
+#endif
+
+#endif
+
 typedef struct HEVCContext {
     const AVClass *c;  // needed by private avoptions
     AVCodecContext *avctx;
@@ -807,13 +987,103 @@
 
     HEVCLocalContext    *HEVClcList[MAX_NB_THREADS];
     HEVCLocalContext    *HEVClc;
-
+#ifdef RPI_WORKER
+    HEVCLocalContextIntra HEVClcIntra;
+#endif
     uint8_t             threads_type;
     uint8_t             threads_number;
 
     int                 width;
     int                 height;
 
+    int used_for_ref;
+
+#ifdef RPI
+    int enable_rpi;
+    HEVCMvCmd *unif_mv_cmds_y[RPI_MAX_JOBS];
+    HEVCMvCmd *unif_mv_cmds_c[RPI_MAX_JOBS];
+    HEVCPredCmd *univ_pred_cmds[RPI_MAX_JOBS];
+    int buf_width;
+    GPU_MEM_PTR_T coeffs_buf_default[RPI_MAX_JOBS];
+    GPU_MEM_PTR_T coeffs_buf_accelerated[RPI_MAX_JOBS];
+    int16_t *coeffs_buf_arm[RPI_MAX_JOBS][4];
+    unsigned int coeffs_buf_vc[RPI_MAX_JOBS][4];
+    int num_coeffs[RPI_MAX_JOBS][4];
+    int num_xfm_cmds[RPI_MAX_JOBS];
+    int num_mv_cmds_y[RPI_MAX_JOBS];
+    int num_mv_cmds_c[RPI_MAX_JOBS];
+    int num_pred_cmds[RPI_MAX_JOBS];
+    int num_dblk_cmds[RPI_MAX_JOBS];
+    int vpu_id;
+    int pass0_job; // Pass0 does coefficient decode
+    int pass1_job; // Pass1 does pixel processing
+    int ctu_count; // Number of CTUs done in pass0 so far
+    int max_ctu_count; // Number of CTUs when we trigger a round of processing
+    int ctu_per_y_chan; // Number of CTUs per luma QPU
+    int ctu_per_uv_chan; // Number of CTUs per chroma QPU
+
+    HEVCRpiJob jobs[RPI_MAX_JOBS];
+#if RPI_TSTATS
+    HEVCRpiStats tstats;
+#endif
+#if RPI_INTER
+    HEVCRpiChromaPred * curr_pred_c;
+    HEVCRpiLumaPred * curr_pred_y;
+    struct qpu_mc_pred_y_s * last_y8_p;
+    struct qpu_mc_pred_y_s * last_y8_lx;
+
+    // Function pointers
+    uint32_t qpu_filter_uv;
+    uint32_t qpu_filter_uv_b0;
+    uint32_t qpu_dummy_frame; // Not a frame - just a bit of memory
+    uint32_t qpu_filter;
+    uint32_t qpu_filter_b;
+#endif
+
+#ifdef RPI_WORKER
+    pthread_t worker_thread;
+    pthread_cond_t worker_cond_head;
+    pthread_cond_t worker_cond_tail;
+    pthread_mutex_t worker_mutex;
+
+    int worker_tail; // Contains the number of posted jobs
+    int worker_head; // Contains the number of completed jobs
+    int kill_worker; // set to 1 to terminate the worker
+#endif
+
+#define RPI_DEBLOCK_VPU_Q_COUNT 2
+
+#ifdef RPI_DEBLOCK_VPU
+    int enable_rpi_deblock;
+
+    int uv_setup_width;
+    int uv_setup_height;
+    int setup_width; // Number of 16x16 blocks across the image
+    int setup_height; // Number of 16x16 blocks down the image
+
+    struct dblk_vpu_q_s
+    {
+        GPU_MEM_PTR_T deblock_vpu_gmem;
+
+        uint8_t (*y_setup_arm)[2][2][2][4];
+        uint8_t (*y_setup_vc)[2][2][2][4];
+
+        uint8_t (*uv_setup_arm)[2][2][2][4];  // Half of this is unused [][][1][], but easier for the VPU as it allows us to store with zeros and addresses are aligned
+        uint8_t (*uv_setup_vc)[2][2][2][4];
+
+        int (*vpu_cmds_arm)[6]; // r0-r5 for each command
+        int vpu_cmds_vc;
+
+        vpu_qpu_wait_h cmd_id;
+    } dvq_ents[RPI_DEBLOCK_VPU_Q_COUNT];
+
+    struct dblk_vpu_q_s * dvq;
+    unsigned int dvq_n;
+
+#endif
+
+#endif
+
     uint8_t *cabac_state;
 
     /** 1 if the independent slice segment header was successfully parsed */
@@ -931,6 +1201,9 @@
     uint32_t max_mastering_luminance;
     uint32_t min_mastering_luminance;
 
+#ifdef RPI
+    int dblk_cmds[RPI_MAX_JOBS][RPI_MAX_DEBLOCK_CMDS][2];
+#endif
 } HEVCContext;
 
 int ff_hevc_decode_short_term_rps(GetBitContext *gb, AVCodecContext *avctx,
@@ -1057,6 +1330,10 @@
                                  int log2_trafo_size, enum ScanType scan_idx,
                                  int c_idx);
 
+#if RPI_INTER
+extern void rpi_flush_ref_frame_progress(HEVCContext * const s, ThreadFrame * const f, const unsigned int n);
+#endif
+
 void ff_hevc_hls_mvd_coding(HEVCContext *s, int x0, int y0, int log2_cb_size);
 
 
@@ -1081,4 +1358,15 @@
 extern const uint8_t ff_hevc_diag_scan8x8_x[64];
 extern const uint8_t ff_hevc_diag_scan8x8_y[64];
 
+#ifdef RPI
+int16_t * rpi_alloc_coeff_buf(HEVCContext * const s, const int buf_no, const int n);
+
+// arm/hevc_misc_neon.S
+// Neon coeff zap fn
+#if HAVE_NEON
+extern void rpi_zap_coeff_vals_neon(int16_t * dst, unsigned int l2ts_m2);
+#endif
+
+#endif
+
 #endif /* AVCODEC_HEVC_H */
diff -Naur ffmpeg-3.2.4/libavcodec/hevcpred.c ffmpeg-3.2.4.patch/libavcodec/hevcpred.c
--- ffmpeg-3.2.4/libavcodec/hevcpred.c	2016-06-27 01:54:29.000000000 +0200
+++ ffmpeg-3.2.4.patch/libavcodec/hevcpred.c	2017-05-28 20:42:45.744088687 +0200
@@ -24,6 +24,7 @@
 
 #include "hevcpred.h"
 
+#define PRED_C 0
 #define BIT_DEPTH 8
 #include "hevcpred_template.c"
 #undef BIT_DEPTH
@@ -39,13 +40,37 @@
 #define BIT_DEPTH 12
 #include "hevcpred_template.c"
 #undef BIT_DEPTH
+#undef PRED_C
+
+#ifdef RPI
+#define PRED_C 1
+#define BIT_DEPTH 8
+#include "hevcpred_template.c"
+#undef BIT_DEPTH
+
+#define BIT_DEPTH 9
+#include "hevcpred_template.c"
+#undef BIT_DEPTH
+
+#define BIT_DEPTH 10
+#include "hevcpred_template.c"
+#undef BIT_DEPTH
+
+#define BIT_DEPTH 12
+#include "hevcpred_template.c"
+#undef BIT_DEPTH
+#undef PRED_C
+#endif
 
 void ff_hevc_pred_init(HEVCPredContext *hpc, int bit_depth)
 {
 #undef FUNC
 #define FUNC(a, depth) a ## _ ## depth
 
-#define HEVC_PRED(depth)                                \
+#undef FUNCC
+#define FUNCC(a, depth) a ## _ ## depth ## _c
+
+#define HEVC_PRED_Y(depth)                                \
     hpc->intra_pred[0]   = FUNC(intra_pred_2, depth);   \
     hpc->intra_pred[1]   = FUNC(intra_pred_3, depth);   \
     hpc->intra_pred[2]   = FUNC(intra_pred_4, depth);   \
@@ -60,6 +85,30 @@
     hpc->pred_angular[2] = FUNC(pred_angular_2, depth); \
     hpc->pred_angular[3] = FUNC(pred_angular_3, depth);
 
+#define HEVC_PRED_C(depth)                                \
+    hpc->intra_pred_c[0]   = FUNCC(intra_pred_2, depth);   \
+    hpc->intra_pred_c[1]   = FUNCC(intra_pred_3, depth);   \
+    hpc->intra_pred_c[2]   = FUNCC(intra_pred_4, depth);   \
+    hpc->intra_pred_c[3]   = FUNCC(intra_pred_5, depth);   \
+    hpc->pred_planar_c[0]  = FUNCC(pred_planar_0, depth);  \
+    hpc->pred_planar_c[1]  = FUNCC(pred_planar_1, depth);  \
+    hpc->pred_planar_c[2]  = FUNCC(pred_planar_2, depth);  \
+    hpc->pred_planar_c[3]  = FUNCC(pred_planar_3, depth);  \
+    hpc->pred_dc_c         = FUNCC(pred_dc, depth);        \
+    hpc->pred_angular_c[0] = FUNCC(pred_angular_0, depth); \
+    hpc->pred_angular_c[1] = FUNCC(pred_angular_1, depth); \
+    hpc->pred_angular_c[2] = FUNCC(pred_angular_2, depth); \
+    hpc->pred_angular_c[3] = FUNCC(pred_angular_3, depth);
+
+#ifdef RPI
+#define HEVC_PRED(depth) \
+    HEVC_PRED_Y(depth); \
+    HEVC_PRED_C(depth);
+#else
+#define HEVC_PRED(depth) \
+    HEVC_PRED_Y(depth);
+#endif
+
     switch (bit_depth) {
     case 9:
         HEVC_PRED(9);
diff -Naur ffmpeg-3.2.4/libavcodec/hevcpred.h ffmpeg-3.2.4.patch/libavcodec/hevcpred.h
--- ffmpeg-3.2.4/libavcodec/hevcpred.h	2016-06-27 01:54:29.000000000 +0200
+++ ffmpeg-3.2.4.patch/libavcodec/hevcpred.h	2017-05-28 20:42:45.745088691 +0200
@@ -38,6 +38,17 @@
     void (*pred_angular[4])(uint8_t *src, const uint8_t *top,
                             const uint8_t *left, ptrdiff_t stride,
                             int c_idx, int mode);
+#ifdef RPI
+    void (*intra_pred_c[4])(struct HEVCContext *s, int x0, int y0, int c_idx);
+
+    void (*pred_planar_c[4])(uint8_t *src, const uint8_t *top,
+                           const uint8_t *left, ptrdiff_t stride);
+    void (*pred_dc_c)(uint8_t *src, const uint8_t *top, const uint8_t *left,
+                    ptrdiff_t stride, int log2_size, int c_idx);
+    void (*pred_angular_c[4])(uint8_t *src, const uint8_t *top,
+                            const uint8_t *left, ptrdiff_t stride,
+                            int c_idx, int mode);
+#endif
 } HEVCPredContext;
 
 void ff_hevc_pred_init(HEVCPredContext *hpc, int bit_depth);
diff -Naur ffmpeg-3.2.4/libavcodec/hevcpred_template.c ffmpeg-3.2.4.patch/libavcodec/hevcpred_template.c
--- ffmpeg-3.2.4/libavcodec/hevcpred_template.c	2016-06-27 01:54:29.000000000 +0200
+++ ffmpeg-3.2.4.patch/libavcodec/hevcpred_template.c	2017-05-28 20:42:45.746088694 +0200
@@ -20,13 +20,55 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
+//#define DISABLE_INTRA
+
 #include "libavutil/pixdesc.h"
 
 #include "bit_depth_template.c"
 #include "hevcpred.h"
 
+#ifdef RPI
+#include "rpi_zc.h"
+#endif
+
+#define DUMP_PRED 0
+
 #define POS(x, y) src[(x) + stride * (y)]
 
+#if PRED_C
+
+typedef uint8_t (* c8_dst_ptr_t)[2];
+typedef const uint8_t (* c8_src_ptr_t)[2];
+
+#if BIT_DEPTH == 8
+#undef BIT_DEPTH
+#define BIT_DEPTH 16
+#include "bit_depth_template.c"
+#undef FUNC
+#define FUNC(a) FUNC3(a, 8, _c)
+#else
+#undef FUNC
+#define FUNC FUNCC
+#endif
+
+#endif
+
+#if DUMP_PRED
+#ifndef DEBUG_ONCE
+#define DEBUG_ONCE
+static void dump_pred_uv(const uint8_t * data, const unsigned int stride, const unsigned int size)
+{
+    for (unsigned int y = 0; y != size; y++, data += stride * 2) {
+        for (unsigned int x = 0; x != size; x++) {
+            printf("%4d", data[x * 2]);
+        }
+        printf("\n");
+    }
+    printf("\n");
+}
+#endif
+#endif
+
 static av_always_inline void FUNC(intra_pred)(HEVCContext *s, int x0, int y0,
                                               int log2_size, int c_idx)
 {
@@ -69,8 +111,11 @@
                 AV_WN4P(&ptr[i], a);                                           \
             else                                                               \
                 a = PIXEL_SPLAT_X4(ptr[i + 3])
-
+#ifdef RPI_WORKER
+    HEVCLocalContextIntra *lc = (s->enable_rpi) ? &s->HEVClcIntra : (HEVCLocalContextIntra *)s->HEVClc ;
+#else
     HEVCLocalContext *lc = s->HEVClc;
+#endif
     int i;
     int hshift = s->ps.sps->hshift[c_idx];
     int vshift = s->ps.sps->vshift[c_idx];
@@ -79,15 +124,23 @@
     int size_in_tbs_h  = size_in_luma_h >> s->ps.sps->log2_min_tb_size;
     int size_in_luma_v = size << vshift;
     int size_in_tbs_v  = size_in_luma_v >> s->ps.sps->log2_min_tb_size;
-    int x = x0 >> hshift;
-    int y = y0 >> vshift;
+    const int x = x0 >> hshift;
+    const int y = y0 >> vshift;
     int x_tb = (x0 >> s->ps.sps->log2_min_tb_size) & s->ps.sps->tb_mask;
     int y_tb = (y0 >> s->ps.sps->log2_min_tb_size) & s->ps.sps->tb_mask;
 
     int cur_tb_addr = MIN_TB_ADDR_ZS(x_tb, y_tb);
 
-    ptrdiff_t stride = s->frame->linesize[c_idx] / sizeof(pixel);
+    const ptrdiff_t stride = s->frame->linesize[c_idx] / sizeof(pixel);
+#if defined(RPI)
+    pixel *const src = s->frame->format != AV_PIX_FMT_SAND128 ?
+            (pixel*)s->frame->data[c_idx] + x + y * stride :
+        c_idx == 0 ?
+            (pixel *)rpi_sliced_frame_pos_y(s->frame, x, y) :
+            (pixel *)rpi_sliced_frame_pos_c(s->frame, x, y);
+#else
     pixel *src = (pixel*)s->frame->data[c_idx] + x + y * stride;
+#endif
 
     int min_pu_width = s->ps.sps->min_pu_width;
 
@@ -95,14 +148,20 @@
                               lc->tu.intra_pred_mode;
     pixel4 a;
     pixel  left_array[2 * MAX_TB_SIZE + 1];
+#if !PRED_C
     pixel  filtered_left_array[2 * MAX_TB_SIZE + 1];
+#endif
     pixel  top_array[2 * MAX_TB_SIZE + 1];
+#if !PRED_C
     pixel  filtered_top_array[2 * MAX_TB_SIZE + 1];
+#endif
 
     pixel  *left          = left_array + 1;
     pixel  *top           = top_array  + 1;
+#if !PRED_C
     pixel  *filtered_left = filtered_left_array + 1;
     pixel  *filtered_top  = filtered_top_array  + 1;
+#endif
     int cand_bottom_left = lc->na.cand_bottom_left && cur_tb_addr > MIN_TB_ADDR_ZS( x_tb - 1, (y_tb + size_in_tbs_v) & s->ps.sps->tb_mask);
     int cand_left        = lc->na.cand_left;
     int cand_up_left     = lc->na.cand_up_left;
@@ -114,6 +173,26 @@
     int top_right_size   = (FFMIN(x0 + 2 * size_in_luma_h, s->ps.sps->width) -
                            (x0 + size_in_luma_h)) >> hshift;
 
+    pixel * src_l = src - 1;
+    pixel * src_u = src - stride;
+    pixel * src_ur = src_u + size;
+
+#ifdef DISABLE_INTRA
+    return;
+#endif
+
+#if defined(RPI)
+    if (s->frame->format == AV_PIX_FMT_SAND128) {
+        const AVFrame * const frame = s->frame;
+        const unsigned int mask = stride - 1; // For chroma pixel=uint16 so stride_c is stride_y / 2
+        const unsigned int stripe_adj = (frame->linesize[3] - 1) * stride;
+        if ((x & mask) == 0)
+            src_l -= stripe_adj;
+        if (((x + size) & mask) == 0)
+            src_ur += stripe_adj;
+    }
+#endif
+
     if (s->ps.pps->constrained_intra_pred_flag == 1) {
         int size_in_luma_pu_v = PU(size_in_luma_v);
         int size_in_luma_pu_h = PU(size_in_luma_h);
@@ -163,23 +242,24 @@
         top[-1] = 128;
     }
     if (cand_up_left) {
-        left[-1] = POS(-1, -1);
+        left[-1] = src_l[-stride];
         top[-1]  = left[-1];
     }
     if (cand_up)
-        memcpy(top, src - stride, size * sizeof(pixel));
+        // Always good - even with sand
+        memcpy(top, src_u, size * sizeof(pixel));
     if (cand_up_right) {
-        memcpy(top + size, src - stride + size, size * sizeof(pixel));
-        EXTEND(top + size + top_right_size, POS(size + top_right_size - 1, -1),
+        memcpy(top + size, src_ur, top_right_size * sizeof(pixel));
+        EXTEND(top + size + top_right_size, top[size + top_right_size - 1],
                size - top_right_size);
     }
     if (cand_left)
         for (i = 0; i < size; i++)
-            left[i] = POS(-1, i);
+            left[i] = src_l[stride * i];
     if (cand_bottom_left) {
         for (i = size; i < size + bottom_left_size; i++)
-            left[i] = POS(-1, i);
-        EXTEND(left + size + bottom_left_size, POS(-1, size + bottom_left_size - 1),
+            left[i] = src_l[stride * i];
+        EXTEND(left + size + bottom_left_size, left[size + bottom_left_size - 1],
                size - bottom_left_size);
     }
 
@@ -268,7 +348,11 @@
             cand_up_left = 1;
             cand_left    = 1;
         } else { // No samples available
+#if PRED_C && BIT_DEPTH == 16
+            left[-1] = 0x8080;
+#else
             left[-1] = (1 << (BIT_DEPTH - 1));
+#endif
             EXTEND(top,  left[-1], 2 * size);
             EXTEND(left, left[-1], 2 * size);
         }
@@ -287,6 +371,9 @@
     top[-1] = left[-1];
 
     // Filtering process
+    // Sand128 can only apply to chroma_format_idc == 1 so we don't need to
+    // worry about chroma smoothing for that case
+#if !PRED_C
     if (!s->ps.sps->intra_smoothing_disabled_flag && (c_idx == 0  || s->ps.sps->chroma_format_idc == 3)) {
         if (mode != INTRA_DC && size != 4){
             int intra_hor_ver_dist_thresh[] = { 7, 1, 0 };
@@ -342,13 +429,46 @@
                                            mode);
         break;
     }
+#else
+    switch (mode) {
+    case INTRA_PLANAR:
+        s->hpc.pred_planar_c[log2_size - 2]((uint8_t *)src, (uint8_t *)top,
+                                          (uint8_t *)left, stride);
+        break;
+    case INTRA_DC:
+        s->hpc.pred_dc_c((uint8_t *)src, (uint8_t *)top,
+                       (uint8_t *)left, stride, log2_size, c_idx);
+        break;
+    default:
+        s->hpc.pred_angular_c[log2_size - 2]((uint8_t *)src, (uint8_t *)top,
+                                           (uint8_t *)left, stride, c_idx,
+                                           mode);
+        break;
+    }
+
+#if DUMP_PRED
+    printf("U pred @ %d, %d: mode=%d\n", x, y, mode);
+    dump_pred_uv((uint8_t *)src, stride, 1 << log2_size);
+    printf("V pred @ %d, %d: mode=%d\n", x, y, mode);
+    dump_pred_uv((uint8_t *)src + 1, stride, 1 << log2_size);
+#endif
+#endif
 }
 
+#if !PRED_C || BIT_DEPTH == 16
 #define INTRA_PRED(size)                                                            \
 static void FUNC(intra_pred_ ## size)(HEVCContext *s, int x0, int y0, int c_idx)    \
 {                                                                                   \
     FUNC(intra_pred)(s, x0, y0, size, c_idx);                                       \
 }
+#else
+#define INTRA_PRED(size)                                                            \
+static void FUNC(intra_pred_ ## size)(HEVCContext *s, int x0, int y0, int c_idx)    \
+{                                                                                   \
+    av_log(NULL, AV_LOG_PANIC, "%s: NIF\n", __func__);                              \
+    abort();                                                                        \
+}
+#endif
 
 INTRA_PRED(2)
 INTRA_PRED(3)
@@ -357,6 +477,7 @@
 
 #undef INTRA_PRED
 
+#if !PRED_C
 static av_always_inline void FUNC(pred_planar)(uint8_t *_src, const uint8_t *_top,
                                   const uint8_t *_left, ptrdiff_t stride,
                                   int trafo_size)
@@ -371,13 +492,46 @@
             POS(x, y) = ((size - 1 - x) * left[y] + (x + 1) * top[size]  +
                          (size - 1 - y) * top[x]  + (y + 1) * left[size] + size) >> (trafo_size + 1);
 }
+#else
+static av_always_inline void FUNC(pred_planar)(uint8_t * _src, const uint8_t * _top,
+                                  const uint8_t * _left, ptrdiff_t stride,
+                                  int trafo_size)
+{
+    int x, y;
+    int size = 1 << trafo_size;
+    c8_dst_ptr_t src = (c8_dst_ptr_t)_src;
+    const c8_src_ptr_t top = (c8_src_ptr_t)_top;
+    const c8_src_ptr_t left = (c8_src_ptr_t)_left;
+
+    for (y = 0; y < size; y++, src += stride)
+    {
+        for (x = 0; x < size; x++)
+        {
+            src[x][0] = ((size - 1 - x) * left[y][0] + (x + 1) * top[size][0]  +
+                         (size - 1 - y) * top[x][0]  + (y + 1) * left[size][0] + size) >> (trafo_size + 1);
+            src[x][1] = ((size - 1 - x) * left[y][1] + (x + 1) * top[size][1]  +
+                         (size - 1 - y) * top[x][1]  + (y + 1) * left[size][1] + size) >> (trafo_size + 1);
+        }
+    }
+}
+#endif
 
+#if !PRED_C || BIT_DEPTH == 16
 #define PRED_PLANAR(size)\
 static void FUNC(pred_planar_ ## size)(uint8_t *src, const uint8_t *top,        \
                                        const uint8_t *left, ptrdiff_t stride)   \
 {                                                                               \
     FUNC(pred_planar)(src, top, left, stride, size + 2);                        \
 }
+#else
+#define PRED_PLANAR(size)\
+static void FUNC(pred_planar_ ## size)(uint8_t *src, const uint8_t *top,        \
+                                       const uint8_t *left, ptrdiff_t stride)   \
+{                                                                               \
+    av_log(NULL, AV_LOG_PANIC, "%s: NIF", __func__);                            \
+    abort();                                                                    \
+}
+#endif
 
 PRED_PLANAR(0)
 PRED_PLANAR(1)
@@ -386,6 +540,7 @@
 
 #undef PRED_PLANAR
 
+#if !PRED_C
 static void FUNC(pred_dc)(uint8_t *_src, const uint8_t *_top,
                           const uint8_t *_left,
                           ptrdiff_t stride, int log2_size, int c_idx)
@@ -416,7 +571,53 @@
             POS(0, y) = (left[y] + 3 * dc + 2) >> 2;
     }
 }
+#else
+static void FUNC(pred_dc)(uint8_t *_src, const uint8_t *_top,
+                          const uint8_t *_left,
+                          ptrdiff_t stride, int log2_size, int c_idx)
+{
+    unsigned int i, j;
+    const unsigned int size = (1 << log2_size);
+    c8_dst_ptr_t src = (c8_dst_ptr_t)_src;
+    const c8_src_ptr_t top = (c8_src_ptr_t)_top;
+    const c8_src_ptr_t left = (c8_src_ptr_t)_left;
+    unsigned int dc0 = size;
+    unsigned int dc1 = size;
+
+    for (i = 0; i < size; i++)
+    {
+        dc0 += left[i][0] + top[i][0];
+        dc1 += left[i][1] + top[i][1];
+    }
+
+    dc0 >>= log2_size + 1;
+    dc1 >>= log2_size + 1;
+
+    for (i = 0; i < size; i++, src += stride)
+    {
+        for (j = 0; j < size; ++j)
+        {
+            src[j][0] = dc0;
+            src[j][1] = dc1;
+
+        }
+    }
+}
+#endif
 
+#ifndef ANGLE_CONSTS
+#define ANGLE_CONSTS
+static const int intra_pred_angle[] = {
+     32,  26,  21,  17, 13,  9,  5, 2, 0, -2, -5, -9, -13, -17, -21, -26, -32,
+    -26, -21, -17, -13, -9, -5, -2, 0, 2,  5,  9, 13,  17,  21,  26,  32
+};
+static const int inv_angle[] = {
+    -4096, -1638, -910, -630, -482, -390, -315, -256, -315, -390, -482,
+    -630, -910, -1638, -4096
+};
+#endif
+
+#if !PRED_C
 static av_always_inline void FUNC(pred_angular)(uint8_t *_src,
                                                 const uint8_t *_top,
                                                 const uint8_t *_left,
@@ -428,15 +629,6 @@
     const pixel *top  = (const pixel *)_top;
     const pixel *left = (const pixel *)_left;
 
-    static const int intra_pred_angle[] = {
-         32,  26,  21,  17, 13,  9,  5, 2, 0, -2, -5, -9, -13, -17, -21, -26, -32,
-        -26, -21, -17, -13, -9, -5, -2, 0, 2,  5,  9, 13,  17,  21,  26,  32
-    };
-    static const int inv_angle[] = {
-        -4096, -1638, -910, -630, -482, -390, -315, -256, -315, -390, -482,
-        -630, -910, -1638, -4096
-    };
-
     int angle = intra_pred_angle[mode - 2];
     pixel ref_array[3 * MAX_TB_SIZE + 4];
     pixel *ref_tmp = ref_array + size;
@@ -509,6 +701,83 @@
         }
     }
 }
+#else
+static av_always_inline void FUNC(pred_angular)(uint8_t *_src,
+                                                const uint8_t *_top,
+                                                const uint8_t *_left,
+                                                ptrdiff_t stride, int c_idx,
+                                                int mode, int size)
+{
+    int x, y;
+    c8_dst_ptr_t src  = (c8_dst_ptr_t)_src;
+    c8_src_ptr_t top  = (c8_src_ptr_t)_top;
+    c8_src_ptr_t left = (c8_src_ptr_t)_left;
+
+    const int angle = intra_pred_angle[mode - 2];
+    uint8_t ref_array[3 * MAX_TB_SIZE + 4][2];
+    c8_dst_ptr_t ref_tmp = ref_array + size;
+    c8_src_ptr_t ref;
+    const int last = (size * angle) >> 5;
+
+    if (mode >= 18) {
+        ref = top - 1;
+        if (angle < 0 && last < -1) {
+            memcpy(ref_tmp, top - 1, (size + 1) * 2);
+            for (x = last; x <= -1; x++)
+            {
+                ref_tmp[x][0] = left[-1 + ((x * inv_angle[mode - 11] + 128) >> 8)][0];
+                ref_tmp[x][1] = left[-1 + ((x * inv_angle[mode - 11] + 128) >> 8)][1];
+            }
+            ref = (c8_src_ptr_t)ref_tmp;
+        }
+
+        for (y = 0; y < size; y++, src += stride) {
+            const int idx  = ((y + 1) * angle) >> 5;
+            const int fact = ((y + 1) * angle) & 31;
+            if (fact) {
+                for (x = 0; x < size; ++x) {
+                    src[x][0] = ((32 - fact) * ref[x + idx + 1][0] +
+                                       fact  * ref[x + idx + 2][0] + 16) >> 5;
+                    src[x][1] = ((32 - fact) * ref[x + idx + 1][1] +
+                                       fact  * ref[x + idx + 2][1] + 16) >> 5;
+                }
+            } else {
+                memcpy(src, ref + idx + 1, size * 2);
+            }
+        }
+    } else {
+        ref = left - 1;
+        if (angle < 0 && last < -1) {
+            memcpy(ref_tmp, left - 1, (size + 1) * 2);
+            for (x = last; x <= -1; x++)
+            {
+                ref_tmp[x][0] = top[-1 + ((x * inv_angle[mode - 11] + 128) >> 8)][0];
+                ref_tmp[x][1] = top[-1 + ((x * inv_angle[mode - 11] + 128) >> 8)][1];
+            }
+            ref = (c8_src_ptr_t)ref_tmp;
+        }
+
+        for (x = 0; x < size; x++, src++) {
+            const int idx  = ((x + 1) * angle) >> 5;
+            const int fact = ((x + 1) * angle) & 31;
+            if (fact) {
+                for (y = 0; y < size; y++) {
+                    src[y * stride][0] = ((32 - fact) * ref[y + idx + 1][0] +
+                                       fact  * ref[y + idx + 2][0] + 16) >> 5;
+                    src[y * stride][1] = ((32 - fact) * ref[y + idx + 1][1] +
+                                       fact  * ref[y + idx + 2][1] + 16) >> 5;
+                }
+            } else {
+                for (y = 0; y < size; y++)
+                {
+                    src[y * stride][0] = ref[y + idx + 1][0];
+                    src[y * stride][1] = ref[y + idx + 1][1];
+                }
+            }
+        }
+    }
+}
+#endif
 
 static void FUNC(pred_angular_0)(uint8_t *src, const uint8_t *top,
                                  const uint8_t *left,
diff -Naur ffmpeg-3.2.4/libavcodec/hevc_ps.c ffmpeg-3.2.4.patch/libavcodec/hevc_ps.c
--- ffmpeg-3.2.4/libavcodec/hevc_ps.c	2017-02-10 14:25:26.000000000 +0100
+++ ffmpeg-3.2.4.patch/libavcodec/hevc_ps.c	2017-05-28 20:42:45.742088680 +0200
@@ -779,7 +779,12 @@
     switch (sps->bit_depth) {
     case 8:
         if (sps->chroma_format_idc == 0) sps->pix_fmt = AV_PIX_FMT_GRAY8;
+#if RPI_HEVC_SAND
+        // *** Horrid kludge s.t. we start out with sand format
+        if (sps->chroma_format_idc == 1) sps->pix_fmt = sps->width <= 2048 && sps->height <= 1088 ? AV_PIX_FMT_SAND128 : AV_PIX_FMT_YUV420P;
+#else
         if (sps->chroma_format_idc == 1) sps->pix_fmt = AV_PIX_FMT_YUV420P;
+#endif
         if (sps->chroma_format_idc == 2) sps->pix_fmt = AV_PIX_FMT_YUV422P;
         if (sps->chroma_format_idc == 3) sps->pix_fmt = AV_PIX_FMT_YUV444P;
        break;
@@ -1001,6 +1006,8 @@
     sps->amp_enabled_flag = get_bits1(gb);
     sps->sao_enabled      = get_bits1(gb);
 
+    av_log(avctx, AV_LOG_INFO, "sao_enabled=%d\n", sps->sao_enabled);
+
     sps->pcm_enabled_flag = get_bits1(gb);
     if (sps->pcm_enabled_flag) {
         sps->pcm.bit_depth   = get_bits(gb, 4) + 1;
diff -Naur ffmpeg-3.2.4/libavcodec/Makefile ffmpeg-3.2.4.patch/libavcodec/Makefile
--- ffmpeg-3.2.4/libavcodec/Makefile	2017-02-10 14:25:26.000000000 +0100
+++ ffmpeg-3.2.4.patch/libavcodec/Makefile	2017-05-28 20:42:45.713088577 +0200
@@ -5,6 +5,12 @@
 HEADERS = avcodec.h                                                     \
           avdct.h                                                       \
           avfft.h                                                       \
+          rpi_qpu.h                                                     \
+          rpi_shader.h                                                  \
+	  rpi_shader_cmd.h                                              \
+          rpi_mailbox.h                                                 \
+          rpi_hevc_transform.h                                          \
+          rpi_zc.h                                                      \
           d3d11va.h                                                     \
           dirac.h                                                       \
           dv_profile.h                                                  \
@@ -45,6 +51,10 @@
        resample.o                                                       \
        resample2.o                                                      \
        utils.o                                                          \
+       rpi_qpu.o                                                        \
+       rpi_shader.o                                                     \
+       rpi_mailbox.o                                                    \
+       rpi_zc.o                                                         \
        vorbis_parser.o                                                  \
        xiph.o                                                           \
 
@@ -1093,3 +1103,15 @@
 $(SUBDIR)sinewin.o: $(SUBDIR)sinewin_tables.h
 $(SUBDIR)sinewin_fixed.o: $(SUBDIR)sinewin_fixed_tables.h
 endif
+
+QASM := $(SUBDIR)../pi-util/qasm.py
+
+ifneq ("$(wildcard $(QASM))","")
+$(SUBDIR)rpi_shader.c: $(SUBDIR)rpi_shader.qasm
+	python $(QASM) -mc_c:rpi_shader,rpi_shader,rpi_shader $< > $@
+
+$(SUBDIR)rpi_shader.h: $(SUBDIR)rpi_shader.qasm
+	python $(QASM) -mc_h:rpi_shader,rpi_shader,rpi_shader $< > $@
+endif
+
+$(SUBDIR)rpi_qpu.o $(SUBDIR)hevc.o: $(SUBDIR)rpi_shader.h
diff -Naur ffmpeg-3.2.4/libavcodec/mmaldec.c ffmpeg-3.2.4.patch/libavcodec/mmaldec.c
--- ffmpeg-3.2.4/libavcodec/mmaldec.c	2017-02-10 14:25:27.000000000 +0100
+++ ffmpeg-3.2.4.patch/libavcodec/mmaldec.c	2017-05-28 20:42:45.746088694 +0200
@@ -24,6 +24,9 @@
  * MMAL Video Decoder
  */
 
+#pragma GCC diagnostic push
+// Many many redundant decls in the header files
+#pragma GCC diagnostic ignored "-Wredundant-decls"
 #include <bcm_host.h>
 #include <interface/mmal/mmal.h>
 #include <interface/mmal/mmal_parameters_video.h>
@@ -31,6 +34,7 @@
 #include <interface/mmal/util/mmal_util_params.h>
 #include <interface/mmal/util/mmal_default_components.h>
 #include <interface/mmal/vc/mmal_vc_api.h>
+#pragma GCC diagnostic pop
 
 #include "avcodec.h"
 #include "internal.h"
diff -Naur ffmpeg-3.2.4/libavcodec/mpeg4videodec.c ffmpeg-3.2.4.patch/libavcodec/mpeg4videodec.c
--- ffmpeg-3.2.4/libavcodec/mpeg4videodec.c	2017-02-10 14:25:27.000000000 +0100
+++ ffmpeg-3.2.4.patch/libavcodec/mpeg4videodec.c	2017-05-28 20:42:45.747088698 +0200
@@ -2212,6 +2212,9 @@
 
         if (ctx->divx_version >= 0)
             s->workaround_bugs |= FF_BUG_HPEL_CHROMA;
+
+        if (ctx->num_sprite_warping_points > 1)
+            s->workaround_bugs |= FF_BUG_GMC_UNSUPPORTED;
     }
 
     if (s->workaround_bugs & FF_BUG_STD_QPEL) {
@@ -2236,6 +2239,7 @@
                s->workaround_bugs, ctx->lavc_build, ctx->xvid_build,
                ctx->divx_version, ctx->divx_build, s->divx_packed ? "p" : "");
 
+    avctx->workaround_bugs = s->workaround_bugs;
     if (CONFIG_MPEG4_DECODER && ctx->xvid_build >= 0 &&
         s->codec_id == AV_CODEC_ID_MPEG4 &&
         avctx->idct_algo == FF_IDCT_AUTO) {
diff -Naur ffmpeg-3.2.4/libavcodec/raw.c ffmpeg-3.2.4.patch/libavcodec/raw.c
--- ffmpeg-3.2.4/libavcodec/raw.c	2017-02-10 14:25:27.000000000 +0100
+++ ffmpeg-3.2.4.patch/libavcodec/raw.c	2017-05-28 20:42:45.747088698 +0200
@@ -269,6 +269,11 @@
     { AV_PIX_FMT_YUV444P16LE, MKTAG('I', '4', 'F', 'L') },
     { AV_PIX_FMT_YUV444P16BE, MKTAG('I', '4', 'F', 'B') },
 
+    /* RPI */
+#ifdef RPI
+    { AV_PIX_FMT_SAND128,     MKTAG('S', 'A', 'N', 'D') },
+#endif
+
     /* special */
     { AV_PIX_FMT_RGB565LE,MKTAG( 3 ,  0 ,  0 ,  0 ) }, /* flipped RGB565LE */
     { AV_PIX_FMT_YUV444P, MKTAG('Y', 'V', '2', '4') }, /* YUV444P, swapped UV */
diff -Naur ffmpeg-3.2.4/libavcodec/rawenc.c ffmpeg-3.2.4.patch/libavcodec/rawenc.c
--- ffmpeg-3.2.4/libavcodec/rawenc.c	2017-02-10 14:25:27.000000000 +0100
+++ ffmpeg-3.2.4.patch/libavcodec/rawenc.c	2017-05-28 20:42:45.748088702 +0200
@@ -47,6 +47,47 @@
     return 0;
 }
 
+static uint8_t * cpy_sand_c(uint8_t * dst, const AVFrame * const frame, const int c_off)
+{
+    for (int y = 0; y != frame->height / 2; ++y) {
+        for (int x = 0; x < frame->width; x += frame->linesize[0]) {
+            const uint8_t * p = frame->data[1] + x * frame->linesize[3] + y * frame->linesize[0] + c_off;
+            const int w = FFMIN(frame->linesize[0], frame->width - x) / 2;
+            for (int i = 0; i < w; ++i)
+                *dst++ = p[i * 2];
+        }
+    }
+    return dst;
+}
+
+static int raw_sand_as_yuv420(AVCodecContext *avctx, AVPacket *pkt,
+                      const AVFrame *frame)
+{
+    int size = frame->width * frame->height * 3 / 2;
+    uint8_t * dst;
+    int ret;
+
+    if ((ret = ff_alloc_packet2(avctx, pkt, size, size)) < 0)
+        return ret;
+
+    dst = pkt->data;
+
+    // Luma is "easy"
+    for (int y = 0; y != frame->height; ++y) {
+        for (int x = 0; x < frame->width; x += frame->linesize[0]) {
+            const int w = FFMIN(frame->linesize[0], frame->width - x);
+            memcpy(dst,
+                frame->data[0] + x * frame->linesize[3] + y * frame->linesize[0], w);
+            dst += w;
+        }
+    }
+    // Chroma is dull
+    dst = cpy_sand_c(dst, frame, 0);
+    dst = cpy_sand_c(dst, frame, 1);
+
+    return 0;
+}
+
 static int raw_encode(AVCodecContext *avctx, AVPacket *pkt,
                       const AVFrame *frame, int *got_packet)
 {
@@ -56,6 +97,12 @@
     if (ret < 0)
         return ret;
 
+    if (frame->format == AV_PIX_FMT_SAND128) {
+        ret = raw_sand_as_yuv420(avctx, pkt, frame);
+        *got_packet = (ret == 0);
+        return ret;
+    }
+
     if ((ret = ff_alloc_packet2(avctx, pkt, ret, ret)) < 0)
         return ret;
     if ((ret = av_image_copy_to_buffer(pkt->data, pkt->size,
diff -Naur ffmpeg-3.2.4/libavcodec/rpi_hevc_transform.h ffmpeg-3.2.4.patch/libavcodec/rpi_hevc_transform.h
--- ffmpeg-3.2.4/libavcodec/rpi_hevc_transform.h	1970-01-01 01:00:00.000000000 +0100
+++ ffmpeg-3.2.4.patch/libavcodec/rpi_hevc_transform.h	2017-05-28 20:42:45.749088705 +0200
@@ -0,0 +1,3070 @@
+unsigned char rpi_hevc_transform [] = {
+21,
+106,
+0,
+144,
+47,
+1,
+37,
+106,
+0,
+144,
+66,
+1,
+53,
+106,
+0,
+144,
+192,
+4,
+69,
+106,
+0,
+144,
+192,
+4,
+85,
+106,
+0,
+144,
+220,
+5,
+169,
+3,
+62,
+64,
+79,
+64,
+3,
+232,
+32,
+0,
+0,
+0,
+12,
+248,
+0,
+136,
+0,
+0,
+192,
+248,
+0,
+0,
+64,
+232,
+0,
+2,
+0,
+0,
+12,
+248,
+0,
+168,
+0,
+0,
+192,
+248,
+0,
+0,
+0,
+96,
+3,
+232,
+32,
+0,
+0,
+0,
+7,
+232,
+0,
+2,
+0,
+0,
+8,
+232,
+0,
+4,
+0,
+0,
+12,
+248,
+0,
+128,
+0,
+0,
+192,
+8,
+4,
+0,
+4,
+232,
+64,
+0,
+0,
+0,
+5,
+232,
+0,
+8,
+0,
+0,
+128,
+69,
+113,
+66,
+12,
+248,
+0,
+128,
+0,
+0,
+192,
+8,
+4,
+0,
+128,
+69,
+113,
+70,
+128,
+144,
+40,
+0,
+4,
+255,
+48,
+192,
+128,
+3,
+32,
+8,
+16,
+0,
+76,
+254,
+48,
+192,
+9,
+4,
+32,
+8,
+0,
+0,
+4,
+254,
+0,
+144,
+128,
+2,
+0,
+8,
+2,
+0,
+128,
+144,
+23,
+0,
+4,
+255,
+48,
+192,
+128,
+3,
+32,
+8,
+20,
+0,
+76,
+254,
+48,
+192,
+4,
+4,
+32,
+8,
+0,
+0,
+140,
+248,
+44,
+0,
+0,
+0,
+32,
+48,
+4,
+0,
+128,
+69,
+113,
+66,
+242,
+140,
+211,
+192,
+34,
+31,
+41,
+3,
+70,
+192,
+80,
+7,
+164,
+255,
+36,
+204,
+96,
+2,
+0,
+248,
+62,
+0,
+3,
+255,
+55,
+208,
+120,
+3,
+224,
+3,
+190,
+11,
+16,
+139,
+246,
+91,
+0,
+103,
+90,
+0,
+70,
+192,
+80,
+7,
+164,
+255,
+36,
+204,
+224,
+2,
+0,
+248,
+62,
+0,
+3,
+255,
+55,
+208,
+120,
+3,
+224,
+3,
+190,
+11,
+16,
+139,
+246,
+91,
+0,
+103,
+90,
+0,
+225,
+64,
+242,
+64,
+3,
+232,
+128,
+0,
+0,
+0,
+7,
+232,
+0,
+2,
+0,
+0,
+57,
+239,
+224,
+247,
+255,
+255,
+72,
+192,
+95,
+207,
+88,
+122,
+88,
+124,
+137,
+64,
+26,
+64,
+4,
+232,
+64,
+0,
+0,
+0,
+149,
+96,
+161,
+64,
+152,
+64,
+128,
+144,
+35,
+0,
+72,
+232,
+0,
+4,
+0,
+0,
+65,
+232,
+32,
+0,
+0,
+0,
+128,
+144,
+27,
+0,
+4,
+232,
+0,
+8,
+0,
+0,
+69,
+96,
+145,
+64,
+168,
+64,
+128,
+144,
+19,
+0,
+72,
+232,
+0,
+4,
+0,
+0,
+65,
+232,
+32,
+0,
+0,
+0,
+128,
+144,
+11,
+0,
+74,
+232,
+0,
+8,
+0,
+0,
+242,
+140,
+221,
+192,
+57,
+239,
+32,
+8,
+0,
+0,
+41,
+3,
+239,
+3,
+12,
+248,
+0,
+128,
+0,
+0,
+192,
+248,
+4,
+0,
+12,
+248,
+0,
+132,
+64,
+0,
+192,
+248,
+4,
+0,
+0,
+96,
+255,
+159,
+154,
+255,
+0,
+232,
+0,
+4,
+0,
+0,
+255,
+159,
+165,
+255,
+4,
+255,
+48,
+204,
+16,
+3,
+224,
+251,
+62,
+0,
+4,
+255,
+51,
+204,
+128,
+3,
+224,
+251,
+16,
+0,
+76,
+254,
+51,
+204,
+128,
+3,
+224,
+251,
+20,
+0,
+128,
+64,
+6,
+232,
+64,
+0,
+0,
+0,
+140,
+248,
+47,
+0,
+0,
+0,
+224,
+99,
+0,
+0,
+32,
+247,
+240,
+207,
+16,
+3,
+32,
+247,
+176,
+207,
+17,
+19,
+32,
+247,
+112,
+207,
+18,
+35,
+32,
+247,
+48,
+207,
+19,
+51,
+32,
+247,
+240,
+206,
+20,
+67,
+32,
+247,
+176,
+206,
+21,
+83,
+32,
+247,
+112,
+206,
+22,
+99,
+32,
+247,
+48,
+206,
+23,
+115,
+32,
+247,
+240,
+205,
+24,
+131,
+32,
+247,
+176,
+205,
+25,
+147,
+32,
+247,
+112,
+205,
+26,
+163,
+32,
+247,
+48,
+205,
+27,
+179,
+32,
+247,
+240,
+204,
+28,
+195,
+32,
+247,
+176,
+204,
+29,
+211,
+32,
+247,
+112,
+204,
+30,
+227,
+32,
+247,
+48,
+204,
+31,
+243,
+4,
+255,
+51,
+204,
+128,
+3,
+224,
+251,
+16,
+0,
+76,
+254,
+51,
+204,
+128,
+3,
+224,
+251,
+20,
+0,
+0,
+237,
+32,
+0,
+0,
+0,
+140,
+248,
+47,
+0,
+0,
+0,
+224,
+99,
+0,
+0,
+111,
+3,
+4,
+254,
+0,
+128,
+0,
+4,
+0,
+248,
+0,
+0,
+2,
+232,
+32,
+0,
+0,
+0,
+140,
+248,
+32,
+0,
+0,
+0,
+224,
+35,
+0,
+0,
+64,
+232,
+0,
+2,
+0,
+0,
+193,
+232,
+0,
+1,
+0,
+0,
+1,
+106,
+116,
+30,
+90,
+0,
+169,
+3,
+73,
+64,
+52,
+64,
+45,
+64,
+2,
+64,
+10,
+64,
+64,
+198,
+1,
+7,
+8,
+232,
+63,
+0,
+0,
+0,
+6,
+232,
+253,
+255,
+255,
+255,
+0,
+246,
+0,
+0,
+0,
+4,
+215,
+64,
+3,
+96,
+2,
+248,
+0,
+35,
+0,
+0,
+64,
+56,
+0,
+0,
+4,
+248,
+0,
+36,
+0,
+0,
+64,
+56,
+8,
+0,
+0,
+240,
+64,
+0,
+132,
+3,
+128,
+240,
+0,
+0,
+132,
+3,
+128,
+144,
+137,
+0,
+131,
+98,
+0,
+255,
+64,
+0,
+0,
+20,
+200,
+243,
+0,
+0,
+128,
+144,
+129,
+0,
+131,
+102,
+0,
+158,
+67,
+0,
+2,
+248,
+0,
+35,
+0,
+0,
+64,
+56,
+0,
+0,
+4,
+248,
+0,
+36,
+0,
+0,
+64,
+56,
+8,
+0,
+0,
+240,
+64,
+0,
+132,
+3,
+128,
+240,
+0,
+0,
+132,
+3,
+128,
+144,
+108,
+0,
+131,
+98,
+0,
+255,
+64,
+0,
+0,
+20,
+200,
+243,
+0,
+0,
+128,
+144,
+100,
+0,
+131,
+102,
+0,
+248,
+64,
+0,
+112,
+0,
+192,
+243,
+211,
+31,
+128,
+248,
+0,
+0,
+112,
+0,
+192,
+243,
+211,
+31,
+128,
+144,
+161,
+0,
+188,
+64,
+67,
+232,
+0,
+2,
+0,
+0,
+0,
+255,
+64,
+0,
+0,
+20,
+200,
+243,
+0,
+0,
+128,
+144,
+150,
+0,
+195,
+232,
+0,
+2,
+0,
+0,
+12,
+128,
+7,
+192,
+130,
+248,
+0,
+0,
+112,
+192,
+224,
+16,
+195,
+31,
+132,
+248,
+1,
+0,
+112,
+0,
+224,
+16,
+203,
+31,
+3,
+99,
+131,
+71,
+68,
+232,
+32,
+0,
+0,
+0,
+0,
+99,
+2,
+99,
+23,
+102,
+7,
+106,
+127,
+156,
+182,
+255,
+0,
+248,
+64,
+0,
+112,
+0,
+192,
+243,
+211,
+31,
+128,
+248,
+0,
+0,
+112,
+0,
+192,
+243,
+211,
+31,
+128,
+144,
+112,
+0,
+188,
+64,
+67,
+232,
+0,
+2,
+0,
+0,
+0,
+255,
+64,
+0,
+0,
+20,
+200,
+243,
+0,
+0,
+128,
+144,
+101,
+0,
+195,
+232,
+0,
+2,
+0,
+0,
+12,
+128,
+7,
+192,
+130,
+248,
+0,
+0,
+112,
+192,
+224,
+16,
+195,
+31,
+132,
+248,
+1,
+0,
+112,
+0,
+224,
+16,
+203,
+31,
+25,
+102,
+9,
+106,
+2,
+30,
+41,
+3,
+26,
+87,
+162,
+64,
+64,
+198,
+1,
+23,
+127,
+158,
+103,
+255,
+239,
+3,
+0,
+254,
+0,
+143,
+92,
+0,
+0,
+240,
+12,
+0,
+0,
+254,
+64,
+143,
+93,
+0,
+0,
+240,
+12,
+0,
+0,
+254,
+128,
+143,
+94,
+0,
+0,
+240,
+12,
+0,
+0,
+254,
+192,
+143,
+95,
+0,
+0,
+240,
+12,
+0,
+0,
+254,
+192,
+142,
+208,
+0,
+0,
+240,
+12,
+0,
+0,
+254,
+128,
+142,
+209,
+0,
+0,
+240,
+12,
+0,
+0,
+254,
+64,
+142,
+210,
+0,
+0,
+240,
+12,
+0,
+0,
+254,
+0,
+142,
+211,
+0,
+0,
+240,
+12,
+0,
+128,
+144,
+107,
+0,
+8,
+255,
+99,
+23,
+0,
+212,
+192,
+51,
+0,
+0,
+8,
+255,
+163,
+23,
+0,
+228,
+192,
+51,
+0,
+0,
+8,
+255,
+227,
+23,
+0,
+244,
+192,
+51,
+0,
+0,
+8,
+255,
+35,
+52,
+0,
+180,
+192,
+51,
+0,
+0,
+8,
+255,
+99,
+52,
+0,
+164,
+192,
+51,
+0,
+0,
+8,
+255,
+163,
+52,
+0,
+148,
+192,
+51,
+0,
+0,
+111,
+3,
+239,
+3,
+0,
+254,
+0,
+143,
+12,
+0,
+0,
+240,
+12,
+0,
+0,
+254,
+64,
+143,
+13,
+0,
+0,
+240,
+12,
+0,
+0,
+254,
+128,
+143,
+14,
+0,
+0,
+240,
+12,
+0,
+0,
+254,
+192,
+143,
+15,
+0,
+0,
+240,
+12,
+0,
+0,
+254,
+192,
+142,
+16,
+0,
+0,
+240,
+12,
+0,
+0,
+254,
+128,
+142,
+17,
+0,
+0,
+240,
+12,
+0,
+0,
+254,
+64,
+142,
+18,
+0,
+0,
+240,
+12,
+0,
+0,
+254,
+0,
+142,
+19,
+0,
+0,
+240,
+12,
+0,
+128,
+144,
+33,
+0,
+8,
+255,
+99,
+3,
+0,
+212,
+192,
+51,
+0,
+0,
+8,
+255,
+163,
+3,
+0,
+228,
+192,
+51,
+0,
+0,
+8,
+255,
+227,
+3,
+0,
+244,
+192,
+51,
+0,
+0,
+8,
+255,
+35,
+4,
+0,
+180,
+192,
+51,
+0,
+0,
+8,
+255,
+99,
+4,
+0,
+164,
+192,
+51,
+0,
+0,
+8,
+255,
+163,
+4,
+0,
+148,
+192,
+51,
+0,
+0,
+111,
+3,
+32,
+246,
+192,
+11,
+1,
+16,
+32,
+246,
+2,
+137,
+47,
+240,
+40,
+246,
+2,
+140,
+47,
+240,
+128,
+245,
+99,
+140,
+5,
+4,
+0,
+247,
+99,
+140,
+1,
+20,
+88,
+246,
+99,
+140,
+1,
+20,
+0,
+247,
+35,
+136,
+62,
+226,
+32,
+247,
+35,
+136,
+32,
+210,
+0,
+247,
+34,
+136,
+63,
+2,
+208,
+246,
+34,
+136,
+0,
+4,
+0,
+247,
+99,
+136,
+58,
+162,
+32,
+247,
+99,
+136,
+33,
+146,
+0,
+247,
+98,
+136,
+59,
+18,
+208,
+246,
+98,
+136,
+0,
+20,
+0,
+247,
+162,
+136,
+33,
+2,
+88,
+246,
+98,
+137,
+2,
+68,
+88,
+246,
+162,
+137,
+3,
+68,
+208,
+254,
+227,
+136,
+60,
+242,
+192,
+243,
+188,
+11,
+208,
+254,
+227,
+136,
+56,
+178,
+192,
+243,
+188,
+10,
+32,
+255,
+226,
+136,
+38,
+58,
+192,
+243,
+60,
+0,
+208,
+254,
+227,
+136,
+59,
+242,
+192,
+243,
+60,
+128,
+32,
+255,
+226,
+136,
+49,
+58,
+192,
+243,
+60,
+128,
+0,
+255,
+226,
+136,
+34,
+34,
+192,
+243,
+60,
+128,
+32,
+255,
+226,
+136,
+37,
+58,
+192,
+243,
+60,
+128,
+0,
+254,
+192,
+136,
+1,
+4,
+0,
+240,
+0,
+160,
+0,
+255,
+194,
+8,
+0,
+52,
+195,
+243,
+0,
+128,
+0,
+255,
+202,
+40,
+0,
+52,
+195,
+243,
+0,
+128,
+0,
+254,
+0,
+240,
+35,
+10,
+0,
+240,
+60,
+0,
+0,
+254,
+192,
+136,
+1,
+4,
+0,
+240,
+0,
+160,
+0,
+255,
+226,
+140,
+34,
+34,
+195,
+243,
+60,
+0,
+32,
+255,
+227,
+140,
+36,
+58,
+192,
+243,
+60,
+0,
+0,
+254,
+192,
+136,
+0,
+4,
+0,
+240,
+0,
+160,
+16,
+246,
+226,
+136,
+35,
+50,
+16,
+246,
+226,
+136,
+35,
+50,
+32,
+246,
+226,
+136,
+35,
+50,
+32,
+254,
+226,
+136,
+35,
+58,
+192,
+243,
+60,
+0,
+11,
+96,
+0,
+254,
+0,
+240,
+1,
+4,
+0,
+240,
+64,
+115,
+5,
+106,
+0,
+144,
+173,
+1,
+27,
+96,
+0,
+254,
+0,
+240,
+1,
+4,
+0,
+240,
+64,
+147,
+5,
+106,
+0,
+144,
+227,
+0,
+64,
+246,
+163,
+140,
+1,
+4,
+0,
+246,
+192,
+175,
+63,
+2,
+0,
+246,
+192,
+174,
+59,
+2,
+0,
+246,
+128,
+175,
+62,
+2,
+0,
+246,
+128,
+174,
+58,
+2,
+0,
+246,
+64,
+175,
+61,
+2,
+0,
+246,
+64,
+174,
+57,
+2,
+0,
+255,
+43,
+240,
+4,
+212,
+192,
+243,
+128,
+11,
+64,
+254,
+43,
+240,
+1,
+228,
+192,
+243,
+128,
+10,
+64,
+254,
+43,
+240,
+1,
+244,
+192,
+243,
+128,
+10,
+64,
+254,
+43,
+240,
+1,
+180,
+192,
+243,
+128,
+10,
+64,
+254,
+43,
+141,
+0,
+164,
+192,
+243,
+128,
+10,
+88,
+246,
+35,
+141,
+3,
+68,
+32,
+247,
+35,
+141,
+191,
+66,
+240,
+246,
+35,
+141,
+50,
+66,
+0,
+255,
+235,
+143,
+52,
+242,
+192,
+243,
+60,
+128,
+0,
+255,
+43,
+240,
+2,
+212,
+192,
+243,
+128,
+11,
+0,
+255,
+43,
+240,
+191,
+226,
+192,
+243,
+188,
+10,
+64,
+254,
+43,
+141,
+0,
+180,
+192,
+243,
+128,
+10,
+88,
+246,
+35,
+141,
+2,
+68,
+32,
+247,
+35,
+141,
+190,
+66,
+240,
+246,
+35,
+141,
+50,
+66,
+0,
+255,
+171,
+143,
+52,
+226,
+192,
+243,
+60,
+128,
+0,
+255,
+43,
+240,
+4,
+180,
+192,
+243,
+128,
+11,
+0,
+255,
+43,
+240,
+191,
+226,
+192,
+243,
+188,
+10,
+128,
+253,
+43,
+240,
+3,
+212,
+192,
+243,
+128,
+10,
+64,
+254,
+35,
+141,
+1,
+196,
+192,
+243,
+128,
+10,
+88,
+246,
+35,
+141,
+3,
+68,
+32,
+247,
+35,
+141,
+189,
+66,
+240,
+246,
+35,
+141,
+50,
+66,
+0,
+255,
+107,
+143,
+52,
+210,
+192,
+243,
+60,
+128,
+0,
+255,
+43,
+240,
+4,
+148,
+192,
+243,
+128,
+11,
+64,
+254,
+43,
+240,
+1,
+164,
+192,
+243,
+128,
+10,
+64,
+254,
+43,
+240,
+1,
+180,
+192,
+243,
+128,
+10,
+64,
+254,
+43,
+240,
+1,
+244,
+192,
+243,
+128,
+10,
+64,
+254,
+43,
+141,
+0,
+228,
+192,
+243,
+128,
+10,
+88,
+246,
+35,
+141,
+3,
+68,
+32,
+247,
+35,
+141,
+187,
+66,
+240,
+246,
+35,
+141,
+50,
+66,
+0,
+255,
+235,
+142,
+52,
+178,
+192,
+243,
+60,
+128,
+0,
+255,
+43,
+240,
+2,
+148,
+192,
+243,
+128,
+11,
+0,
+255,
+43,
+240,
+187,
+162,
+192,
+243,
+188,
+10,
+64,
+254,
+43,
+141,
+0,
+244,
+192,
+243,
+128,
+10,
+88,
+246,
+35,
+141,
+2,
+68,
+32,
+247,
+35,
+141,
+186,
+66,
+240,
+246,
+35,
+141,
+50,
+66,
+0,
+255,
+171,
+142,
+52,
+162,
+192,
+243,
+60,
+128,
+0,
+255,
+43,
+240,
+4,
+244,
+192,
+243,
+128,
+11,
+0,
+255,
+43,
+240,
+187,
+162,
+192,
+243,
+188,
+10,
+128,
+253,
+43,
+240,
+3,
+148,
+192,
+243,
+128,
+10,
+64,
+254,
+35,
+141,
+1,
+132,
+192,
+243,
+128,
+10,
+88,
+246,
+35,
+141,
+3,
+68,
+32,
+247,
+35,
+141,
+185,
+66,
+240,
+246,
+35,
+141,
+50,
+66,
+0,
+255,
+107,
+142,
+52,
+146,
+192,
+243,
+60,
+128,
+64,
+255,
+98,
+141,
+0,
+52,
+192,
+243,
+0,
+0,
+0,
+254,
+0,
+240,
+53,
+10,
+0,
+240,
+60,
+0,
+0,
+254,
+0,
+240,
+1,
+4,
+0,
+240,
+64,
+147,
+5,
+106,
+0,
+144,
+177,
+0,
+88,
+246,
+163,
+140,
+1,
+4,
+128,
+245,
+99,
+141,
+10,
+4,
+88,
+246,
+162,
+138,
+1,
+68,
+0,
+247,
+162,
+138,
+36,
+162,
+88,
+254,
+162,
+138,
+3,
+164,
+192,
+243,
+128,
+11,
+0,
+255,
+226,
+137,
+32,
+2,
+195,
+243,
+60,
+0,
+32,
+247,
+226,
+137,
+42,
+114,
+0,
+255,
+34,
+138,
+33,
+18,
+195,
+243,
+60,
+0,
+32,
+247,
+34,
+138,
+42,
+130,
+16,
+246,
+98,
+138,
+40,
+114,
+16,
+246,
+98,
+138,
+41,
+146,
+32,
+246,
+98,
+138,
+41,
+146,
+32,
+246,
+226,
+137,
+41,
+146,
+40,
+246,
+34,
+138,
+41,
+146,
+32,
+247,
+163,
+141,
+63,
+178,
+32,
+247,
+227,
+141,
+62,
+162,
+0,
+254,
+0,
+240,
+8,
+4,
+0,
+240,
+128,
+11,
+128,
+253,
+35,
+240,
+9,
+100,
+192,
+243,
+128,
+10,
+128,
+253,
+163,
+141,
+128,
+115,
+192,
+243,
+152,
+10,
+88,
+246,
+163,
+141,
+4,
+100,
+208,
+246,
+35,
+139,
+0,
+100,
+32,
+255,
+34,
+139,
+53,
+202,
+192,
+243,
+60,
+128,
+0,
+254,
+0,
+139,
+0,
+4,
+0,
+240,
+0,
+160,
+240,
+246,
+163,
+141,
+48,
+98,
+0,
+247,
+99,
+139,
+63,
+210,
+0,
+247,
+98,
+139,
+1,
+212,
+88,
+254,
+98,
+139,
+1,
+212,
+192,
+243,
+128,
+11,
+32,
+255,
+99,
+139,
+62,
+98,
+192,
+243,
+188,
+10,
+88,
+246,
+98,
+139,
+1,
+212,
+240,
+246,
+98,
+139,
+50,
+210,
+0,
+247,
+163,
+128,
+59,
+146,
+0,
+247,
+160,
+128,
+1,
+36,
+88,
+254,
+160,
+128,
+1,
+36,
+192,
+243,
+128,
+11,
+0,
+247,
+163,
+128,
+58,
+98,
+64,
+255,
+35,
+240,
+0,
+100,
+192,
+243,
+128,
+10,
+64,
+255,
+163,
+128,
+0,
+164,
+192,
+243,
+128,
+10,
+88,
+246,
+160,
+128,
+1,
+36,
+240,
+246,
+160,
+128,
+50,
+34,
+8,
+255,
+227,
+143,
+54,
+242,
+192,
+243,
+60,
+128,
+40,
+255,
+227,
+142,
+54,
+178,
+192,
+243,
+60,
+128,
+0,
+254,
+0,
+240,
+39,
+10,
+0,
+240,
+60,
+128,
+8,
+255,
+163,
+143,
+45,
+226,
+192,
+243,
+60,
+128,
+0,
+254,
+0,
+240,
+44,
+10,
+0,
+240,
+60,
+0,
+0,
+254,
+0,
+240,
+40,
+10,
+0,
+240,
+60,
+128,
+8,
+255,
+163,
+142,
+2,
+162,
+192,
+243,
+60,
+128,
+90,
+0,
+169,
+3,
+14,
+96,
+4,
+31,
+169,
+3,
+30,
+96,
+1,
+31,
+73,
+64,
+52,
+64,
+45,
+64,
+2,
+64,
+10,
+64,
+64,
+198,
+1,
+7,
+8,
+232,
+63,
+0,
+0,
+0,
+6,
+232,
+253,
+255,
+255,
+255,
+0,
+246,
+0,
+0,
+0,
+4,
+215,
+64,
+3,
+96,
+2,
+248,
+0,
+35,
+0,
+0,
+64,
+56,
+0,
+0,
+4,
+248,
+0,
+36,
+0,
+0,
+64,
+56,
+8,
+0,
+0,
+240,
+64,
+0,
+132,
+3,
+30,
+106,
+132,
+24,
+128,
+240,
+0,
+0,
+132,
+3,
+128,
+144,
+143,
+0,
+131,
+98,
+0,
+255,
+64,
+0,
+0,
+20,
+200,
+243,
+0,
+0,
+128,
+144,
+135,
+0,
+131,
+102,
+0,
+158,
+71,
+0,
+2,
+248,
+0,
+35,
+0,
+0,
+64,
+56,
+0,
+0,
+4,
+248,
+0,
+36,
+0,
+0,
+64,
+56,
+8,
+0,
+0,
+240,
+64,
+0,
+132,
+3,
+30,
+106,
+132,
+24,
+128,
+240,
+0,
+0,
+132,
+3,
+128,
+144,
+112,
+0,
+131,
+98,
+0,
+255,
+64,
+0,
+0,
+20,
+200,
+243,
+0,
+0,
+128,
+144,
+104,
+0,
+131,
+102,
+0,
+248,
+64,
+0,
+112,
+0,
+192,
+243,
+211,
+31,
+30,
+106,
+134,
+24,
+128,
+248,
+0,
+0,
+112,
+0,
+192,
+243,
+211,
+31,
+128,
+144,
+123,
+0,
+188,
+64,
+67,
+232,
+0,
+2,
+0,
+0,
+0,
+255,
+64,
+0,
+0,
+20,
+200,
+243,
+0,
+0,
+128,
+144,
+112,
+0,
+195,
+232,
+0,
+2,
+0,
+0,
+12,
+128,
+7,
+192,
+130,
+248,
+0,
+0,
+112,
+192,
+224,
+16,
+195,
+31,
+132,
+248,
+1,
+0,
+112,
+0,
+224,
+16,
+203,
+31,
+3,
+99,
+131,
+71,
+68,
+232,
+32,
+0,
+0,
+0,
+0,
+99,
+2,
+99,
+23,
+102,
+7,
+106,
+127,
+156,
+178,
+255,
+0,
+248,
+64,
+0,
+112,
+0,
+192,
+243,
+211,
+31,
+30,
+106,
+134,
+24,
+128,
+248,
+0,
+0,
+112,
+0,
+192,
+243,
+211,
+31,
+128,
+144,
+72,
+0,
+188,
+64,
+67,
+232,
+0,
+2,
+0,
+0,
+0,
+255,
+64,
+0,
+0,
+20,
+200,
+243,
+0,
+0,
+128,
+144,
+61,
+0,
+195,
+232,
+0,
+2,
+0,
+0,
+12,
+128,
+7,
+192,
+130,
+248,
+0,
+0,
+112,
+192,
+224,
+16,
+195,
+31,
+132,
+248,
+1,
+0,
+112,
+0,
+224,
+16,
+203,
+31,
+25,
+102,
+9,
+106,
+2,
+30,
+41,
+3,
+26,
+87,
+162,
+64,
+64,
+198,
+1,
+23,
+127,
+158,
+95,
+255,
+239,
+3,
+0,
+254,
+128,
+143,
+94,
+0,
+0,
+240,
+12,
+0,
+0,
+254,
+192,
+143,
+95,
+0,
+0,
+240,
+12,
+0,
+0,
+254,
+192,
+142,
+208,
+0,
+0,
+240,
+12,
+0,
+0,
+254,
+128,
+142,
+209,
+0,
+0,
+240,
+12,
+0,
+128,
+144,
+47,
+0,
+8,
+255,
+227,
+23,
+0,
+244,
+192,
+51,
+0,
+0,
+8,
+255,
+35,
+52,
+0,
+180,
+192,
+51,
+0,
+0,
+111,
+3,
+239,
+3,
+0,
+254,
+128,
+143,
+14,
+0,
+0,
+240,
+12,
+0,
+0,
+254,
+192,
+143,
+15,
+0,
+0,
+240,
+12,
+0,
+0,
+254,
+192,
+142,
+16,
+0,
+0,
+240,
+12,
+0,
+0,
+254,
+128,
+142,
+17,
+0,
+0,
+240,
+12,
+0,
+128,
+144,
+13,
+0,
+8,
+255,
+227,
+3,
+0,
+244,
+192,
+51,
+0,
+0,
+8,
+255,
+35,
+4,
+0,
+180,
+192,
+51,
+0,
+0,
+111,
+3,
+32,
+246,
+192,
+11,
+1,
+16,
+32,
+246,
+2,
+140,
+47,
+240,
+32,
+247,
+35,
+141,
+63,
+178,
+64,
+254,
+35,
+141,
+2,
+68,
+192,
+243,
+128,
+11,
+32,
+255,
+35,
+240,
+58,
+226,
+192,
+243,
+188,
+10,
+0,
+254,
+0,
+141,
+4,
+4,
+0,
+240,
+128,
+10,
+88,
+246,
+35,
+141,
+3,
+68,
+240,
+246,
+35,
+141,
+48,
+66,
+0,
+247,
+227,
+143,
+52,
+242,
+32,
+247,
+227,
+142,
+52,
+178,
+90,
+0,
+161,
+3,
+6,
+64,
+23,
+64,
+96,
+8,
+70,
+98,
+97,
+8,
+70,
+98,
+98,
+8,
+70,
+98,
+99,
+8,
+70,
+98,
+100,
+8,
+70,
+98,
+101,
+8,
+70,
+98,
+255,
+159,
+8,
+250,
+23,
+102,
+7,
+106,
+112,
+30,
+33,
+3,
+};
diff -Naur ffmpeg-3.2.4/libavcodec/rpi_hevc_transform.s ffmpeg-3.2.4.patch/libavcodec/rpi_hevc_transform.s
--- ffmpeg-3.2.4/libavcodec/rpi_hevc_transform.s	1970-01-01 01:00:00.000000000 +0100
+++ ffmpeg-3.2.4.patch/libavcodec/rpi_hevc_transform.s	2017-05-28 20:42:45.750088709 +0200
@@ -0,0 +1,917 @@
+# ******************************************************************************
+# Argon Design Ltd.
+# (c) Copyright 2015 Argon Design Ltd. All rights reserved.
+#
+# Module : HEVC
+# Author : Peter de Rivaz
+# ******************************************************************************
+
+# HEVC VPU Transform
+#
+# Transform matrix can be thought of as
+#   output row vector = input row vector * transMatrix2
+#
+# The even rows of the matrix are symmetric
+# The odd rows of the matrix are antisymmetric
+#
+# So only need to compute the first half of the results, then can compute the remainder with a butterfly
+#
+# EXAMPLE
+#   (a b c d) (1 2  2  1)
+#             (3 4 -4 -3)
+#             (5 6  6  5)
+#             (7 8 -8 -7)
+#
+#  x=(a c)(1 2) = 1a+5c 2a+6c
+#         (5 6)
+#
+#  y=(b d)(3 4) = 3b+7d 4b+8d
+#         (7 8)
+#
+#  u=x+y = 1a+5c+3b+7d 2a+4b+6c+8d
+#  v=x-y = 1a+5c-3b-7d 2a+6c-4b-8d
+#
+#  Final results are (u , v[::-1])
+#
+#
+#  For 32x1 input, load even rows into HX(0++,0), odd rows into HX(16++,0)
+#  Apply the even matrix first and stop before rounding
+#  Then apply the odd matrix in a full manner:
+#
+#   First step is to compute partial products with the first input (16 cycles)
+#   1a 3b 5c 7d   16x1 input coefficients produce 16x16 output
+#   2a 4b 6c 8d
+#   2a -4b 6c -8d
+#   1a -3b 5c -7d
+#
+#   Second step is to sum partial products into final position (8 cycles)
+#   1a+3b+5c+7d
+#   2a+4b+6c+8d
+#   2a-4b+6c-8d
+#   1a-3b+5c-7d
+#
+#   Then can apply butterfly to combine even results and odd results + rounding to produce 16 rows of output at a time (need to save in transposed format)
+#
+#   For 16x16 no butterfly is required and can store final results in original location  (Could do 2 16x16s in parallel to make use of the trick - saves on the adds)
+#
+#   For 8x8 we could compute two in parallel.
+#
+#
+
+# Columns are transformed first
+#
+# Store top left half of transMatrix2 in
+# Store bottom left half of transMatrix2 in HX(32,32)
+#
+# For 16x16
+# HX(0:15,0) contains input data before transform
+# HY(0:15,0) contains 32bit output data after transform
+# HX(32,0) contains even rows of left half of transMatrix2
+# HX(32,32) contains odd rows of left half of transMatrix2
+# HY(48,0) contains partial products ready for summing
+#
+
+
+# hevc_trans_16x16(short *transMatrix2, short *coeffs, int num) # TODO add size so we can branch to correct implementation (or perhaps have coeffs32 and num32 as secondary inputs!)
+# transMatrix2: address of the constant matrix (must be at 32 byte aligned address in Videocore memory)
+# coeffs: address of the transform coefficients (must be at 32 byte aligned address in Videocore memory)
+# num: number of 16x16 transforms to be done
+# coeffs32
+# num32: number of 32x32 transforms
+# command 0 for transform, 1 for memclear16(int16_t *dst,num16)
+#
+hevc_trans_16x16:
+  cmp r5,1
+  beq memclear16
+  cmp r5,2
+  beq hevc_deblock_16x16
+  cmp r5,3
+  beq hevc_uv_deblock_16x16
+  cmp r5,4
+  beq hevc_uv_deblock_16x16_with_clear
+  cmp r5,5
+  beq hevc_run_command_list
+
+  push r6-r15, lr # TODO cut down number of used registers
+  mov r14,r3 # coeffs32
+  mov r15,r4 # num32
+  mov r3, 16*2 # Stride of transMatrix2 in bytes
+  vldh HX(32++,0),(r0 += r3) REP 16 # This is the 16x16 matrix, a transform is equivalent to multiplying input row vector * matrix
+
+  add r0, 16*16*2 # For 32x32 transforms we also need this matrix
+  vldh HX(32++,32),(r0 += r3) REP 16 # This is the odd 16x16 matrix
+
+  # Now use r0 to describe which matrix we are working on.
+  # Allows us to prefetch the next block of coefficients for efficiency.
+  mov r0,0 # This describes the location where we read our coefficients from
+  mov r3,16*2 # Stride of coefficients in bytes (TODO remove)
+  mov r7,16*16*2 # Total block size
+  mov r8,64*16 # Value used to swap from current to next VRF location
+  vldh HX(0++,0)+r0,(r1 += r3) REP 16
+  mov r4,64 # Constant used for rounding first pass
+  mov r5,1<<11 # Constant used for rounding second pass
+
+  # At start of block r0,r1 point to the current block (that has already been loaded)
+block_loop:
+  eor r0,r8
+  add r1,r7
+  # Prefetch the next block
+  vldh HX(0++,0)+r0,(r1 += r3) REP 16
+  eor r0,r8
+  sub r1,r7
+
+  # Transform the current block
+  bl col_trans_16
+  vadd HY(0++,0)+r0,HY(0++,0)+r0,r4 REP 16   # Now add on rounding, shift down by 7, and saturate
+  #vsasls HY(0++,0)+r0,HY(0++,0)+r0,9 REP 16 # 9+7=16 so this ends up with the output saturated and in the top half of the word.
+  vasl HY(0++,0)+r0,HY(0++,0)+r0,9 REP 16    # This should be saturating, but the instruction above does not assemble?
+  vmov VX(0,0++)+r0, HX(0++,32)+r0 REP 16    # For simplicity transpose this back to the original position
+
+  bl col_trans_16
+  vadd HY(0++,0)+r0,HY(0++,0)+r0,r5 REP 16   # Now add on rounding, shift down by 7, and saturate
+  #vsasls HY(0++,0)+r0,HY(0++,0)+r0,4 REP 16 # 4+12=16 so this ends up with the output saturated and in the top half of the word.
+  vasl HY(0++,0)+r0,HY(0++,0)+r0,4 REP 16    # This should be saturating, but the instruction above does not assemble?  (Probably because it ends with ls which is interpreted as a condition flag)
+
+  # Save results - note there has been a transposition during the processing so we save columns
+  vsth VX(0,32++)+r0, (r1 += r3) REP 16
+
+  # Move onto next block
+  eor r0,r8
+  add r1,r7
+
+  addcmpbgt r2,-1,0,block_loop
+
+  # Now go and do any 32x32 transforms
+  b hevc_trans_32x32
+
+  pop r6-r15, pc
+
+# r1,r2,r3 r7,r8 should be preserved
+# HX(0++,0)+r0 is the block to be transformed
+# HX(32++,0)+r6 is the 16x16 matrix of transform coefficients
+# Use HY(48,0) for intermediate results
+# r0 can be used, but should be returned to its original value at the end
+col_trans_16:
+  add r6,r0,16 # Final value for this loop
+col_trans_16_loop:
+  # First compute partial products for a single column
+  vmul32s HY(48++,0), VX(0,0)+r0, VX(32,0++) REP 16
+  # Then sum up the results and place back
+  vadd VY(0,0)+r0, VY(48,0++), VY(48,8++) REP 8 CLRA SACC
+  addcmpblt r0,1,r6,col_trans_16_loop
+  sub r0,16  # put r0 back to its original value
+  b lr
+
+col_trans_odd_16:
+  add r6,r0,16 # Final value for this loop
+col_trans_odd_16_loop:
+  # First compute partial products for a single column
+  vmul32s HY(48++,0), VX(0,0)+r0, VX(32,32++) REP 16
+  # Then sum up the results and place back
+  vadd VY(0,0)+r0, VY(48,0++), VY(48,8++) REP 8 CLRA SACC
+  addcmpblt r0,1,r6,col_trans_odd_16_loop
+  sub r0,16  # put r0 back to its original value
+  b lr
+
+# hevc_trans_32x32(short *transMatrix2, short *coeffs, int num)
+# transMatrix2: address of the constant matrix (must be at 32 byte aligned address in Videocore memory) Even followed by odd
+# coeffs: address of the transform coefficients (must be at 32 byte aligned address in Videocore memory)
+# num: number of 16x16 transforms to be done
+#
+hevc_trans_32x32:
+  mov r1,r14 # coeffs
+  mov r2,r15 # num
+
+  # Fetch odd transform matrix
+  #mov r3, 16*2 # Stride of transMatrix2 in bytes (and of coefficients)
+  #vldh HX(32++,0),(r0 += r3) REP 16 # This is the even 16x16 matrix
+  #add r0, 16*16*2
+  #vldh HX(32++,32),(r0 += r3) REP 16 # This is the odd 16x16 matrix
+
+  mov r3, 32*2*2 # Stride used to fetch alternate rows of our input coefficient buffer
+  mov r7, 16*16*2 # Total block size
+  sub sp,sp,32*32*2+32 # Allocate some space on the stack for us to store 32*32 shorts as temporary results (needs to be aligned)
+  # set r8 to 32byte aligned stack pointer
+  add r8,sp,31
+  lsr r8,5
+  lsl r8,5
+  mov r9,r8  # Backup of the temporary storage
+  mov r10,r1 # Backup of the coefficient buffer
+block_loop32:
+
+  # COLUMN TRANSFORM
+  mov r4, 64 # Constant used for rounding first pass
+  mov r5, 9 # left shift used for rounding first pass
+
+  # Transform the first 16 columns
+  mov r1,r10  # Input Coefficient buffer
+  mov r8,r9   # Output temporary storage
+  bl trans32
+  # Transform the second 16 columns
+  add r8,32*16*2
+  add r1,32
+  bl trans32
+
+  # ROW TRANSFORM
+  mov r4, 1<<11 # Constant used for rounding second pass
+  mov r5, 4 # left shift used for rounding second pass
+
+  mov r1,r9  # Input temporary storage
+  mov r8,r10   # Output Coefficient buffer
+  bl trans32
+  # Transform the second 16 columns
+  add r8,32*16*2
+  add r1,32
+  bl trans32
+
+  add r10, 32*32*2 # move onto next block of coefficients
+  addcmpbgt r2,-1,0,block_loop32
+
+  add sp,sp,32*32*2+32 # Restore stack
+
+  pop r6-r15, pc
+
+trans32:
+  push lr
+  # We can no longer afford the VRF space to do prefetching when doing 32x32
+  # Fetch the even rows
+  vldh HX(0++,0),(r1 += r3) REP 16
+  # Fetch the odd rows
+  vldh HX(16++,0),64(r1 += r3) REP 16 # First odd row is 32 shorts ahead of r1
+
+  # Transform the even rows using even matrix
+  mov r0, 0 # Even rows
+  bl col_trans_16
+
+  # Now transform the odd rows using odd matrix
+  mov r0, 64*16 # Odd rows
+  bl col_trans_odd_16
+
+  # Now apply butterfly to compute the first 16 results
+  vadd HY(48++,0),HY(0++,0),HY(16++,0) REP 16
+  vadd HY(48++,0),HY(48++,0),r4 REP 16   # add on rounding,
+  vasl HY(48++,0),HY(48++,0),r5 REP 16    # shift down by 7, and saturate
+  # 16bit results now in HX(48,32)
+  mov r0,r8
+  mov r6,32*2
+  vsth VX(48,32++),(r0+=r6) REP 16
+
+  # Now apply butterfly to compute the second 16 results (in reverse order)
+  vsub HY(63,0),HY(0 ,0),HY(16,0)
+  vsub HY(62,0),HY(1 ,0),HY(17,0)
+  vsub HY(61,0),HY(2 ,0),HY(18,0)
+  vsub HY(60,0),HY(3 ,0),HY(19,0)
+  vsub HY(59,0),HY(4 ,0),HY(20,0)
+  vsub HY(58,0),HY(5 ,0),HY(21,0)
+  vsub HY(57,0),HY(6 ,0),HY(22,0)
+  vsub HY(56,0),HY(7 ,0),HY(23,0)
+  vsub HY(55,0),HY(8 ,0),HY(24,0)
+  vsub HY(54,0),HY(9 ,0),HY(25,0)
+  vsub HY(53,0),HY(10,0),HY(26,0)
+  vsub HY(52,0),HY(11,0),HY(27,0)
+  vsub HY(51,0),HY(12,0),HY(28,0)
+  vsub HY(50,0),HY(13,0),HY(29,0)
+  vsub HY(49,0),HY(14,0),HY(30,0)
+  vsub HY(48,0),HY(15,0),HY(31,0)
+  vadd HY(48++,0),HY(48++,0),r4 REP 16   # add on rounding,
+  vasl HY(48++,0),HY(48++,0),r5 REP 16    # shift down by 7, and saturate
+  add r0,r8,32
+  vsth VX(48,32++),(r0+=r6) REP 16
+  pop pc
+
+memclear16:
+  # r0 is address
+  # r1 is number of 16bits values to set to 0 (may overrun past end and clear more than specified)
+  vmov HX(0++,0),0 REP 16
+  mov r2,32
+loop:
+  vsth HX(0++,0),(r0+=r2) REP 16
+  add r0,16*16*2
+  sub r1,16*16
+  cmp r1,0
+  bgt loop
+  b lr
+
+
+################################################################################
+# HEVC VPU Deblock
+#
+# Vertical edges before horizontal
+# Decision can change every 4 pixels, but only 8 pixel boundaries are deblocked
+#
+# ARM is responsible for storing beta and tc for each 4 pixels horiz and vert edge.
+# The VPU code works in units of 16x16 blocks.
+# We do vertical filtering for the current block followed by horizontal filtering for the previous (except for the first time).
+# One final horizontal filter is required at the end.
+# PCM is not allowed in this code.
+#
+#
+# H(16-4:16+15,0) contains previous block (note that we need 4 lines above of context that may get altered during filtering)
+# H(16:31,16) contains current block (note that we do not need the upper lines until the horizontal filtering.
+
+.set P0,63
+.set P1,62
+.set P2,61
+.set P3,60
+.set Q0,59
+.set Q1,58
+.set Q2,57
+.set Q3,56
+
+.set dp,32
+.set dq,33
+.set d,34
+.set decision,35
+.set beta,36
+.set beta2,37
+.set beta3,38
+.set ptest,39
+.set qtest,40
+.set pqtest,41
+.set thresh,42
+.set deltatest, 44
+.set deltap1, 45
+.set tc25, 46
+.set setup,47
+.set tc,48
+.set tc25,49
+.set tc2, 50
+.set do_filter, 51
+.set delta, 52
+.set tc10, 53
+.set delta0, 54
+.set delta1, 55
+.set zeros, 0
+.set setup_input, 1
+.set deltaq1, 2
+
+
+
+# hevc_deblock_16x16 deblocks an entire row that is 16 pixels high by the full width of the image.
+# Row has num16 16x16 blocks across
+# Beta goes from 0 to 64
+# tc goes from 0 to 24
+# setup[block_idx][0=vert,1=horz][0=first edge, 1=second edge][0=beta,1=tc][0..3=edge number]
+#   has 8 bytes per edge
+#   has 16 bytes per direction
+#   has 32 bytes per 16x16 block
+# hevc_deblock_16x16(uint8_t *img (r0), int stride (r1), int num16w (r2), uint8_t setup[num16][2][2][2][4](r3),int num16h(r4))
+hevc_deblock_16x16:
+  push r6-r15, lr
+  mov r9,r4
+  mov r4,r3
+  mov r13,r2
+  mov r2,r0
+  mov r10,r0
+  subscale4 r0,r1
+  mov r8,63
+  mov r6,-3
+  vmov H(zeros,0),0
+# r7 is number of blocks still to load
+# r0 is location of current block - 4 * stride
+# r1 is stride
+# r2 is location of current block
+# r3 is offset of start of block (actual edges start at H(16,16)+r3 for horizontal and H(16,0)+r3 for vertical
+# r4 is setup
+# r5 is for temporary calculations
+# r8 holds 63
+# r6 holds -3
+# r9 holds the number of 16 high rows to process
+# r10 holds the original img base
+# r11 returns 0 if no filtering was done on the edge
+# r12 saves a copy of this
+# r13 is copy of width
+
+process_row:
+  # First iteration does not do horizontal filtering on previous
+  mov r7, r13
+  mov r3,0
+  vldb H(12++,16)+r3,(r0 += r1) REP 4    # Load the current block
+  vldb H(16++,16)+r3,(r2 += r1) REP 16
+  vldb H(setup_input,0), (r4)  # We may wish to prefetch these
+  vstb H(zeros,0),(r4)
+  bl vert_filter
+  add r3,8
+  vadd H(setup_input,0),H(setup_input,8),0 # Rotate to second set of 8
+  bl vert_filter
+  sub r3,8
+  b start_deblock_loop
+deblock_loop:
+  # Middle iterations do vertical on current block and horizontal on preceding
+  vldb H(12++,16)+r3,(r0 += r1) REP 4  # load the current block
+  vldb H(16++,16)+r3,(r2 += r1) REP 16
+  vldb H(setup_input,0), (r4)
+  vstb H(zeros,0),(r4)
+  bl vert_filter
+  add r3,8
+  vadd H(setup_input,0),H(setup_input,8),0
+  bl vert_filter
+  sub r3,8
+  vldb H(setup_input,0), -16(r4)
+  vstb H(zeros,0),-16(r4)
+  bl horz_filter
+  mov r12,r11
+  add r3,8*64
+  vadd H(setup_input,0),H(setup_input,8),0
+  bl horz_filter
+  sub r3,8*64
+  addcmpbeq r12,0,0,skip_save_top
+  vstb H(12++,0)+r3,-16(r0 += r1) REP 4  # Save the deblocked pixels for the previous block
+skip_save_top:
+  vstb H(16++,0)+r3,-16(r2 += r1) REP 16
+start_deblock_loop:
+  # move onto next 16x16 (could do this with circular buffer support instead)
+  add r3,16
+  and r3,r8
+  add r4,32
+  # Perform loop counter operations (may work with an addcmpbgt as well?)
+  add r0,16
+  add r2,16
+  sub r7,1
+  cmp r7,0 # Are there still more blocks to load
+  bgt deblock_loop
+
+  # Final iteration needs to just do horizontal filtering
+  vldb H(setup_input,0), -16(r4)
+  vstb H(zeros,0),-16(r4)
+  bl horz_filter
+  mov r12,r11
+  add r3,8*64
+  vadd H(setup_input,0),H(setup_input,8),0
+  bl horz_filter
+  sub r3,64*8
+  addcmpbeq r12,0,0,skip_save_top2
+  vstb H(12++,0)+r3,-16(r0 += r1) REP 4  # Save the deblocked pixels for the previous block
+skip_save_top2:
+  vstb H(16++,0)+r3,-16(r2 += r1) REP 16
+
+# Now look to see if we should do another row
+  sub r9,1
+  cmp r9,0
+  bgt start_again
+  pop r6-r15, pc
+start_again:
+  # Need to sort out r0,r2 to point to next row down
+  addscale16 r10,r1
+  mov r2,r10
+  subscale4 r0,r2,r1
+  b process_row
+
+
+# At this stage H(16,16)+r3 points to the first pixel of the 16 high edge to be filtered
+# So we can reuse the code we move the parts to be filtered into HX(P0/P1/P2/P3/Q0/Q1/Q2/Q3,0) - we will perform a final saturation step on placing them back into the correct locations
+
+vert_filter:
+  push lr
+
+  vmov HX(P3,0), V(16,12)+r3
+  vmov HX(P2,0), V(16,13)+r3
+  vmov HX(P1,0), V(16,14)+r3
+  vmov HX(P0,0), V(16,15)+r3
+  vmov HX(Q0,0), V(16,16)+r3
+  vmov HX(Q1,0), V(16,17)+r3
+  vmov HX(Q2,0), V(16,18)+r3
+  vmov HX(Q3,0), V(16,19)+r3
+
+  bl do_luma_filter
+
+  vadds V(16,13)+r3, HX(P2,0), 0
+  vadds V(16,14)+r3, HX(P1,0), 0
+  vadds V(16,15)+r3, HX(P0,0), 0
+  # P3 and Q3 never change so don't bother saving back
+  vadds V(16,16)+r3, HX(Q0,0), 0
+  vadds V(16,17)+r3, HX(Q1,0), 0
+  vadds V(16,18)+r3, HX(Q2,0), 0
+
+  pop pc
+
+# Filter edge at H(16,0)+r3
+horz_filter:
+  push lr
+
+  vmov HX(P3,0), H(12,0)+r3
+  vmov HX(P2,0), H(13,0)+r3
+  vmov HX(P1,0), H(14,0)+r3
+  vmov HX(P0,0), H(15,0)+r3
+  vmov HX(Q0,0), H(16,0)+r3
+  vmov HX(Q1,0), H(17,0)+r3
+  vmov HX(Q2,0), H(18,0)+r3
+  vmov HX(Q3,0), H(19,0)+r3
+
+  bl do_luma_filter
+
+  vadds H(13,0)+r3, HX(P2,0), 0
+  vadds H(14,0)+r3, HX(P1,0), 0
+  vadds H(15,0)+r3, HX(P0,0), 0
+  # P3 and Q3 never change so don't bother saving back
+  vadds H(16,0)+r3, HX(Q0,0), 0
+  vadds H(17,0)+r3, HX(Q1,0), 0
+  vadds H(18,0)+r3, HX(Q2,0), 0
+
+  pop pc
+
+# r4 points to array of beta/tc for each 4 length edge
+do_luma_filter:
+  valtl H(setup,0),H(setup_input,0),H(setup_input,0) # b*8tc*8
+  valtl HX(beta,0),H(setup,0),H(setup,0)
+  valtu HX(tc,0),H(setup,0),H(setup,0)
+  vmul HX(tc25,0), HX(tc,0), 5
+  vadd HX(tc25,0),HX(tc25,0), 1
+  vasr HX(tc25,0), HX(tc25,0), 1
+
+  # Compute decision
+  vadd HX(dp,0),HX(P1,0),HX(P1,0) # 2*P1
+  vsub HX(dp,0),HX(P2,0),HX(dp,0) # P2-2*P1
+  vadd HX(dp,0),HX(dp,0),HX(P0,0) # P2-2*P1+P0
+  vdist HX(dp,0),HX(dp,0),0 # abs(P2-2*P1+P0) # dp0
+
+  vadd HX(dq,0),HX(Q1,0),HX(Q1,0) # 2*Q1
+  vsub HX(dq,0),HX(Q2,0),HX(dq,0) # Q2-2*Q1
+  vadd HX(dq,0),HX(dq,0),HX(Q0,0) # Q2-2*Q1+Q0
+  vdist HX(dq,0),HX(dq,0),0 # abs(Q2-2*Q1+Q0) # dq0
+
+  vadd HX(d,0), HX(dp,0), HX(dq,0)
+  vasr HX(beta2,0),HX(beta,0),2
+  vasr HX(beta3,0),HX(beta,0),3
+
+  # Compute flags that are negative if all conditions pass
+  vdist HX(decision,0), HX(P0,0), HX(P3,0) CLRA SACC
+  vdist HX(decision,0), HX(Q0,0), HX(Q3,0) SACC
+  vsub HX(decision,0), HX(decision,0), HX(beta3,0) SETF
+
+  vdist HX(decision,0), HX(P0,0), HX(Q0,0) IFN
+  vsub HX(decision,0), HX(decision,0), HX(tc25,0) IFN SETF
+  vadd HX(decision,0), HX(d,0), HX(d,0) IFN
+  vsub HX(decision,0), HX(decision,0), HX(beta2,0) IFN SETF
+  vmov HX(decision,0), 1 IFNN
+  vadd H(decision,0),H(decision,3),0 IFN
+  vadd H(decision,16),H(decision,19),0 IFN
+  vmov -,HX(decision,0) SETF   # N marks strong filter
+  vmov HX(decision,0), 1 IFNN  # NN marks normal filter
+
+  vadd HX(do_filter,0), HX(d,3), HX(d,0)
+  vsub HX(do_filter,0), HX(do_filter,0), HX(beta,0) SETF # IFNN means no filter
+  vmov HX(decision,0),0 IFNN # Z marks no filter
+
+  # Expand out decision (currently valid one every 4 pixels)  0...1...2...3
+  # First extract out even terms
+  vodd HX(decision,0),HX(decision,0),HX(decision,0)  # 0.1.2.3
+  vodd HX(decision,0),HX(decision,0),HX(decision,0)  # 0123
+  # Now expand back
+  valtl HX(decision,0),HX(decision,0),HX(decision,0) # 00112233
+  valtl HX(decision,0),HX(decision,0),HX(decision,0) SETF # 0000111122223333
+
+  # HX(decision,0) is negative if want strong filtering, 1 if want normal filtering, 0 if want no filtering
+
+  # Do a quick check to see if there is anything to do
+  mov r11, 0 # Signal no filtering
+  vmov -,1 IFNZ SUMS r5
+  cmp r5,0
+  beq filtering_done
+  mov r11, 1 # Signal some filtering
+  # And whether there is any strong filtering
+  vmov -,1 IFN SUMS r5
+  cmp r5,0
+  beq normal_filtering
+
+  ##############################################################################
+  # Strong filtering - could maybe fast case if all have same sign? (especially if all disabled!)
+  vshl HX(tc2,0), HX(tc,0), 1  # Note that in normal filtering tx2 is tc/2, while here it is tc*2
+
+  # Take a copy of the original pixels for use in decision calculation
+  vmov HX(P0,32),HX(P0,0)
+  vmov HX(Q0,32),HX(Q0,0)
+  vmov HX(P1,32),HX(P1,0)
+  vmov HX(Q1,32),HX(Q1,0)
+  vmov HX(P2,32),HX(P2,0)
+  vmov HX(Q2,32),HX(Q2,0)
+
+  vadd -,HX(P2,32),4 CLRA SACC
+  vshl -,HX(P1,32),1 SACC
+  vshl -,HX(P0,32),1 SACC
+  vshl -,HX(Q0,32),1 SACC
+  vshl HX(delta,0),HX(Q1,32),0 SACC
+  vasr HX(delta,0),HX(delta,0), 3
+  vsub HX(delta,0),HX(delta,0),HX(P0,32)
+  vclamps HX(delta,0), HX(delta,0), HX(tc2,0)
+  vadd HX(P0,0),HX(P0,32),HX(delta,0) IFN
+
+  vadd -,HX(P2,32),2 CLRA SACC
+  vadd -,HX(P1,32),HX(P0,32) SACC
+  vshl HX(delta,0),HX(Q0,32),0 SACC
+  vasr HX(delta,0),HX(delta,0), 2
+  vsub HX(delta,0),HX(delta,0),HX(P1,32)
+  vclamps HX(delta,0), HX(delta,0), HX(tc2,0)
+  vadd HX(P1,0),HX(P1,32),HX(delta,0) IFN
+
+  vadd -,HX(Q0,32),4 CLRA SACC
+  vadd -,HX(P1,32),HX(P0,32) SACC
+  vmul -,HX(P2,32),3 SACC
+  vshl HX(delta,0),HX(P3,0),1 SACC # Note that we have not made a copy of P3, so using P3,0 is correct
+  vasr HX(delta,0),HX(delta,0), 3
+  vsub HX(delta,0),HX(delta,0),HX(P2,32)
+  vclamps HX(delta,0), HX(delta,0), HX(tc2,0)
+  vadd HX(P2,0),HX(P2,32),HX(delta,0) IFN
+  #vmov HX(P2,0),3 IFN
+
+  # Now reverse all P/Qs
+
+  vadd -,HX(Q2,32),4 CLRA SACC
+  vshl -,HX(Q1,32),1 SACC
+  vshl -,HX(Q0,32),1 SACC
+  vshl -,HX(P0,32),1 SACC
+  vshl HX(delta,0),HX(P1,32),0 SACC
+  vasr HX(delta,0),HX(delta,0), 3
+  vsub HX(delta,0),HX(delta,0),HX(Q0,32)
+  vclamps HX(delta,0), HX(delta,0), HX(tc2,0)
+  vadd HX(Q0,0),HX(Q0,32),HX(delta,0) IFN
+
+  vadd -,HX(Q2,32),2 CLRA SACC
+  vadd -,HX(Q1,32),HX(Q0,32) SACC
+  vshl HX(delta,0),HX(P0,32),0 SACC
+  vasr HX(delta,0),HX(delta,0), 2
+  vsub HX(delta,0),HX(delta,0),HX(Q1,32)
+  vclamps HX(delta,0), HX(delta,0), HX(tc2,0)
+  vadd HX(Q1,0),HX(Q1,32),HX(delta,0) IFN
+
+  vadd -,HX(P0,32),4 CLRA SACC
+  vadd -,HX(Q1,32),HX(Q0,32) SACC
+  vmul -,HX(Q2,32),3 SACC
+  vshl HX(delta,0),HX(Q3,0),1 SACC # Note that we have not made a copy of Q3, so using Q3,0 is correct
+  vasr HX(delta,0),HX(delta,0), 3
+  vsub HX(delta,0),HX(delta,0),HX(Q2,32)
+  vclamps HX(delta,0), HX(delta,0), HX(tc2,0)
+  vadd HX(Q2,0),HX(Q2,32),HX(delta,0) IFN
+
+  ##############################################################################
+  # Normal filtering
+normal_filtering:
+  # Invert the decision flags
+  # make instruction more complicated as assembler has error and loses SETF
+  vrsub HX(tc10,0), HX(decision,0), 0 SETF # IFN means normal filtering
+  vmov  -, HX(tc10,0) SETF # IFN means normal filtering
+
+  vmov -,1 IFN SUMS r5
+  cmp r5,0
+  beq filtering_done
+
+  vasr HX(tc2,0), HX(tc,0), 1
+  vmul HX(tc10,0), HX(tc,0), 10
+
+  vasr HX(thresh,0), HX(beta,0), 1
+  vadd HX(thresh,0), HX(thresh,0), HX(beta,0)
+  vasr HX(thresh,0), HX(thresh,0), 3 CLRA SACC
+
+  vadd HX(ptest,0),HX(dp,3),HX(dp,0)
+  vsub HX(ptest,0),HX(ptest,0),HX(thresh,0) # ptest is negative if we need to do the P2 pixel
+  vadd HX(qtest,0),HX(dq,3),HX(dq,0)
+  vsub HX(qtest,0),HX(qtest,0),HX(thresh,0) # qtest is negative if we need to do the Q2 pixel
+  # Expand ptest and qtest together
+  vodd HX(pqtest,0),HX(ptest,0),HX(qtest,0)  # p.p.p.p.q.q.q.q
+  vodd HX(pqtest,0),HX(pqtest,0),HX(pqtest,0) # ppppqqqq........
+  valtl HX(pqtest,0),HX(pqtest,0),HX(pqtest,0) # ppppppppqqqqqqqq
+  valtl HX(ptest,0),HX(pqtest,0),HX(pqtest,0)
+  valtu HX(qtest,0),HX(pqtest,0),HX(pqtest,0)
+
+  vsub HX(delta0,0), HX(Q0,0), HX(P0,0)
+  vsub HX(delta1,0), HX(Q1,0), HX(P1,0)
+  vmov -,8 CLRA SACC
+  vmul -,HX(delta0,0), 9 SACC
+  vmul HX(delta0,0),HX(delta1,0), r6 SACC
+  vasr HX(delta0,0), HX(delta0,0), 4
+  vdist HX(deltatest,0), HX(delta0,0), 0
+  vsub HX(deltatest,0), HX(deltatest,0), HX(tc10,0) IFN SETF # negative if still need to do something
+  vmov HX(deltatest,0), 0 IFNN # clear if no need to do anything so we can reload flags later
+
+  vclamps HX(delta0,0), HX(delta0,0), HX(tc,0)
+
+  vadd HX(deltap1,0), HX(P2,0), HX(P0,0)
+  vadd HX(deltap1,0), HX(deltap1,0), 1
+  vasr HX(deltap1,0), HX(deltap1,0), 1 CLRA SACC
+  vsub HX(deltap1,0), HX(delta0,0), HX(P1,0) SACC
+  vasr HX(deltap1,0), HX(deltap1,0), 1
+  vclamps HX(deltap1,0), HX(deltap1,0), HX(tc2,0)
+
+  vadd HX(deltaq1,0), HX(Q2,0), HX(Q0,0)
+  vadd HX(deltaq1,0), HX(deltaq1,0), 1
+  vasr HX(deltaq1,0), HX(deltaq1,0), 1 CLRA SACC
+  vadd HX(deltaq1,0), HX(delta0,0), HX(Q1,0)
+  vrsub -, HX(delta0,0), 0 SACC
+  vrsub HX(deltaq1,0), HX(Q1,0), 0 SACC
+  vasr HX(deltaq1,0), HX(deltaq1,0), 1
+  vclamps HX(deltaq1,0), HX(deltaq1,0), HX(tc2,0)
+
+  vadds HX(P0,0), HX(P0,0), HX(delta0,0) IFN
+  vsubs HX(Q0,0), HX(Q0,0), HX(delta0,0) IFN
+
+  vmov -,HX(ptest,0) IFN SETF # Negative if need to do p1
+  vadds HX(P1,0), HX(P1,0), HX(deltap1,0) IFN
+
+  vmov -,HX(deltatest,0) SETF
+  vmov -,HX(qtest,0) IFN SETF # Negative if need to do q1
+  vadds HX(Q1,0), HX(Q1,0), HX(deltaq1,0) IFN
+
+  #vmov HX(P2,0),1 IFN
+
+filtering_done:
+  b lr
+
+
+hevc_uv_deblock_16x16:
+  push r6-r15, lr
+  mov r14,0
+  b hevc_uv_start
+hevc_uv_deblock_16x16_with_clear:
+  push r6-r15, lr
+  mov r14,1
+  b hevc_uv_start
+
+hevc_uv_start:
+  mov r9,r4
+  mov r4,r3
+  mov r13,r2
+  mov r2,r0
+  mov r10,r0
+  subscale4 r0,r1
+  mov r8,63
+  mov r6,-3
+  vmov H(zeros,0),0
+# r7 is number of blocks still to load
+# r0 is location of current block - 4 * stride
+# r1 is stride
+# r2 is location of current block
+# r3 is offset of start of block (actual edges start at H(16,16)+r3 for horizontal and H(16,0)+r3 for vertical
+# r4 is setup
+# r5 is for temporary calculations
+# r8 holds 63
+# r6 holds -3
+# r9 holds the number of 16 high rows to process
+# r10 holds the original img base
+# r11 returns 0 if no filtering was done on the edge
+# r12 saves a copy of this
+# r13 is copy of width
+# r14 is 1 if we should clear the old contents, or 0 if not
+
+uv_process_row:
+  # First iteration does not do horizontal filtering on previous
+  mov r7, r13
+  mov r3,0
+  vldb H(12++,16)+r3,(r0 += r1) REP 4    # Load the current block
+  vldb H(16++,16)+r3,(r2 += r1) REP 16
+  vldb H(setup_input,0), (r4)  # We may wish to prefetch these
+  cmp r14,1
+  bne uv_skip0
+  vstb H(zeros,0),(r4)
+uv_skip0:
+  bl uv_vert_filter
+  add r3,8
+  vadd H(setup_input,0),H(setup_input,8),0 # Rotate to second set of 8
+  bl uv_vert_filter
+  sub r3,8
+  b uv_start_deblock_loop
+uv_deblock_loop:
+  # Middle iterations do vertical on current block and horizontal on preceding
+  vldb H(12++,16)+r3,(r0 += r1) REP 4  # load the current block
+  vldb H(16++,16)+r3,(r2 += r1) REP 16
+  vldb H(setup_input,0), (r4)
+  cmp r14,1
+  bne uv_skip1
+  vstb H(zeros,0),(r4)
+uv_skip1:
+  bl uv_vert_filter
+  add r3,8
+  vadd H(setup_input,0),H(setup_input,8),0
+  bl uv_vert_filter
+  sub r3,8
+  vldb H(setup_input,0), -16(r4)
+  cmp r14,1
+  bne uv_skip3
+  vstb H(zeros,0),-16(r4)
+uv_skip3:
+  bl uv_horz_filter
+  mov r12,r11
+  add r3,8*64
+  vadd H(setup_input,0),H(setup_input,8),0
+  bl uv_horz_filter
+  sub r3,8*64
+  addcmpbeq r12,0,0,uv_skip_save_top
+  vstb H(12++,0)+r3,-16(r0 += r1) REP 4  # Save the deblocked pixels for the previous block
+uv_skip_save_top:
+  vstb H(16++,0)+r3,-16(r2 += r1) REP 16
+uv_start_deblock_loop:
+  # move onto next 16x16 (could do this with circular buffer support instead)
+  add r3,16
+  and r3,r8
+  add r4,32
+  # Perform loop counter operations (may work with an addcmpbgt as well?)
+  add r0,16
+  add r2,16
+  sub r7,1
+  cmp r7,0 # Are there still more blocks to load
+  bgt uv_deblock_loop
+
+  # Final iteration needs to just do horizontal filtering
+  vldb H(setup_input,0), -16(r4)
+  cmp r14,1
+  bne uv_skip2
+  vstb H(zeros,0),-16(r4)
+uv_skip2:
+  bl uv_horz_filter
+  mov r12,r11
+  add r3,8*64
+  vadd H(setup_input,0),H(setup_input,8),0
+  bl uv_horz_filter
+  sub r3,64*8
+  addcmpbeq r12,0,0,uv_skip_save_top2
+  vstb H(12++,0)+r3,-16(r0 += r1) REP 4  # Save the deblocked pixels for the previous block
+uv_skip_save_top2:
+  vstb H(16++,0)+r3,-16(r2 += r1) REP 16
+
+# Now look to see if we should do another row
+  sub r9,1
+  cmp r9,0
+  bgt uv_start_again
+  pop r6-r15, pc
+uv_start_again:
+  # Need to sort out r0,r2 to point to next row down
+  addscale16 r10,r1
+  mov r2,r10
+  subscale4 r0,r2,r1
+  b uv_process_row
+
+
+# At this stage H(16,16)+r3 points to the first pixel of the 16 high edge to be filtered
+# So we can reuse the code we move the parts to be filtered into HX(P0/P1/P2/P3/Q0/Q1/Q2/Q3,0) - we will perform a final saturation step on placing them back into the correct locations
+
+uv_vert_filter:
+  push lr
+
+  vmov HX(P1,0), V(16,14)+r3
+  vmov HX(P0,0), V(16,15)+r3
+  vmov HX(Q0,0), V(16,16)+r3
+  vmov HX(Q1,0), V(16,17)+r3
+
+  bl do_chroma_filter
+
+  vadds V(16,15)+r3, HX(P0,0), 0
+  vadds V(16,16)+r3, HX(Q0,0), 0
+
+  pop pc
+
+# Filter edge at H(16,0)+r3
+uv_horz_filter:
+  push lr
+
+  vmov HX(P1,0), H(14,0)+r3
+  vmov HX(P0,0), H(15,0)+r3
+  vmov HX(Q0,0), H(16,0)+r3
+  vmov HX(Q1,0), H(17,0)+r3
+
+  bl do_chroma_filter
+
+  vadds H(15,0)+r3, HX(P0,0), 0
+  # P3 and Q3 never change so don't bother saving back
+  vadds H(16,0)+r3, HX(Q0,0), 0
+
+  pop pc
+
+# r4 points to array of beta/tc for each 4 length edge
+do_chroma_filter:
+  valtl H(setup,0),H(setup_input,0),H(setup_input,0) # tc*8
+  valtl HX(tc,0),H(setup,0),H(setup,0)
+
+  vsub HX(delta,0),HX(Q0,0),HX(P0,0)
+  vshl HX(delta,0),HX(delta,0),2 CLRA SACC
+  vsub -,HX(P1,0),HX(Q1,0) SACC
+  vmov HX(delta,0),4 SACC
+  vasr HX(delta,0),HX(delta,0),3
+  vclamps HX(delta,0), HX(delta,0), HX(tc,0)
+  vadd HX(P0,0),HX(P0,0),HX(delta,0)
+  vsub HX(Q0,0),HX(Q0,0),HX(delta,0)
+  b lr
+
+# r0 = list
+# r1 = number
+hevc_run_command_list:
+  push r6-r7, lr
+  mov r6, r0
+  mov r7, r1
+loop_cmds:
+  ld r0,(r6) # How to encode r6++?
+  add r6,4
+  ld r1,(r6)
+  add r6,4
+  ld r2,(r6)
+  add r6,4
+  ld r3,(r6)
+  add r6,4
+  ld r4,(r6)
+  add r6,4
+  ld r5,(r6)
+  add r6,4
+  bl hevc_trans_16x16
+  sub r7,1
+  cmp r7,0
+  bgt loop_cmds
+
+  pop r6-r7, pc
diff -Naur ffmpeg-3.2.4/libavcodec/rpi_mailbox.c ffmpeg-3.2.4.patch/libavcodec/rpi_mailbox.c
--- ffmpeg-3.2.4/libavcodec/rpi_mailbox.c	1970-01-01 01:00:00.000000000 +0100
+++ ffmpeg-3.2.4.patch/libavcodec/rpi_mailbox.c	2017-05-28 20:42:45.750088709 +0200
@@ -0,0 +1,149 @@
+/*
+Copyright (c) 2012, Broadcom Europe Ltd.
+All rights reserved.
+
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are met:
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in the
+      documentation and/or other materials provided with the distribution.
+    * Neither the name of the copyright holder nor the
+      names of its contributors may be used to endorse or promote products
+      derived from this software without specific prior written permission.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
+ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY
+DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+*/
+
+#ifdef RPI
+
+#include <stdio.h>
+#include <string.h>
+#include <stdlib.h>
+#include <fcntl.h>
+#include <unistd.h>
+#include <assert.h>
+#include <stdint.h>
+#include <sys/ioctl.h>
+
+#include <linux/ioctl.h>
+
+#define MAJOR_NUM 100
+#define IOCTL_MBOX_PROPERTY _IOWR(MAJOR_NUM, 0, char *)
+#define DEVICE_FILE_NAME "/dev/vcio"
+
+#include "rpi_mailbox.h"
+//#include <interface/vctypes/vc_image_structs.h>
+
+/*
+ * use ioctl to send mbox property message
+ */
+
+static int mbox_property(int file_desc, void *buf)
+{
+   int ret_val = ioctl(file_desc, IOCTL_MBOX_PROPERTY, buf);
+
+   if (ret_val < 0) {
+      printf("ioctl_set_msg failed:%d\n", ret_val);
+   }
+
+#ifdef DEBUG
+   unsigned *p = buf; int i; unsigned size = *(unsigned *)buf;
+   for (i=0; i<size/4; i++)
+      printf("%04x: 0x%08x\n", i*sizeof *p, p[i]);
+#endif
+   return ret_val;
+}
+
+unsigned mbox_mem_lock(int file_desc, unsigned handle)
+{
+   int i=0;
+   unsigned p[32];
+   p[i++] = 0; // size
+   p[i++] = 0x00000000; // process request
+
+   p[i++] = 0x3000d; // (the tag id)
+   p[i++] = 4; // (size of the buffer)
+   p[i++] = 4; // (size of the data)
+   p[i++] = handle;
+
+   p[i++] = 0x00000000; // end tag
+   p[0] = i*sizeof *p; // actual size
+
+   mbox_property(file_desc, p);
+   return p[5];
+}
+
+unsigned mbox_mem_unlock(int file_desc, unsigned handle)
+{
+   int i=0;
+   unsigned p[32];
+   p[i++] = 0; // size
+   p[i++] = 0x00000000; // process request
+
+   p[i++] = 0x3000e; // (the tag id)
+   p[i++] = 4; // (size of the buffer)
+   p[i++] = 4; // (size of the data)
+   p[i++] = handle;
+
+   p[i++] = 0x00000000; // end tag
+   p[0] = i*sizeof *p; // actual size
+
+   mbox_property(file_desc, p);
+   return p[5];
+}
+
+#define GET_VCIMAGE_PARAMS 0x30044
+
+int mbox_get_image_params(int fd, VC_IMAGE_T * img)
+{
+    uint32_t buf[sizeof(*img) / sizeof(uint32_t) + 32];
+    uint32_t * p = buf;
+    void * rimg;
+    int rv;
+
+    *p++ = 0; // size
+    *p++ = 0; // process request
+    *p++ = GET_VCIMAGE_PARAMS;
+    *p++ = sizeof(*img);
+    *p++ = sizeof(*img);
+    rimg = p;
+    memcpy(p, img, sizeof(*img));
+    p += sizeof(*img) / sizeof(*p);
+    *p++ = 0;  // End tag
+    buf[0] = (p - buf) * sizeof(*p);
+
+    rv = mbox_property(fd, buf);
+    memcpy(img, rimg, sizeof(*img));
+
+    return rv;
+}
+
+int mbox_open() {
+   int file_desc;
+
+   // open a char device file used for communicating with kernel mbox driver
+   file_desc = open(DEVICE_FILE_NAME, 0);
+   if (file_desc < 0) {
+      printf("Can't open device file: %s\n", DEVICE_FILE_NAME);
+      printf("Try creating a device file with: sudo mknod %s c %d 0\n", DEVICE_FILE_NAME, MAJOR_NUM);
+   }
+   return file_desc;
+}
+
+void mbox_close(int file_desc) {
+  close(file_desc);
+}
+
+#endif
+
diff -Naur ffmpeg-3.2.4/libavcodec/rpi_mailbox.h ffmpeg-3.2.4.patch/libavcodec/rpi_mailbox.h
--- ffmpeg-3.2.4/libavcodec/rpi_mailbox.h	1970-01-01 01:00:00.000000000 +0100
+++ ffmpeg-3.2.4.patch/libavcodec/rpi_mailbox.h	2017-05-28 20:42:45.750088709 +0200
@@ -0,0 +1,58 @@
+#ifndef RPI_MAILBOX_H
+#define RPI_MAILBOX_H
+
+/* The image structure. */
+typedef struct vc_image_extra_uv_s {
+  void *u, *v;
+  int vpitch;
+} VC_IMAGE_EXTRA_UV_T;
+
+typedef union {
+    VC_IMAGE_EXTRA_UV_T uv;
+//  VC_IMAGE_EXTRA_RGBA_T rgba;
+//  VC_IMAGE_EXTRA_PAL_T pal;
+//  VC_IMAGE_EXTRA_TF_T tf;
+//  VC_IMAGE_EXTRA_BAYER_T bayer;
+//  VC_IMAGE_EXTRA_MSBAYER_T msbayer;
+//  VC_IMAGE_EXTRA_CODEC_T codec;
+//  VC_IMAGE_EXTRA_OPENGL_T opengl;
+} VC_IMAGE_EXTRA_T;
+
+
+typedef struct VC_IMAGE_T {
+  unsigned short                  type;           /* should restrict to 16 bits */
+  unsigned short                  info;           /* format-specific info; zero for VC02 behaviour */
+  unsigned short                  width;          /* width in pixels */
+  unsigned short                  height;         /* height in pixels */
+  int                             pitch;          /* pitch of image_data array in bytes */
+  int                             size;           /* number of bytes available in image_data array */
+  void                           *image_data;     /* pixel data */
+  VC_IMAGE_EXTRA_T                extra;          /* extra data like palette pointer */
+  void                           *metadata;       /* metadata header for the image */
+  void                           *pool_object;    /* nonNULL if image was allocated from a vc_pool */
+  int                             mem_handle;     /* the mem handle for relocatable memory storage */
+  int                             metadata_size;  /* size of metadata of each channel in bytes */
+  int                             channel_offset; /* offset of consecutive channels in bytes */
+  uint32_t                        video_timestamp;/* 90000 Hz RTP times domain - derived from audio timestamp */
+  uint8_t                         num_channels;   /* number of channels (2 for stereo) */
+  uint8_t                         current_channel;/* the channel this header is currently pointing to */
+  uint8_t                         linked_multichann_flag;/* Indicate the header has the linked-multichannel structure*/
+  uint8_t                         is_channel_linked;     /* Track if the above structure is been used to link the header
+                                                            into a linked-mulitchannel image */
+  uint8_t                         channel_index;         /* index of the channel this header represents while
+                                                            it is being linked. */
+  uint8_t                         _dummy[3];      /* pad struct to 64 bytes */
+} VC_IMAGE_T;
+
+typedef int vc_image_t_size_check[(sizeof(VC_IMAGE_T) == 64) * 2 - 1];
+
+
+extern int mbox_open(void);
+extern void mbox_close(int file_desc);
+
+extern unsigned mbox_mem_lock(int file_desc, unsigned handle);
+extern unsigned mbox_mem_unlock(int file_desc, unsigned handle);
+
+int mbox_get_image_params(int fd, VC_IMAGE_T * img);
+
+#endif
diff -Naur ffmpeg-3.2.4/libavcodec/rpi_qpu.c ffmpeg-3.2.4.patch/libavcodec/rpi_qpu.c
--- ffmpeg-3.2.4/libavcodec/rpi_qpu.c	1970-01-01 01:00:00.000000000 +0100
+++ ffmpeg-3.2.4.patch/libavcodec/rpi_qpu.c	2017-05-28 20:42:45.751088712 +0200
@@ -0,0 +1,902 @@
+#ifdef RPI
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+#include <stddef.h>
+#include <stdint.h>
+#include "libavutil/avassert.h"
+
+#include "config.h"
+
+#include <pthread.h>
+#include <time.h>
+
+#include <interface/vcsm/user-vcsm.h>
+
+#include "rpi_mailbox.h"
+#include "rpi_qpu.h"
+#include "rpi_shader.h"
+#include "rpi_hevc_transform.h"
+#include "rpi_zc.h"
+
+#pragma GCC diagnostic push
+// Many many redundant decls in the header files
+#pragma GCC diagnostic ignored "-Wredundant-decls"
+#include "interface/vmcs_host/vc_vchi_gpuserv.h"
+#pragma GCC diagnostic pop
+
+// Trace time spent waiting for GPU (VPU/QPU) (1=Yes, 0=No)
+#define RPI_TRACE_TIME_VPU_QPU_WAIT     0
+
+// Add profile flags to all QPU requests - generates output in "vcdbg log msg"
+// Beware this is expensive and will probably throw off all other timing by >10%
+#define RPI_TRACE_QPU_PROFILE_ALL       0
+
+// QPU "noflush" flags
+// a mixture of flushing & profiling
+
+#define QPU_FLAGS_NO_FLUSH_VPU          1       // If unset VPU cache will be flushed
+#define QPU_FLAGS_PROF_CLEAR_AND_ENABLE 2       // Clear & Enable detailed QPU profiling registers
+#define QPU_FLAGS_PROF_OUTPUT_COUNTS    4       // Print the results
+#define QPU_FLAGS_OUTPUT_QPU_TIMES      8       // Print QPU times - independant of the profiling
+#define QPU_FLAGS_NO_FLUSH_QPU          16      // If unset flush QPU caches & TMUs (uniforms always flushed)
+
+// On Pi2 there is no way to access the VPU L2 cache
+// GPU_MEM_FLG should be 4 for uncached memory.  (Or C for alias to allocate in the VPU L2 cache)
+// However, if using VCSM allocated buffers, need to use C at the moment because VCSM does not allocate uncached memory correctly
+// The QPU crashes if we mix L2 cached and L2 uncached accesses due to a HW bug.
+#define GPU_MEM_FLG 0x4
+// GPU_MEM_MAP is meaningless on the Pi2 and should be left at 0  (On Pi1 it allows ARM to access VPU L2 cache)
+#define GPU_MEM_MAP 0x0
+
+#define vcos_verify_ge0(x) ((x)>=0)
+
+/*static const unsigned code[] =
+{
+  #include "rpi_shader.hex"
+};*/
+
+// Size in 32bit words
+#define QPU_CODE_SIZE 2048
+#define VPU_CODE_SIZE 2048
+
+const short rpi_transMatrix2even[32][16] = { // Even rows first
+{64,  64,  64,  64,  64,  64,  64,  64,  64,  64,  64,  64,  64,  64,  64,  64},
+{90,  87,  80,  70,  57,  43,  25,   9,  -9, -25, -43, -57, -70, -80, -87, -90},
+{89,  75,  50,  18, -18, -50, -75, -89, -89, -75, -50, -18,  18,  50,  75,  89},
+{87,  57,   9, -43, -80, -90, -70, -25,  25,  70,  90,  80,  43,  -9, -57, -87},
+{83,  36, -36, -83, -83, -36,  36,  83,  83,  36, -36, -83, -83, -36,  36,  83},
+{80,   9, -70, -87, -25,  57,  90,  43, -43, -90, -57,  25,  87,  70,  -9, -80},
+{75, -18, -89, -50,  50,  89,  18, -75, -75,  18,  89,  50, -50, -89, -18,  75},
+{70, -43, -87,   9,  90,  25, -80, -57,  57,  80, -25, -90,  -9,  87,  43, -70},
+{64, -64, -64,  64,  64, -64, -64,  64,  64, -64, -64,  64,  64, -64, -64,  64},
+{57, -80, -25,  90,  -9, -87,  43,  70, -70, -43,  87,   9, -90,  25,  80, -57},
+{50, -89,  18,  75, -75, -18,  89, -50, -50,  89, -18, -75,  75,  18, -89,  50},
+{43, -90,  57,  25, -87,  70,   9, -80,  80,  -9, -70,  87, -25, -57,  90, -43},
+{36, -83,  83, -36, -36,  83, -83,  36,  36, -83,  83, -36, -36,  83, -83,  36},
+{25, -70,  90, -80,  43,   9, -57,  87, -87,  57,  -9, -43,  80, -90,  70, -25},
+{18, -50,  75, -89,  89, -75,  50, -18, -18,  50, -75,  89, -89,  75, -50,  18},
+{ 9, -25,  43, -57,  70, -80,  87, -90,  90, -87,  80, -70,  57, -43,  25,  -9},
+// Odd rows
+{90,  90,  88,  85,  82,  78,  73,  67,  61,  54,  46,  38,  31,  22,  13,   4},
+{90,  82,  67,  46,  22,  -4, -31, -54, -73, -85, -90, -88, -78, -61, -38, -13},
+{88,  67,  31, -13, -54, -82, -90, -78, -46,  -4,  38,  73,  90,  85,  61,  22},
+{85,  46, -13, -67, -90, -73, -22,  38,  82,  88,  54,  -4, -61, -90, -78, -31},
+{82,  22, -54, -90, -61,  13,  78,  85,  31, -46, -90, -67,   4,  73,  88,  38},
+{78,  -4, -82, -73,  13,  85,  67, -22, -88, -61,  31,  90,  54, -38, -90, -46},
+{73, -31, -90, -22,  78,  67, -38, -90, -13,  82,  61, -46, -88,  -4,  85,  54},
+{67, -54, -78,  38,  85, -22, -90,   4,  90,  13, -88, -31,  82,  46, -73, -61},
+{61, -73, -46,  82,  31, -88, -13,  90,  -4, -90,  22,  85, -38, -78,  54,  67},
+{54, -85,  -4,  88, -46, -61,  82,  13, -90,  38,  67, -78, -22,  90, -31, -73},
+{46, -90,  38,  54, -90,  31,  61, -88,  22,  67, -85,  13,  73, -82,   4,  78},
+{38, -88,  73,  -4, -67,  90, -46, -31,  85, -78,  13,  61, -90,  54,  22, -82},
+{31, -78,  90, -61,   4,  54, -88,  82, -38, -22,  73, -90,  67, -13, -46,  85},
+{22, -61,  85, -90,  73, -38,  -4,  46, -78,  90, -82,  54, -13, -31,  67, -88},
+{13, -38,  61, -78,  88, -90,  85, -73,  54, -31,   4,  22, -46,  67, -82,  90},
+{ 4, -13,  22, -31,  38, -46,  54, -61,  67, -73,  78, -82,  85, -88,  90, -90}
+};
+
+// Code/constants on GPU
+struct GPU
+{
+  unsigned int qpu_code[QPU_CODE_SIZE];
+  unsigned int vpu_code[VPU_CODE_SIZE];
+  short transMatrix2even[16*16*2];
+};
+
+#define CFE_ENTS_PER_A 8
+// If we have a sliced frame 2048 wide @ 64 per slice then there are 32 slices
+// in a line & we want to flush luma + chroma + a couple of bits so ents ~ 70
+// allow 128
+#define CFE_ENT_COUNT  128
+#define CFE_A_COUNT    (CFE_ENT_COUNT / CFE_ENTS_PER_A)
+
+struct rpi_cache_flush_env_s {
+    unsigned int n;
+    struct vcsm_user_clean_invalid_s a[CFE_A_COUNT];
+};
+
+#define WAIT_COUNT_MAX 16
+
+typedef struct trace_time_one_s
+{
+  int count;
+  int64_t start[WAIT_COUNT_MAX];
+  int64_t total[WAIT_COUNT_MAX];
+} trace_time_one_t;
+
+typedef struct trace_time_wait_s
+{
+  unsigned int jcount;
+  int64_t start0;
+  int64_t last_update;
+  trace_time_one_t active;
+  trace_time_one_t wait;
+} trace_time_wait_t;
+
+typedef struct vq_wait_s
+{
+  sem_t sem;
+  unsigned int cost;
+  struct vq_wait_s * next;
+} vq_wait_t;
+
+#define VQ_WAIT_POOL_SIZE 16
+typedef struct vq_wait_pool_s
+{
+  vq_wait_t * head;
+  vq_wait_t pool[VQ_WAIT_POOL_SIZE];
+} vq_wait_pool_t;
+
+static void vq_wait_pool_init(vq_wait_pool_t * const pool);
+static void vq_wait_pool_deinit(vq_wait_pool_t * const pool);
+
+typedef struct gpu_env_s
+{
+  int open_count;
+  int init_count;
+  int mb;
+  unsigned int current_load;
+  GPU_MEM_PTR_T code_gm_ptr;
+  vq_wait_pool_t wait_pool;
+#if RPI_TRACE_TIME_VPU_QPU_WAIT
+  trace_time_wait_t ttw;
+#endif
+} gpu_env_t;
+
+// Stop more than one thread trying to allocate memory or use the processing resources at once
+static pthread_mutex_t gpu_mutex = PTHREAD_MUTEX_INITIALIZER;
+static gpu_env_t * gpu = NULL;
+
+#if RPI_TRACE_TIME_VPU_QPU_WAIT
+
+static int64_t ns_time(void)
+{
+    struct timespec ts;
+    clock_gettime(CLOCK_MONOTONIC, &ts);
+    return (int64_t)ts.tv_sec * (int64_t)1000000000 + ts.tv_nsec;
+}
+
+
+#define WAIT_TIME_PRINT_PERIOD (int64_t)2000000000
+
+#define T_MS(t) ((unsigned int)((t)/(int64_t)1000000) % 1000U)
+#define T_SEC(t) (unsigned int)((t)/(int64_t)1000000000)
+#define T_ARG(t) T_SEC(t), T_MS(t)
+#define T_FMT "%u.%03u"
+
+static void tto_print(trace_time_one_t * tto, const int64_t now, const int64_t start0, const char * const prefix)
+{
+  // Update totals for levels that are still pending
+  for (int i = 0; i < tto->count; ++i) {
+    tto->total[i] += now - tto->start[i];
+    tto->start[i] = now;
+  }
+
+  printf("%s: Idle:" T_FMT ", 1:" T_FMT ", 2:" T_FMT ", 3:" T_FMT ", 4:" T_FMT "\n",
+         prefix,
+         T_ARG(now - start0 - tto->total[0]),
+         T_ARG(tto->total[0]),
+         T_ARG(tto->total[1]),
+         T_ARG(tto->total[2]),
+         T_ARG(tto->total[3]));
+}
+
+
+static void tto_start(trace_time_one_t * const tto, const int64_t now)
+{
+  av_assert0(tto->count < WAIT_COUNT_MAX);
+  tto->start[tto->count++] = now;
+}
+
+static void tto_end(trace_time_one_t * const tto, const int64_t now)
+{
+  const int n = --tto->count;
+  av_assert0(n >= 0);
+  tto->total[n] += now - tto->start[n];
+}
+
+static void ttw_print(trace_time_wait_t * const ttw, const int64_t now)
+{
+  printf("Jobs:%d, Total time=" T_FMT "\n", ttw->jcount, T_ARG(now - ttw->start0));
+  tto_print(&ttw->active, now, ttw->start0, "Active");
+  tto_print(&ttw->wait,   now, ttw->start0, "  Wait");
+}
+
+#endif
+
+// GPU memory alloc fns (internal)
+
+// GPU_MEM_PTR_T alloc fns
+static int gpu_malloc_cached_internal(const int mb, const int numbytes, GPU_MEM_PTR_T * const p) {
+  p->numbytes = numbytes;
+  p->vcsm_handle = vcsm_malloc_cache(numbytes, VCSM_CACHE_TYPE_HOST, (char *)"Video Frame" );
+  //p->vcsm_handle = vcsm_malloc_cache(numbytes, VCSM_CACHE_TYPE_VC, (char *)"Video Frame" );
+  //p->vcsm_handle = vcsm_malloc_cache(numbytes, VCSM_CACHE_TYPE_NONE, (char *)"Video Frame" );
+  //p->vcsm_handle = vcsm_malloc_cache(numbytes, VCSM_CACHE_TYPE_HOST_AND_VC, (char *)"Video Frame" );
+  av_assert0(p->vcsm_handle);
+  p->vc_handle = vcsm_vc_hdl_from_hdl(p->vcsm_handle);
+  av_assert0(p->vc_handle);
+  p->arm = vcsm_lock(p->vcsm_handle);
+  av_assert0(p->arm);
+  p->vc = mbox_mem_lock(mb, p->vc_handle);
+  av_assert0(p->vc);
+  return 0;
+}
+
+static int gpu_malloc_uncached_internal(const int mb, const int numbytes, GPU_MEM_PTR_T * const p) {
+  p->numbytes = numbytes;
+  p->vcsm_handle = vcsm_malloc_cache(numbytes, VCSM_CACHE_TYPE_NONE, (char *)"Video Frame" );
+  av_assert0(p->vcsm_handle);
+  p->vc_handle = vcsm_vc_hdl_from_hdl(p->vcsm_handle);
+  av_assert0(p->vc_handle);
+  p->arm = vcsm_lock(p->vcsm_handle);
+  av_assert0(p->arm);
+  p->vc = mbox_mem_lock(mb, p->vc_handle);
+  av_assert0(p->vc);
+  return 0;
+}
+
+static void gpu_free_internal(const int mb, GPU_MEM_PTR_T * const p) {
+  mbox_mem_unlock(mb, p->vc_handle);
+  vcsm_unlock_ptr(p->arm);
+  vcsm_free(p->vcsm_handle);
+  memset(p, 0, sizeof(*p));  // Ensure we crash hard if we try and use this again
+}
+
+
+// GPU init, free, lock, unlock
+
+static void gpu_term(void)
+{
+  gpu_env_t * const ge = gpu;
+
+  // We have to hope that eveything has terminated...
+  gpu = NULL;
+
+  vc_gpuserv_deinit();
+
+  gpu_free_internal(ge->mb, &ge->code_gm_ptr);
+
+  vcsm_exit();
+
+  mbox_close(ge->mb);
+
+  vq_wait_pool_deinit(&ge->wait_pool);
+
+  free(ge);
+}
+
+
+// Connect to QPU, returns 0 on success.
+static int gpu_init(gpu_env_t ** const gpu) {
+  volatile struct GPU* ptr;
+  gpu_env_t * const ge = calloc(1, sizeof(gpu_env_t));
+  *gpu = NULL;
+
+  if (ge == NULL)
+    return -1;
+
+  if ((ge->mb = mbox_open()) < 0)
+    return -1;
+
+  vq_wait_pool_init(&ge->wait_pool);
+
+  vcsm_init();
+
+  gpu_malloc_uncached_internal(ge->mb, sizeof(struct GPU), &ge->code_gm_ptr);
+  ptr = (volatile struct GPU*)ge->code_gm_ptr.arm;
+
+  // Zero everything so we have zeros between the code bits
+  memset((void *)ptr, 0, sizeof(*ptr));
+
+  // Now copy over the QPU code into GPU memory
+  {
+    int num_bytes = (char *)mc_end - (char *)rpi_shader;
+    av_assert0(num_bytes<=QPU_CODE_SIZE*sizeof(unsigned int));
+    memcpy((void*)ptr->qpu_code, rpi_shader, num_bytes);
+  }
+  // And the VPU code
+  {
+    int num_bytes = sizeof(rpi_hevc_transform);
+    av_assert0(num_bytes<=VPU_CODE_SIZE*sizeof(unsigned int));
+    memcpy((void*)ptr->vpu_code, rpi_hevc_transform, num_bytes);
+  }
+  // And the transform coefficients
+  memcpy((void*)ptr->transMatrix2even, rpi_transMatrix2even, sizeof(rpi_transMatrix2even));
+
+  *gpu = ge;
+  return 0;
+}
+
+
+
+static void gpu_unlock(void) {
+  pthread_mutex_unlock(&gpu_mutex);
+}
+
+// Make sure we have exclusive access to the mailbox, and enable qpu if necessary.
+static gpu_env_t * gpu_lock(void) {
+  pthread_mutex_lock(&gpu_mutex);
+
+  av_assert0(gpu != NULL);
+  return gpu;
+}
+
+static gpu_env_t * gpu_lock_ref(void)
+{
+  pthread_mutex_lock(&gpu_mutex);
+
+  if (gpu == NULL) {
+    int rv = gpu_init(&gpu);
+    if (rv != 0) {
+      gpu_unlock();
+      return NULL;
+    }
+  }
+
+  ++gpu->open_count;
+  return gpu;
+}
+
+static void gpu_unlock_unref(gpu_env_t * const ge)
+{
+  if (--ge->open_count == 0)
+    gpu_term();
+
+  gpu_unlock();
+}
+
+static inline gpu_env_t * gpu_ptr(void)
+{
+  av_assert0(gpu != NULL);
+  return gpu;
+}
+
+// Public gpu fns
+
+// Allocate memory on GPU
+// Fills in structure <p> containing ARM pointer, videocore handle, videocore memory address, numbytes
+// Returns 0 on success.
+// This allocates memory that will not be cached in ARM's data cache.
+// Therefore safe to use without data cache flushing.
+int gpu_malloc_uncached(int numbytes, GPU_MEM_PTR_T *p)
+{
+  int r;
+  gpu_env_t * const ge = gpu_lock_ref();
+  if (ge == NULL)
+    return -1;
+  r = gpu_malloc_uncached_internal(ge->mb, numbytes, p);
+  gpu_unlock();
+  return r;
+}
+
+// This allocates data that will be
+//    Cached in ARM L2
+//    Uncached in VPU L2
+int gpu_malloc_cached(int numbytes, GPU_MEM_PTR_T *p)
+{
+  int r;
+  gpu_env_t * const ge = gpu_lock_ref();
+  if (ge == NULL)
+    return -1;
+  r = gpu_malloc_cached_internal(ge->mb, numbytes, p);
+  gpu_unlock();
+  return r;
+}
+
+void gpu_free(GPU_MEM_PTR_T * const p) {
+  gpu_env_t * const ge = gpu_lock();
+  gpu_free_internal(ge->mb, p);
+  gpu_unlock_unref(ge);
+}
+
+unsigned int vpu_get_fn(void) {
+  // Make sure that the gpu is initialized
+  av_assert0(gpu != NULL);
+  return gpu->code_gm_ptr.vc + offsetof(struct GPU, vpu_code);
+}
+
+unsigned int vpu_get_constants(void) {
+  av_assert0(gpu != NULL);
+  return gpu->code_gm_ptr.vc + offsetof(struct GPU,transMatrix2even);
+}
+
+int gpu_get_mailbox(void)
+{
+  av_assert0(gpu);
+  return gpu->mb;
+}
+
+void gpu_ref(void)
+{
+  gpu_lock_ref();
+  gpu_unlock();
+}
+
+void gpu_unref(void)
+{
+  gpu_env_t * const ge = gpu_lock();
+  gpu_unlock_unref(ge);
+}
+
+// ----------------------------------------------------------------------------
+//
+// Cache flush functions
+
+
+rpi_cache_flush_env_t * rpi_cache_flush_init()
+{
+    rpi_cache_flush_env_t * const rfe = malloc(sizeof(rpi_cache_flush_env_t));
+    if (rfe == NULL)
+        return NULL;
+
+    rfe->n = 0;
+    return rfe;
+}
+
+void rpi_cache_flush_abort(rpi_cache_flush_env_t * const rfe)
+{
+    if (rfe != NULL)
+        free(rfe);
+}
+
+int rpi_cache_flush_finish(rpi_cache_flush_env_t * const rfe)
+{
+    int rc = 0;
+    unsigned int na;
+    unsigned int nr;
+
+    // Clear any reamaining ents in the final block
+    if ((nr = rfe->n % CFE_ENTS_PER_A) != 0)
+        memset(rfe->a[rfe->n / CFE_ENTS_PER_A].s + nr, 0, (CFE_ENTS_PER_A - nr) * sizeof(rfe->a[0].s[0]));
+
+    for (na = 0; na * CFE_ENTS_PER_A < rfe->n; ++na)
+    {
+        if (vcsm_clean_invalid(rfe->a + na) != 0)
+            rc = -1;
+    }
+
+    free(rfe);
+
+    if (rc == 0)
+        return 0;
+
+    av_log(NULL, AV_LOG_ERROR, "vcsm_clean_invalid failed: errno=%d\n", errno);
+    return rc;
+}
+
+void rpi_cache_flush_add_gm_ptr(rpi_cache_flush_env_t * const rfe, const GPU_MEM_PTR_T * const gm, const unsigned int mode)
+{
+    // Deal with empty pointer trivially
+    if (gm == NULL || gm->numbytes == 0)
+        return;
+
+    {
+        struct vcsm_user_clean_invalid_s * const a = rfe->a + (rfe->n / CFE_ENTS_PER_A);
+        const unsigned int n = rfe->n % CFE_ENTS_PER_A;
+
+        av_assert0(rfe->n < CFE_ENT_COUNT);
+
+        a->s[n].cmd = mode;
+        a->s[n].handle = gm->vcsm_handle;
+        a->s[n].addr = (unsigned int)gm->arm;
+        a->s[n].size = gm->numbytes;
+        ++rfe->n;
+    }
+}
+
+void rpi_cache_flush_add_gm_range(rpi_cache_flush_env_t * const rfe, const GPU_MEM_PTR_T * const gm, const unsigned int mode,
+  const unsigned int offset, const unsigned int size)
+{
+    // Deal with empty pointer trivially
+    if (gm == NULL || size == 0)
+        return;
+
+//    printf("[%d] offset=%d, size=%d, numbytes=%d\n", rfe->n, offset, size, gm->numbytes);
+
+    av_assert0(offset <= gm->numbytes);
+    av_assert0(size <= gm->numbytes);
+    av_assert0(offset + size <= gm->numbytes);
+
+    {
+        struct vcsm_user_clean_invalid_s * const a = rfe->a + (rfe->n / CFE_ENTS_PER_A);
+        const unsigned int n = rfe->n % CFE_ENTS_PER_A;
+
+        av_assert0(rfe->n < CFE_ENT_COUNT);
+
+        a->s[n].cmd = mode;
+        a->s[n].handle = gm->vcsm_handle;
+        a->s[n].addr = (unsigned int)gm->arm + offset;
+        a->s[n].size = size;
+        ++rfe->n;
+    }
+}
+
+void rpi_cache_flush_add_frame(rpi_cache_flush_env_t * const rfe, const AVFrame * const frame, const unsigned int mode)
+{
+#if !RPI_ONE_BUF
+#error Fixme! (NIF)
+#endif
+  if (gpu_is_buf1(frame)) {
+    rpi_cache_flush_add_gm_ptr(rfe, gpu_buf1_gmem(frame), mode);
+  }
+  else
+  {
+    rpi_cache_flush_add_gm_ptr(rfe, gpu_buf3_gmem(frame, 0), mode);
+    rpi_cache_flush_add_gm_ptr(rfe, gpu_buf3_gmem(frame, 1), mode);
+    rpi_cache_flush_add_gm_ptr(rfe, gpu_buf3_gmem(frame, 2), mode);
+  }
+}
+
+void rpi_cache_flush_add_frame_lines(rpi_cache_flush_env_t * const rfe, const AVFrame * const frame, const unsigned int mode,
+  const unsigned int start_line, const unsigned int n, const unsigned int uv_shift, const int do_luma, const int do_chroma)
+{
+  const unsigned int y_offset = frame->linesize[0] * start_line;
+  const unsigned int y_size = frame->linesize[0] * n;
+  // Round UV up/down to get everything
+  const unsigned int uv_rnd = (1U << uv_shift) >> 1;
+  const unsigned int uv_offset = frame->linesize[1] * (start_line >> uv_shift);
+  const unsigned int uv_size = frame->linesize[1] * ((start_line + n + uv_rnd) >> uv_shift) - uv_offset;
+
+  // As all unsigned they will also reject -ve
+  // Test individually as well as added to reject overflow
+  av_assert0(start_line <= (unsigned int)frame->height);
+  av_assert0(n <= (unsigned int)frame->height);
+  av_assert0(start_line + n <= (unsigned int)frame->height);
+
+  if (!gpu_is_buf1(frame))
+  {
+    if (do_luma) {
+      rpi_cache_flush_add_gm_range(rfe, gpu_buf3_gmem(frame, 0), mode, y_offset, y_size);
+    }
+    if (do_chroma) {
+      rpi_cache_flush_add_gm_range(rfe, gpu_buf3_gmem(frame, 1), mode, uv_offset, uv_size);
+      rpi_cache_flush_add_gm_range(rfe, gpu_buf3_gmem(frame, 2), mode, uv_offset, uv_size);
+    }
+  }
+  else if (!rpi_sliced_frame(frame))
+  {
+    const GPU_MEM_PTR_T * const gm = gpu_buf1_gmem(frame);
+    if (do_luma) {
+      rpi_cache_flush_add_gm_range(rfe, gm, mode, (frame->data[0] - gm->arm) + y_offset, y_size);
+    }
+    if (do_chroma) {
+      rpi_cache_flush_add_gm_range(rfe, gm, mode, (frame->data[1] - gm->arm) + uv_offset, uv_size);
+      rpi_cache_flush_add_gm_range(rfe, gm, mode, (frame->data[2] - gm->arm) + uv_offset, uv_size);
+    }
+  }
+  else
+  {
+    const GPU_MEM_PTR_T * const gm = gpu_buf1_gmem(frame);
+//    printf("%s: start_line=%d, lines=%d, %c%c\n", __func__, start_line, n, do_luma ? 'l' : ' ', do_chroma ? 'c' : ' ');
+    for (int x = 0; x < frame->width; x += frame->linesize[0]) {
+      if (do_luma) {
+        rpi_cache_flush_add_gm_range(rfe, gm, mode, rpi_sliced_frame_off_y(frame, x, start_line), y_size);
+      }
+      if (do_chroma) {
+        rpi_cache_flush_add_gm_range(rfe, gm, mode,
+                                     (frame->data[1] - gm->arm) + rpi_sliced_frame_off_c(frame, x >> 1, start_line >> 1), uv_size);
+      }
+    }
+  }
+}
+
+// Call this to clean and invalidate a region of memory
+void rpi_cache_flush_one_gm_ptr(const GPU_MEM_PTR_T *const p, const rpi_cache_flush_mode_t mode)
+{
+  rpi_cache_flush_env_t * rfe = rpi_cache_flush_init();
+  rpi_cache_flush_add_gm_ptr(rfe, p, mode);
+  rpi_cache_flush_finish(rfe);
+}
+
+
+// ----------------------------------------------------------------------------
+
+
+// Wait abstractions - mostly so we can easily add profile code
+static void vq_wait_pool_init(vq_wait_pool_t * const wp)
+{
+  unsigned int i;
+  for (i = 0; i != VQ_WAIT_POOL_SIZE; ++i) {
+    sem_init(&wp->pool[i].sem, 0, 0);
+    wp->pool[i].next = wp->pool + i + 1;
+  }
+  wp->head = wp->pool + 0;
+  wp->pool[VQ_WAIT_POOL_SIZE - 1].next = NULL;
+}
+
+static void vq_wait_pool_deinit(vq_wait_pool_t * const wp)
+{
+  unsigned int i;
+  wp->head = NULL;
+  for (i = 0; i != VQ_WAIT_POOL_SIZE; ++i) {
+    sem_destroy(&wp->pool[i].sem);
+    wp->pool[i].next = NULL;
+  }
+}
+
+
+// If sem_init actually takes time then maybe we want a pool...
+static vq_wait_t * vq_wait_new(const unsigned int cost)
+{
+  gpu_env_t * const ge = gpu_lock_ref();
+  vq_wait_t * const wait = ge->wait_pool.head;
+  ge->wait_pool.head = wait->next;
+  ge->current_load += cost;
+  wait->cost = cost;
+  wait->next = NULL;
+
+#if RPI_TRACE_TIME_VPU_QPU_WAIT
+  tto_start(&ge->ttw.active, ns_time());
+#endif
+
+  gpu_unlock();
+  return wait;
+}
+
+static void vq_wait_delete(vq_wait_t * const wait)
+{
+  gpu_env_t * const ge = gpu_lock();
+  wait->next = ge->wait_pool.head;
+  ge->wait_pool.head = wait;
+
+#if RPI_TRACE_TIME_VPU_QPU_WAIT
+  {
+    trace_time_wait_t * const ttw = &ge->ttw;
+    const int64_t now = ns_time();
+    ++ttw->jcount;
+    tto_end(&ttw->wait, now);
+
+    if (ttw->start0 == 0)
+    {
+      ttw->start0 = ttw->active.start[0];
+      ttw->last_update = ttw->start0;
+    }
+    if (now - ttw->last_update > WAIT_TIME_PRINT_PERIOD)
+    {
+      ttw->last_update += WAIT_TIME_PRINT_PERIOD;
+      ttw_print(ttw, now);
+    }
+  }
+#endif
+  gpu_unlock_unref(ge);
+}
+
+static void vq_wait_wait(vq_wait_t * const wait)
+{
+#if RPI_TRACE_TIME_VPU_QPU_WAIT
+  {
+      const int64_t now = ns_time();
+      gpu_env_t * const ge = gpu_lock();
+      tto_start(&ge->ttw.wait, now);
+      gpu_unlock();
+  }
+#endif
+
+  while (sem_wait(&wait->sem) == -1 && errno == EINTR)
+    /* loop */;
+}
+
+static void vq_wait_post(vq_wait_t * const wait)
+{
+#if !RPI_TRACE_TIME_VPU_QPU_WAIT
+  if (wait->cost != 0)
+#endif
+  {
+    gpu_env_t *const ge = gpu_lock();
+    ge->current_load -= wait->cost;
+#if RPI_TRACE_TIME_VPU_QPU_WAIT
+    tto_end(&ge->ttw.active, ns_time());
+#endif
+    gpu_unlock();
+  }
+
+  sem_post(&wait->sem);
+}
+
+
+
+// Header comments were wrong for these two
+#define VPU_QPU_MASK_QPU  1
+#define VPU_QPU_MASK_VPU  2
+
+#define VPU_QPU_JOB_MAX 4
+struct vpu_qpu_job_env_s
+{
+  unsigned int n;
+  unsigned int mask;
+  unsigned int cost;
+  struct gpu_job_s j[VPU_QPU_JOB_MAX];
+};
+
+typedef struct vpu_qpu_job_env_s vpu_qpu_job_env_t;
+
+vpu_qpu_job_env_t * vpu_qpu_job_new(void)
+{
+  vpu_qpu_job_env_t * vqj = calloc(1, sizeof(vpu_qpu_job_env_t));
+  return vqj;
+}
+
+void vpu_qpu_job_delete(vpu_qpu_job_env_t * const vqj)
+{
+  memset(vqj, 0, sizeof(*vqj));
+  free(vqj);
+}
+
+static inline struct gpu_job_s * new_job(vpu_qpu_job_env_t * const vqj)
+{
+  struct gpu_job_s * const j = vqj->j + vqj->n++;
+  av_assert0(vqj->n <= VPU_QPU_JOB_MAX);
+  return j;
+}
+
+void vpu_qpu_job_add_vpu(vpu_qpu_job_env_t * const vqj, const uint32_t vpu_code,
+  const unsigned r0, const unsigned r1, const unsigned r2, const unsigned r3, const unsigned r4, const unsigned r5)
+{
+  if (vpu_code != 0) {
+    struct gpu_job_s *const j = new_job(vqj);
+    vqj->mask |= VPU_QPU_MASK_VPU;
+
+    j->command = EXECUTE_VPU;
+    j->u.v.q[0] = vpu_code;
+    j->u.v.q[1] = r0;
+    j->u.v.q[2] = r1;
+    j->u.v.q[3] = r2;
+    j->u.v.q[4] = r3;
+    j->u.v.q[5] = r4;
+    j->u.v.q[6] = r5;
+  }
+}
+
+// flags are QPU_FLAGS_xxx
+void vpu_qpu_job_add_qpu(vpu_qpu_job_env_t * const vqj, const unsigned int n, const unsigned int cost, const uint32_t * const mail)
+{
+  if (n != 0) {
+    struct gpu_job_s *const j = new_job(vqj);
+    vqj->mask |= VPU_QPU_MASK_QPU;
+    vqj->cost += cost;
+
+    j->command = EXECUTE_QPU;
+    j->u.q.jobs = n;
+#if RPI_TRACE_QPU_PROFILE_ALL
+    j->u.q.noflush = QPU_FLAGS_NO_FLUSH_VPU | QPU_FLAGS_PROF_CLEAR_AND_ENABLE | QPU_FLAGS_PROF_OUTPUT_COUNTS;
+#else
+    j->u.q.noflush = QPU_FLAGS_NO_FLUSH_VPU;
+#endif
+    j->u.q.timeout = 5000;
+    memcpy(j->u.q.control, mail, n * QPU_MAIL_EL_VALS * sizeof(uint32_t));
+  }
+}
+
+// Convert callback to sem post
+static void vpu_qpu_job_callback_wait(void * v)
+{
+  vq_wait_post(v);
+}
+
+void vpu_qpu_job_add_sync_this(vpu_qpu_job_env_t * const vqj, vpu_qpu_wait_h * const wait_h)
+{
+  vq_wait_t * wait;
+
+  if (vqj->mask == 0) {
+    *wait_h = NULL;
+    return;
+  }
+
+  // We are going to want a sync object
+  wait = vq_wait_new(vqj->cost);
+
+  // There are 2 VPU Qs & 1 QPU Q so we can collapse sync
+  // If we only posted one thing or only QPU jobs
+  if (vqj->n == 1 || vqj->mask == VPU_QPU_MASK_QPU)
+  {
+    struct gpu_job_s * const j = vqj->j + (vqj->n - 1);
+    av_assert0(j->callback.func == 0);
+
+    j->callback.func = vpu_qpu_job_callback_wait;
+    j->callback.cookie = wait;
+  }
+  else
+  {
+    struct gpu_job_s *const j = new_job(vqj);
+
+    j->command = EXECUTE_SYNC;
+    j->u.s.mask = vqj->mask;
+    j->callback.func = vpu_qpu_job_callback_wait;
+    j->callback.cookie = wait;
+  }
+
+  vqj->cost = 0;
+  vqj->mask = 0;
+  *wait_h = wait;
+}
+
+int vpu_qpu_job_start(vpu_qpu_job_env_t * const vqj)
+{
+  return vqj->n == 0 ? 0 : vc_gpuserv_execute_code(vqj->n, vqj->j);
+}
+
+// Simple wrapper of start + delete
+int vpu_qpu_job_finish(vpu_qpu_job_env_t * const vqj)
+{
+  int rv;
+  rv = vpu_qpu_job_start(vqj);
+  vpu_qpu_job_delete(vqj);
+  return rv;
+}
+
+unsigned int vpu_qpu_current_load(void)
+{
+  return gpu_ptr()->current_load;
+}
+
+void vpu_qpu_wait(vpu_qpu_wait_h * const wait_h)
+{
+  if (wait_h != NULL)
+  {
+    vq_wait_t * const wait = *wait_h;
+    if (wait != NULL) {
+      *wait_h = NULL;
+      vq_wait_wait(wait);
+      vq_wait_delete(wait);
+    }
+  }
+}
+
+int vpu_qpu_init()
+{
+  gpu_env_t * const ge = gpu_lock_ref();
+  if (ge == NULL)
+    return -1;
+
+  if (ge->init_count++ == 0)
+  {
+    vc_gpuserv_init();
+  }
+
+  gpu_unlock();
+  return 0;
+}
+
+void vpu_qpu_term()
+{
+  gpu_env_t * const ge = gpu_lock();
+
+  if (--ge->init_count == 0) {
+    vc_gpuserv_deinit();
+
+#if RPI_TRACE_TIME_VPU_QPU_WAIT
+    ttw_print(&ge->ttw, ns_time());
+#endif
+  }
+
+  gpu_unlock_unref(ge);
+}
+
+uint32_t qpu_fn(const int * const mc_fn)
+{
+  return gpu->code_gm_ptr.vc + ((const char *)mc_fn - (const char *)rpi_shader) + offsetof(struct GPU, qpu_code);
+}
+
+#endif // RPI
diff -Naur ffmpeg-3.2.4/libavcodec/rpi_qpu.h ffmpeg-3.2.4.patch/libavcodec/rpi_qpu.h
--- ffmpeg-3.2.4/libavcodec/rpi_qpu.h	1970-01-01 01:00:00.000000000 +0100
+++ ffmpeg-3.2.4.patch/libavcodec/rpi_qpu.h	2017-05-28 20:42:45.752088716 +0200
@@ -0,0 +1,200 @@
+#ifndef RPI_QPU_H
+#define RPI_QPU_H
+
+#define RPI_ONE_BUF 1
+
+typedef struct gpu_mem_ptr_s {
+  unsigned char *arm; // Pointer to memory mapped on ARM side
+  int vc_handle;   // Videocore handle of relocatable memory
+  int vcsm_handle; // Handle for use by VCSM
+  int vc;       // Address for use in GPU code
+  int numbytes; // Size of memory block
+} GPU_MEM_PTR_T;
+
+// General GPU functions
+extern int gpu_malloc_cached(int numbytes, GPU_MEM_PTR_T *p);
+extern int gpu_malloc_uncached(int numbytes, GPU_MEM_PTR_T *p);
+extern void gpu_free(GPU_MEM_PTR_T * const p);
+
+#include "libavutil/frame.h"
+#if !RPI_ONE_BUF
+static inline uint32_t get_vc_address_y(const AVFrame * const frame) {
+    GPU_MEM_PTR_T *p = av_buffer_pool_opaque(frame->buf[0]);
+    return p->vc;
+}
+
+static inline uint32_t get_vc_address_u(const AVFrame * const frame) {
+    GPU_MEM_PTR_T *p = av_buffer_pool_opaque(frame->buf[1]);
+    return p->vc;
+}
+
+static inline uint32_t get_vc_address_v(const AVFrame * const frame) {
+    GPU_MEM_PTR_T *p = av_buffer_pool_opaque(frame->buf[2]);
+    return p->vc;
+}
+
+static inline GPU_MEM_PTR_T get_gpu_mem_ptr_y(const AVFrame * const frame) {
+    return *(GPU_MEM_PTR_T *)av_buffer_pool_opaque(frame->buf[0]);
+}
+
+static inline GPU_MEM_PTR_T get_gpu_mem_ptr_u(const AVFrame * const frame) {
+    return *(GPU_MEM_PTR_T *)av_buffer_pool_opaque(frame->buf[1]);
+}
+
+static inline GPU_MEM_PTR_T get_gpu_mem_ptr_v(const AVFrame * const frame) {
+    return *(GPU_MEM_PTR_T *)av_buffer_pool_opaque(frame->buf[2]);
+}
+
+#else
+
+static inline int gpu_is_buf1(const AVFrame * const frame)
+{
+    return frame->buf[1] == NULL;
+}
+
+static inline GPU_MEM_PTR_T * gpu_buf1_gmem(const AVFrame * const frame)
+{
+    return av_buffer_get_opaque(frame->buf[0]);
+}
+
+static inline GPU_MEM_PTR_T * gpu_buf3_gmem(const AVFrame * const frame, const unsigned int n)
+{
+    return av_buffer_pool_opaque(frame->buf[n]);
+}
+
+static inline uint32_t get_vc_address3(const AVFrame * const frame, const unsigned int n)
+{
+    const GPU_MEM_PTR_T * const gm = gpu_is_buf1(frame) ? gpu_buf1_gmem(frame) : gpu_buf3_gmem(frame, n);
+    return gm->vc + (frame->data[n] - gm->arm);
+}
+
+
+static inline uint32_t get_vc_address_y(const AVFrame * const frame) {
+    return get_vc_address3(frame, 0);
+}
+
+static inline uint32_t get_vc_address_u(const AVFrame * const frame) {
+    return get_vc_address3(frame, 1);
+}
+
+static inline uint32_t get_vc_address_v(const AVFrame * const frame) {
+    return get_vc_address3(frame, 2);
+}
+
+#if 0
+static inline GPU_MEM_PTR_T get_gpu_mem_ptr_y(const AVFrame * const frame) {
+    if (gpu_is_buf1(frame))
+    {
+        GPU_MEM_PTR_T g = *gpu_buf1_gmem(frame);
+        g.numbytes = frame->data[1] - frame->data[0];
+        return g;
+    }
+    else
+        return *gpu_buf3_gmem(frame, 0);
+}
+
+static inline GPU_MEM_PTR_T get_gpu_mem_ptr_u(const AVFrame * const frame) {
+    if (gpu_is_buf1(frame))
+    {
+        GPU_MEM_PTR_T g = *gpu_buf1_gmem(frame);
+        g.arm += frame->data[1] - frame->data[0];
+        g.vc += frame->data[1] - frame->data[0];
+        g.numbytes = frame->data[2] - frame->data[1];  // chroma size
+        return g;
+    }
+    else
+        return *gpu_buf3_gmem(frame, 1);
+}
+
+static inline GPU_MEM_PTR_T get_gpu_mem_ptr_v(const AVFrame * const frame) {
+    if (gpu_is_buf1(frame))
+    {
+        GPU_MEM_PTR_T g = *gpu_buf1_gmem(frame);
+        g.arm += frame->data[2] - frame->data[0];
+        g.vc += frame->data[2] - frame->data[0];
+        g.numbytes = frame->data[2] - frame->data[1];  // chroma size
+        return g;
+    }
+    else
+        return *gpu_buf3_gmem(frame, 2);
+}
+#endif
+#endif
+
+// Cache flush stuff
+
+struct rpi_cache_flush_env_s;
+typedef struct rpi_cache_flush_env_s rpi_cache_flush_env_t;
+
+rpi_cache_flush_env_t * rpi_cache_flush_init(void);
+// Free env without flushing
+void rpi_cache_flush_abort(rpi_cache_flush_env_t * const rfe);
+// Do the accumulated flush & free the env
+int rpi_cache_flush_finish(rpi_cache_flush_env_t * const rfe);
+
+typedef enum
+{
+    RPI_CACHE_FLUSH_MODE_INVALIDATE     = 1,
+    RPI_CACHE_FLUSH_MODE_WRITEBACK      = 2,
+    RPI_CACHE_FLUSH_MODE_WB_INVALIDATE  = 3
+} rpi_cache_flush_mode_t;
+
+void rpi_cache_flush_add_gm_ptr(rpi_cache_flush_env_t * const rfe, const GPU_MEM_PTR_T * const gm, const rpi_cache_flush_mode_t mode);
+void rpi_cache_flush_add_gm_range(rpi_cache_flush_env_t * const rfe, const GPU_MEM_PTR_T * const gm, const rpi_cache_flush_mode_t mode,
+  const unsigned int offset, const unsigned int size);
+void rpi_cache_flush_add_frame(rpi_cache_flush_env_t * const rfe, const AVFrame * const frame, const rpi_cache_flush_mode_t mode);
+void rpi_cache_flush_add_frame_lines(rpi_cache_flush_env_t * const rfe, const AVFrame * const frame, const rpi_cache_flush_mode_t mode,
+  const unsigned int start_line, const unsigned int n, const unsigned int uv_shift, const int do_luma, const int do_chroma);
+
+// init, add, finish for one gm ptr
+void rpi_cache_flush_one_gm_ptr(const GPU_MEM_PTR_T * const p, const rpi_cache_flush_mode_t mode);
+
+
+// QPU specific functions
+uint32_t qpu_fn(const int * const mc_fn);
+
+#define QPU_N_GRP_UV 4
+#define QPU_N_UV     8
+#define QPU_N_GRP_Y  4  // 4 QPUs per TMU
+#define QPU_N_Y      12
+
+#define QPU_MAIL_EL_VALS  2
+
+struct vpu_qpu_wait_s;
+typedef struct vq_wait_s * vpu_qpu_wait_h;
+
+// VPU specific functions
+
+struct vpu_qpu_job_env_s;
+typedef struct vpu_qpu_job_env_s * vpu_qpu_job_h;
+
+vpu_qpu_job_h vpu_qpu_job_new(void);
+void vpu_qpu_job_delete(const vpu_qpu_job_h vqj);
+void vpu_qpu_job_add_vpu(const vpu_qpu_job_h vqj, const uint32_t vpu_code,
+  const unsigned r0, const unsigned r1, const unsigned r2, const unsigned r3, const unsigned r4, const unsigned r5);
+void vpu_qpu_job_add_qpu(const vpu_qpu_job_h vqj, const unsigned int n, const unsigned int cost, const uint32_t * const mail);
+void vpu_qpu_job_add_sync_this(const vpu_qpu_job_h vqj, vpu_qpu_wait_h * const wait_h);
+int vpu_qpu_job_start(const vpu_qpu_job_h vqj);
+int vpu_qpu_job_finish(const vpu_qpu_job_h vqj);
+
+
+extern unsigned int vpu_get_fn(void);
+extern unsigned int vpu_get_constants(void);
+
+// Waits for previous post_codee to complete and Will null out *wait_h after use
+void vpu_qpu_wait(vpu_qpu_wait_h * const wait_h);
+unsigned int vpu_qpu_current_load(void);
+int vpu_qpu_init(void);
+void vpu_qpu_term(void);
+
+// Simple test of shader code
+extern int rpi_test_shader(void);
+
+extern void rpi_do_block(const unsigned char *in_buffer_vc, int src_pitch, unsigned char *dst_vc, int dst_pitch, unsigned char *dst);
+extern void rpi_do_block_arm(const unsigned char *in_buffer, int src_pitch, unsigned char *dst, int dst_pitch);
+
+extern int gpu_get_mailbox(void);
+void gpu_ref(void);
+void gpu_unref(void);
+
+#endif
diff -Naur ffmpeg-3.2.4/libavcodec/rpi_shader.c ffmpeg-3.2.4.patch/libavcodec/rpi_shader.c
--- ffmpeg-3.2.4/libavcodec/rpi_shader.c	1970-01-01 01:00:00.000000000 +0100
+++ ffmpeg-3.2.4.patch/libavcodec/rpi_shader.c	2017-05-28 20:42:45.753088719 +0200
@@ -0,0 +1,670 @@
+#include "rpi_shader.h"
+
+#ifdef _MSC_VER
+   #include <stdint.h>
+   /* cast through uintptr_t to avoid warnings */
+   #define POINTER_TO_UINT(X) ((unsigned int)(uintptr_t)(X))
+#else
+   #define POINTER_TO_UINT(X) ((unsigned int)(X))
+#endif
+
+#ifdef __cplusplus
+extern "C" { /* the types are probably wrong... */
+#endif
+#ifdef __cplusplus
+}
+#endif
+
+#ifdef _MSC_VER
+__declspec(align(8))
+#elif defined(__GNUC__)
+__attribute__((aligned(8)))
+#endif
+unsigned int rpi_shader[] = {
+// ::mc_setup_c
+/* [0x00000000] */ 0x95801ff6, 0xd0020927, // mov tmurs, 1          ; mov -, unif
+/* [0x00000008] */ 0x15827d80, 0x10020027, // mov ra0, unif
+/* [0x00000010] */ 0x15827d80, 0x10020627, // mov ra_base, unif
+/* [0x00000018] */ 0x0d801dc0, 0xd0021667, // sub rb_max_x, unif, 1
+/* [0x00000020] */ 0x0d801dc0, 0xd00217a7, // sub rb_max_y, unif, 1
+/* [0x00000028] */ 0x00000001, 0xe0020527, // mov ra_k1, 1
+/* [0x00000030] */ 0x00000100, 0xe00205a7, // mov ra_k256, 256
+/* [0x00000038] */ 0x000000ff, 0xe00215a7, // mov rb_k255, 255
+/* [0x00000040] */ 0x00000000, 0xe00205e7, // mov ra_k0, 0
+/* [0x00000048] */ 0x00000000, 0xe0024104, // mov ra4, 0 ; mov rb4, 0
+/* [0x00000050] */ 0x00000000, 0xe0024145, // mov ra5, 0 ; mov rb5, 0
+/* [0x00000058] */ 0x00000000, 0xe0024186, // mov ra6, 0 ; mov rb6, 0
+/* [0x00000060] */ 0x00000000, 0xe00241c7, // mov ra7, 0 ; mov rb7, 0
+/* [0x00000068] */ 0xc0000000, 0xe0020867, // mov r1, vdw_setup_1(0)
+/* [0x00000070] */ 0x95800dbf, 0xd002550c, // mov rb_xpitch, unif   ; mov ra12, 0
+/* [0x00000078] */ 0x95800dbf, 0xd002540d, // mov rb_pitch, unif    ; mov ra13, 0
+/* [0x00000080] */ 0x95980dbf, 0xd002580e, // mov r0, elem_num      ; mov ra14, 0
+/* [0x00000088] */ 0x8c5d03f6, 0x1002560f, // add rb24, r1, rb_pitch ; mov ra15, ra_k0
+/* [0x00000090] */ 0x0c027180, 0x14020827, // add r0, r0, ra0.16b
+/* [0x00000098] */ 0x930001f6, 0xd2225811, // max r0, r0, 0         ; mov ra_y, ra0.16a
+/* [0x000000a0] */ 0x129d91c0, 0x10020827, // min r0, r0, rb_max_x
+/* [0x000000a8] */ 0x149c11c0, 0xd0020867, // and r1, r0, 1
+/* [0x000000b0] */ 0x119c43c0, 0xd01204e7, // shl ra_xshift_next, r1, 4
+/* [0x000000b8] */ 0x149de1c0, 0xd0020827, // and r0, r0, -2
+/* [0x000000c0] */ 0xec9e7009, 0x10024821, // add r0, r0, r0        ; v8subs r1, r1, r1
+/* [0x000000c8] */ 0x0d9d03c0, 0x10020867, // sub r1, r1, rb_pitch
+/* [0x000000d0] */ 0x149e7040, 0x10020867, // and r1, r0, r1
+/* [0x000000d8] */ 0x569d404f, 0x10024821, // xor r0, r0, r1        ; mul24 r1, r1, rb_xpitch
+/* [0x000000e0] */ 0x8c467076, 0x14024821, // add r0, r0, r1        ; mov r1, ra_y
+/* [0x000000e8] */ 0x0c627c00, 0x10020627, // add ra_base, ra_base, r0
+/* [0x000000f0] */ 0x139c03c0, 0xd0020827, // max r0, r1, 0
+/* [0x000000f8] */ 0x129de1c0, 0x10020827, // min r0, r0, rb_max_y
+/* [0x00000100] */ 0x4c510387, 0x10024860, // add r1, r1, ra_k1     ; mul24 r0, r0, rb_pitch
+/* [0x00000108] */ 0x0c627c00, 0x10020e27, // add t0s, ra_base, r0
+/* [0x00000110] */ 0x139c03c0, 0xd0020827, // max r0, r1, 0
+/* [0x00000118] */ 0x129de1c0, 0x10020827, // min r0, r0, rb_max_y
+/* [0x00000120] */ 0x4c510387, 0x10224460, // add ra_y, r1, ra_k1   ; mul24 r0, r0, rb_pitch
+/* [0x00000128] */ 0x0c627c00, 0x10020e27, // add t0s, ra_base, r0
+/* [0x00000130] */ 0x0c809f80, 0xd0021367, // add rb13, 9, unif
+/* [0x00000138] */ 0x15827d80, 0x100009e7, // mov -, unif
+/* [0x00000140] */ 0x159e6fc0, 0x100208a7, // mov r2, qpu_num
+/* [0x00000148] */ 0x0f9c15c0, 0xd0020867, // asr r1, r2, 1
+/* [0x00000150] */ 0x119c53c0, 0xd0020867, // shl r1, r1, 5
+/* [0x00000158] */ 0x149c15c0, 0xd0020827, // and r0, r2, 1
+/* [0x00000160] */ 0x159e7040, 0x10020827, // or  r0, r0, r1
+/* [0x00000168] */ 0x00002900, 0xe0020867, // mov r1, vpm_setup(0, 2, h16p(0, 0))
+/* [0x00000170] */ 0x0c9e7040, 0x10021727, // add r_vpm, r0, r1
+/* [0x00000178] */ 0x80004002, 0xe0020867, // mov r1, vdw_setup_0(0, 0, dma_h16p(0,0,0))
+/* [0x00000180] */ 0x119c61c0, 0xd0020827, // shl r0, r0, 6
+/* [0x00000188] */ 0x0c9e7040, 0x100216e7, // add r_dma, r0, r1
+/* [0x00000190] */ 0x15827d80, 0x100207a7, // mov ra_link, unif
+/* [0x00000198] */ 0x15827d80, 0x10020027, // mov ra0, unif
+/* [0x000001a0] */ 0x15827d80, 0x10020667, // mov ra_base2, unif
+/* [0x000001a8] */ 0x15027d80, 0x12120567, // mov ra_y2, ra0.16a
+/* [0x000001b0] */ 0x15027d80, 0x14020827, // mov r0, ra0.16b
+/* [0x000001b8] */ 0x0c9a7180, 0x10020827, // add r0, r0, elem_num
+/* [0x000001c0] */ 0x938001f6, 0xd0020827, // max r0, r0, 0         ; mov -, unif
+/* [0x000001c8] */ 0x928191f6, 0x10020827, // min r0, r0, rb_max_x  ; mov -, unif
+/* [0x000001d0] */ 0x948011f6, 0xd0020867, // and r1, r0, 1         ; mov -, unif
+/* [0x000001d8] */ 0x119c43c0, 0xd0021067, // shl rb_xshift2_next, r1, 4
+/* [0x000001e0] */ 0x149de1c0, 0xd0020827, // and r0, r0, -2
+/* [0x000001e8] */ 0xec9e7009, 0x10024821, // add r0, r0, r0        ; v8subs r1, r1, r1
+/* [0x000001f0] */ 0x0d9d03c0, 0x10020867, // sub r1, r1, rb_pitch
+/* [0x000001f8] */ 0x149e7040, 0x10020867, // and r1, r0, r1
+/* [0x00000200] */ 0x569d404f, 0x10024821, // xor r0, r0, r1        ; mul24 r1, r1, rb_xpitch
+/* [0x00000208] */ 0x8c567076, 0x12024821, // add r0, r0, r1        ; mov r1, ra_y2
+/* [0x00000210] */ 0x0c667c00, 0x10020667, // add ra_base2, ra_base2, r0
+/* [0x00000218] */ 0x139c03c0, 0xd0020827, // max r0, r1, 0
+/* [0x00000220] */ 0x129de1c0, 0x10020827, // min r0, r0, rb_max_y
+/* [0x00000228] */ 0x4c510387, 0x10024860, // add r1, r1, ra_k1     ; mul24 r0, r0, rb_pitch
+/* [0x00000230] */ 0x8c660c3f, 0x10020f27, // add t1s, ra_base2, r0 ; mov -, unif
+/* [0x00000238] */ 0x938003f6, 0xd0020827, // max r0, r1, 0         ; mov -, unif
+/* [0x00000240] */ 0x00000000, 0xf0f7c9e7, // bra -, ra_link
+/* [0x00000248] */ 0x9281e1f6, 0x10020827, // min r0, r0, rb_max_y  ; mov -, unif
+/* [0x00000250] */ 0x4c510387, 0x10124560, // add ra_y2, r1, ra_k1   ; mul24 r0, r0, rb_pitch
+/* [0x00000258] */ 0x0c667c00, 0x10020f27, // add t1s, ra_base2, r0
+// ::mc_filter_uv
+/* [0x00000260] */ 0x9581cdbf, 0x100247b1, // mov ra_link, unif     ; mov vw_setup, rb28
+/* [0x00000268] */ 0x959a0ff6, 0x100240a0, // mov ra2, unif         ; mov r0, elem_num
+/* [0x00000270] */ 0x0000ff00, 0xe20229e7, // mov.setf -, [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]
+/* [0x00000278] */ 0xec0a7c09, 0x14024821, // add r0, ra2.16b, r0   ; v8subs r1, r1, r1
+/* [0x00000280] */ 0x8d8103f6, 0x10024863, // sub r1, r1, rb_pitch  ; mov r3, unif
+/* [0x00000288] */ 0x934c01f6, 0xd2024800, // max r0, r0, 0         ; mov rb_xshift2, ra_xshift_next
+/* [0x00000290] */ 0x928191f6, 0x10025801, // min r0, r0, rb_max_x  ; mov ra1, unif
+/* [0x00000298] */ 0x119c41c0, 0xd01204e7, // shl ra_xshift_next, r0, 4
+/* [0x000002a0] */ 0x9481e1f6, 0xd0025800, // and r0, r0, -2        ; mov ra0, unif
+/* [0x000002a8] */ 0x8c0a7036, 0x12225813, // add r0, r0, r0        ; mov ra_y_next, ra2.16a
+/* [0x000002b0] */ 0x54042077, 0xd4024862, // and r1, r0, r1        ; mul24 r2, ra1.16b, 2
+/* [0x000002b8] */ 0x569d404f, 0x10024821, // xor r0, r0, r1        ; mul24 r1, r1, rb_xpitch
+/* [0x000002c0] */ 0x8c067076, 0x12024821, // add r0, r0, r1        ; mov r1, ra1.16a
+/* [0x000002c8] */ 0x4c5a760e, 0x100246a0, // add ra_base_next, r3, r0 ; mul24 r0, r1, ra_k256
+/* [0x000002d0] */ 0x8d818eb6, 0x10025743, // sub rb29, rb24, r2    ; mov ra3, unif
+/* [0x000002d8] */ 0x8c8013f6, 0xd0025441, // add rb17, r1, 1       ; mov ra1, unif
+/* [0x000002e0] */ 0x8c8033f6, 0xd002d481, // add rb18, r1, 3       ; mov.ifnz ra1, unif
+/* [0x000002e8] */ 0x8c0e70b6, 0x18024808, // add r0,   r0, r2      ; mov rb8,  ra3.8a
+/* [0x000002f0] */ 0x910cf1f6, 0xda024809, // shl r0,   r0, 15      ; mov rb9,  ra3.8b
+/* [0x000002f8] */ 0x8c05b1f6, 0x140256a1, // add rb26, r0, rb27    ; mov r1, ra1.16b
+/* [0x00000300] */ 0x910cd3f6, 0x1c02484a, // shl r1, r1, rb13      ; mov rb10, ra3.8c
+/* [0x00000308] */ 0x950c0ff6, 0xde0248cb, // mov r3, 0             ; mov rb11, ra3.8d
+/* [0x00000310] */ 0x0f9c13c0, 0xd0021327, // asr rb12, r1, 1
+/* [0x00000318] */ 0x11041dc0, 0xd20213a7, // shl rb14, ra1.16a, 1
+// :uvloop
+/* [0x00000320] */ 0xcd5117de, 0xa00269df, // sub.setf -, r3, rb17  ; v8adds rb31, r3, ra_k1 ; ldtmu0
+/* [0x00000328] */ 0x8e4c09f6, 0x14028823, // shr r0, r4, rb_xshift2 ; mov.ifz r3, ra_y_next
+/* [0x00000330] */ 0x8e4481f6, 0xd402c863, // shr r1, r0, 8         ; mov.ifnz r3, ra_y
+/* [0x00000338] */ 0x936807f6, 0xd0029898, // max r2, r3, 0         ; mov.ifz ra_base, ra_base_next
+/* [0x00000340] */ 0x129de5c0, 0x100208a7, // min r2, r2, rb_max_y
+/* [0x00000348] */ 0x4c510797, 0x10224462, // add ra_y, r3, ra_k1   ; mul24 r2, r2, rb_pitch
+/* [0x00000350] */ 0x8c616c87, 0x10024e20, // add t0s, ra_base, r2  ; v8min r0, r0, rb_k255
+/* [0x00000358] */ 0x0000ff00, 0xe20229e7, // mov.setf -, [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]
+/* [0x00000360] */ 0x540163f0, 0x18024863, // and r1, r1, rb_k255   ; mul24      r3, ra0.8a,       r0
+/* [0x00000368] */ 0x4003f030, 0xda0049e2, // nop                   ; mul24      r2, ra0.8b << 1,  r0 << 1  @ "mul_used", 0
+/* [0x00000370] */ 0x40038031, 0xd800c9e3, // nop                   ; mul24.ifnz r3, ra0.8a << 8,  r1 << 8  @ "mul_used", 0
+/* [0x00000378] */ 0x40037031, 0xda00c9e2, // nop                   ; mul24.ifnz r2, ra0.8b << 9,  r1 << 9  @ "mul_used", 0
+/* [0x00000380] */ 0x4d03e4f0, 0xdc0248a3, // sub r2, r2, r3        ; mul24      r3, ra0.8c << 2,  r0 << 2  @ "mul_used", 0
+/* [0x00000388] */ 0x40036031, 0xdc00c9e3, // nop                   ; mul24.ifnz r3, ra0.8c << 10, r1 << 10 @ "mul_used", 0
+/* [0x00000390] */ 0x4c03d4f0, 0xde0248a3, // add r2, r2, r3        ; mul24      r3, ra0.8d << 3,  r0 << 3  @ "mul_used", 0
+/* [0x00000398] */ 0x40035031, 0xde00c9e3, // nop                   ; mul24.ifnz r3, ra0.8d << 11, r1 << 11 @ "mul_used", 0
+/* [0x000003a0] */ 0x8d9df4ff, 0x10024823, // sub r0, r2, r3        ; mov r3, rb31
+/* [0x000003a8] */ 0x8d3447f6, 0xd00279cc, // sub.setf -, r3, 4     ; mov ra12, ra13
+/* [0x000003b0] */ 0xffffff50, 0xf06809e7, // brr.anyn -, r:uvloop
+/* [0x000003b8] */ 0x55389db7, 0x10024361, // mov ra13, ra14        ; mul24 r1, ra14, rb9
+/* [0x000003c0] */ 0x153e7d80, 0x100203a7, // mov ra14, ra15
+/* [0x000003c8] */ 0x55308037, 0x100243e0, // mov ra15, r0          ; mul24 r0, ra12, rb8
+/* [0x000003d0] */ 0x4d38a237, 0x10024860, // sub r1, r1, r0        ; mul24 r0, ra14, rb10
+/* [0x000003d8] */ 0x4c3cb237, 0x10024860, // add r1, r1, r0        ; mul24 r0, ra15, rb11
+/* [0x000003e0] */ 0x0d9e7200, 0x10020867, // sub r1, r1, r0
+/* [0x000003e8] */ 0x4d5927ce, 0x100269e1, // sub.setf -, r3, rb18  ; mul24 r1, r1, ra_k256
+/* [0x000003f0] */ 0x0f9ce3c0, 0xd0020867, // asr r1, r1, 14
+/* [0x000003f8] */ 0x409ce00f, 0x100049e1, // nop                   ; mul24 r1, r1, rb14
+/* [0x00000400] */ 0x119c83c0, 0xd0020867, // shl r1, r1, 8
+/* [0x00000408] */ 0x0c9cc3c0, 0x10020867, // add r1, r1, rb12
+/* [0x00000410] */ 0x0f9cd3c0, 0x10c20067, // asr ra1.8as, r1, rb13
+/* [0x00000418] */ 0x809f8009, 0xd00049e1, // nop                   ; mov r1, r1 << 8
+/* [0x00000420] */ 0xfffffee0, 0xf06809e7, // brr.anyn -, r:uvloop
+/* [0x00000428] */ 0x0f9cd3c0, 0x10d20067, // asr ra1.8bs, r1, rb13
+/* [0x00000430] */ 0x159f2fc0, 0x100009e7, // mov -, vw_wait
+/* [0x00000438] */ 0x15067d80, 0x10020c27, // mov vpm, ra1
+/* [0x00000440] */ 0x00000000, 0xf0f7c9e7, // bra -, ra_link
+/* [0x00000448] */ 0x159dafc0, 0x10021c67, // mov vw_setup, rb26
+/* [0x00000450] */ 0x159ddfc0, 0x10021c67, // mov vw_setup, rb29
+/* [0x00000458] */ 0x15827d80, 0x10021ca7, // mov vw_addr, unif
+// ::mc_filter_uv_b0
+/* [0x00000460] */ 0x9581cdbf, 0x100049f1, // mov -, unif           ; mov vw_setup, rb28
+/* [0x00000468] */ 0x959a0ff6, 0x100240a0, // mov ra2, unif         ; mov r0, elem_num
+/* [0x00000470] */ 0x0000ff00, 0xe20229e7, // mov.setf -, [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]
+/* [0x00000478] */ 0xec0a7c09, 0x14024821, // add r0, ra2.16b, r0   ; v8subs r1, r1, r1
+/* [0x00000480] */ 0x8d8103f6, 0x10024863, // sub r1, r1, rb_pitch  ; mov r3, unif
+/* [0x00000488] */ 0x934c01f6, 0xd2024800, // max r0, r0, 0         ; mov rb_xshift2, ra_xshift_next
+/* [0x00000490] */ 0x928191f6, 0x10025801, // min r0, r0, rb_max_x  ; mov ra1, unif
+/* [0x00000498] */ 0x119c41c0, 0xd01204e7, // shl ra_xshift_next, r0, 4
+/* [0x000004a0] */ 0x9481e1f6, 0xd0025800, // and r0, r0, -2        ; mov ra0, unif
+/* [0x000004a8] */ 0x8c0a7036, 0x12225813, // add r0, r0, r0        ; mov ra_y_next, ra2.16a
+/* [0x000004b0] */ 0x54042077, 0xd4024862, // and r1, r0, r1        ; mul24 r2, ra1.16b, 2
+/* [0x000004b8] */ 0x569d404f, 0x10024821, // xor r0, r0, r1        ; mul24 r1, r1, rb_xpitch
+/* [0x000004c0] */ 0x8c067076, 0x12024821, // add r0, r0, r1        ; mov r1, ra1.16a
+/* [0x000004c8] */ 0x4c5a760e, 0x100246a0, // add ra_base_next, r3, r0 ; mul24 r0, r1, ra_k256
+/* [0x000004d0] */ 0x8d818eb6, 0x10025743, // sub rb29, rb24, r2    ; mov ra3, unif
+/* [0x000004d8] */ 0x0c9c13c0, 0xd0021467, // add rb17, r1, 1
+/* [0x000004e0] */ 0x8c0c33f6, 0xd80247c8, // add ra31, r1, 3       ; mov rb8,  ra3.8a
+/* [0x000004e8] */ 0x8c0e70b6, 0x1a024809, // add r0,   r0, r2      ; mov rb9,  ra3.8b
+/* [0x000004f0] */ 0x910cf1f6, 0xdc02480a, // shl r0,   r0, 15      ; mov rb10, ra3.8c
+/* [0x000004f8] */ 0x0c9db1c0, 0x100216a7, // add rb26, r0, rb27
+/* [0x00000500] */ 0x950c0ff6, 0xde0248cb, // mov r3, 0             ; mov rb11, ra3.8d
+/* [0x00000508] */ 0x15827d80, 0x100213a7, // mov rb14, unif
+/* [0x00000510] */ 0x15827d80, 0x100613a7, // mov.ifnz rb14, unif
+// :uvloop_b0
+/* [0x00000518] */ 0xcd5117de, 0xa00269df, // sub.setf -, r3, rb17  ; v8adds rb31, r3, ra_k1 ; ldtmu0
+/* [0x00000520] */ 0x8e4c09f6, 0x14028823, // shr r0, r4, rb_xshift2 ; mov.ifz r3, ra_y_next
+/* [0x00000528] */ 0x8e4481f6, 0xd402c863, // shr r1, r0, 8         ; mov.ifnz r3, ra_y
+/* [0x00000530] */ 0x936807f6, 0xd0029898, // max r2, r3, 0         ; mov.ifz ra_base, ra_base_next
+/* [0x00000538] */ 0x129de5c0, 0x100208a7, // min r2, r2, rb_max_y
+/* [0x00000540] */ 0x4c510797, 0x10224462, // add ra_y, r3, ra_k1   ; mul24 r2, r2, rb_pitch
+/* [0x00000548] */ 0x8c616c87, 0x10024e20, // add t0s, ra_base, r2  ; v8min r0, r0, rb_k255
+/* [0x00000550] */ 0x0000ff00, 0xe20229e7, // mov.setf -, [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]
+/* [0x00000558] */ 0x540163f0, 0x18024863, // and r1, r1, rb_k255   ; mul24      r3, ra0.8a,       r0
+/* [0x00000560] */ 0x4003f030, 0xda0049e2, // nop                   ; mul24      r2, ra0.8b << 1,  r0 << 1  @ "mul_used", 0
+/* [0x00000568] */ 0x40038031, 0xd800c9e3, // nop                   ; mul24.ifnz r3, ra0.8a << 8,  r1 << 8  @ "mul_used", 0
+/* [0x00000570] */ 0x40037031, 0xda00c9e2, // nop                   ; mul24.ifnz r2, ra0.8b << 9,  r1 << 9  @ "mul_used", 0
+/* [0x00000578] */ 0x4d03e4f0, 0xdc0248a3, // sub r2, r2, r3        ; mul24      r3, ra0.8c << 2,  r0 << 2  @ "mul_used", 0
+/* [0x00000580] */ 0x40036031, 0xdc00c9e3, // nop                   ; mul24.ifnz r3, ra0.8c << 10, r1 << 10 @ "mul_used", 0
+/* [0x00000588] */ 0x4c03d4f0, 0xde0248a3, // add r2, r2, r3        ; mul24      r3, ra0.8d << 3,  r0 << 3  @ "mul_used", 0
+/* [0x00000590] */ 0x40035031, 0xde00c9e3, // nop                   ; mul24.ifnz r3, ra0.8d << 11, r1 << 11 @ "mul_used", 0
+/* [0x00000598] */ 0x8d9df4ff, 0x10024823, // sub r0, r2, r3        ; mov r3, rb31
+/* [0x000005a0] */ 0x8d3447f6, 0xd00279cc, // sub.setf -, r3, 4     ; mov ra12, ra13
+/* [0x000005a8] */ 0xffffff50, 0xf06809e7, // brr.anyn -, r:uvloop_b0
+/* [0x000005b0] */ 0x55389db7, 0x10024361, // mov ra13, ra14        ; mul24 r1, ra14, rb9
+/* [0x000005b8] */ 0x553cadb7, 0x100243a2, // mov ra14, ra15        ; mul24 r2, ra15, rb10
+/* [0x000005c0] */ 0x55308037, 0x100243e0, // mov ra15, r0          ; mul24 r0, ra12, rb8
+/* [0x000005c8] */ 0x8d1e7236, 0x10225848, // sub r1, r1, r0        ; mov ra8.16b, ra7
+/* [0x000005d0] */ 0x4c3cb2b7, 0x10024860, // add r1, r1, r2        ; mul24 r0, ra15, rb11
+/* [0x000005d8] */ 0x8d9c623f, 0x10025847, // sub r1, r1, r0        ; mov ra7, rb6
+/* [0x000005e0] */ 0x0d7e7780, 0x100229e7, // sub.setf -, r3, ra31
+/* [0x000005e8] */ 0x8f1463f6, 0xd0124206, // asr ra8.16a, r1, 6    ; mov rb6, ra5
+/* [0x000005f0] */ 0xffffff08, 0xf06809e7, // brr.anyn -, r:uvloop_b0
+/* [0x000005f8] */ 0x95104ff6, 0x10024144, // mov ra5, rb4          ; mov rb4, ra4
+/* [0x00000600] */ 0x95185ff6, 0x10024105, // mov ra4, rb5          ; mov rb5, ra6
+/* [0x00000608] */ 0x95207ff6, 0x10024187, // mov ra6, rb7          ; mov rb7, ra8
+/* [0x00000610] */ 0x0d9cfec0, 0xd00229e7, // sub.setf -, 15, r3
+/* [0x00000618] */ 0x00000090, 0xf06809e7, // brr.anyn -, r:uv_b0_post_fin
+/* [0x00000620] */ 0x8d80bef6, 0xd00208e7, // sub r3, 11, r3        ; mov -, unif
+/* [0x00000628] */ 0x95810ff6, 0xd002581e, // mov r0, i_shift16     ; mov ra_link, unif
+/* [0x00000630] */ 0x00010000, 0xe0020867, // mov r1, 0x10000
+/* [0x00000638] */ 0x00000040, 0xf02809e7, // brr.anyz -, r:uv_b0_post12
+/* [0x00000640] */ 0x511c7c39, 0x1006c1c7, // shl.ifnz ra7, ra7, r0 ; mul24.ifnz rb7, rb7, r1
+/* [0x00000648] */ 0x51186c39, 0x1006c186, // shl.ifnz ra6, ra6, r0 ; mul24.ifnz rb6, rb6, r1
+/* [0x00000650] */ 0x51145c39, 0x1006c145, // shl.ifnz ra5, ra5, r0 ; mul24.ifnz rb5, rb5, r1
+/* [0x00000658] */ 0x51104c39, 0x10024104, // shl ra4, ra4, r0      ; mul24 rb4, rb4, r1
+/* [0x00000660] */ 0x119de7c0, 0xd00229e7, // shl.setf -, r3, i_shift30
+/* [0x00000668] */ 0x95105dbf, 0x100d81c6, // mov.ifc ra7, ra4      ; mov.ifc rb6, rb5
+/* [0x00000670] */ 0x95187dbf, 0x100d8144, // mov.ifc ra5, ra6      ; mov.ifc rb4, rb7
+/* [0x00000678] */ 0x00000030, 0xf0f809e7, // brr -, r:uv_b0_post_fin
+/* [0x00000680] */ 0x95144dbf, 0x100901c6, // mov.ifn ra7, ra5      ; mov.ifn rb6, rb4
+/* [0x00000688] */ 0x95105dbf, 0x10090144, // mov.ifn ra5, ra4      ; mov.ifn rb4, rb5
+/* [0x00000690] */ 0x95187dbf, 0x10090105, // mov.ifn ra4, ra6      ; mov.ifn rb5, rb7
+// :uv_b0_post12
+/* [0x00000698] */ 0x95187dbf, 0x100248a3, // mov r2, ra6           ; mov r3, rb7
+/* [0x000006a0] */ 0x51144c39, 0x10024187, // shl ra6, ra5, r0      ; mul24 rb7, rb4, r1
+/* [0x000006a8] */ 0x959e749b, 0x10024144, // mov ra5, r2           ; mov rb4, r3
+/* [0x000006b0] */ 0x95105dbf, 0x100248a3, // mov r2,  ra4          ; mov r3,  rb5
+/* [0x000006b8] */ 0x511c6c39, 0x10024105, // shl ra4, ra7, r0      ; mul24 rb5, rb6, r1
+/* [0x000006c0] */ 0x959e749b, 0x100241c6, // mov ra7, r2           ; mov rb6, r3
+// :uv_b0_post_fin
+/* [0x000006c8] */ 0x959a0ff6, 0x100240a0, // mov ra2, unif         ; mov r0, elem_num
+/* [0x000006d0] */ 0x0000ff00, 0xe20229e7, // mov.setf -, [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]
+/* [0x000006d8] */ 0xec0a7c09, 0x14024821, // add r0, ra2.16b, r0   ; v8subs r1, r1, r1
+/* [0x000006e0] */ 0x8d8103f6, 0x10024863, // sub r1, r1, rb_pitch  ; mov r3, unif
+/* [0x000006e8] */ 0x935c11bf, 0x10024800, // max r0, r0, ra_k0     ; mov rb_xshift2, rb_xshift2_next
+/* [0x000006f0] */ 0x928191f6, 0x10020827, // min r0, r0, rb_max_x  ; mov -, unif
+/* [0x000006f8] */ 0x119c41c0, 0xd0021067, // shl rb_xshift2_next, r0, 4
+/* [0x00000700] */ 0x9481e1f6, 0xd0025800, // and r0, r0, -2        ; mov ra0, unif
+/* [0x00000708] */ 0x8c0a7036, 0x12225815, // add r0, r0, r0        ; mov ra_y2_next, ra2.16a
+/* [0x00000710] */ 0x94827076, 0x10025843, // and r1, r0, r1        ; mov ra3, unif
+/* [0x00000718] */ 0x569d404f, 0x10024821, // xor r0, r0, r1        ; mul24 r1, r1, rb_xpitch
+/* [0x00000720] */ 0x8c0e7076, 0x18024808, // add r0, r0, r1        ; mov rb8,  ra3.8a
+/* [0x00000728] */ 0x0c9e7600, 0x100214e7, // add rb_base2_next, r3, r0
+/* [0x00000730] */ 0x950e0ff6, 0x1a024049, // mov ra1, unif         ; mov rb9,  ra3.8b
+/* [0x00000738] */ 0x950e0ff6, 0x1c06404a, // mov.ifnz ra1, unif    ; mov rb10, ra3.8c
+/* [0x00000740] */ 0x800e7036, 0x1e0049cb, // nop                   ; mov rb11, ra3.8d
+/* [0x00000748] */ 0xf104dddb, 0x14024863, // shl r1, ra1.16b, rb13 ; v8subs r3, r3, r3
+/* [0x00000750] */ 0x0f9c13c0, 0xd0021327, // asr rb12, r1, 1
+// :uvloop_b
+/* [0x00000758] */ 0xcd5117de, 0xb00269df, // sub.setf -, r3, rb17  ; v8adds rb31, r3, ra_k1 ; ldtmu1
+/* [0x00000760] */ 0x8e5409f6, 0x14028823, // shr r0, r4, rb_xshift2 ; mov.ifz r3, ra_y2_next
+/* [0x00000768] */ 0x8e5481f6, 0xd202c863, // shr r1, r0, 8         ; mov.ifnz r3, ra_y2
+/* [0x00000770] */ 0x935d37bf, 0x10029899, // max r2, r3, ra_k0     ; mov.ifz ra_base2, rb_base2_next
+/* [0x00000778] */ 0x129de5c0, 0x100208a7, // min r2, r2, rb_max_y
+/* [0x00000780] */ 0x4c510797, 0x10124562, // add ra_y2, r3, ra_k1  ; mul24 r2, r2, rb_pitch
+/* [0x00000788] */ 0x8c656c87, 0x10024f20, // add t1s, ra_base2, r2 ; v8min r0, r0, rb_k255
+/* [0x00000790] */ 0x0000ff00, 0xe20229e7, // mov.setf -, [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]
+/* [0x00000798] */ 0x540163f0, 0x18024863, // and r1, r1, rb_k255  ; mul24      r3, ra0.8a,       r0
+/* [0x000007a0] */ 0x4003f030, 0xda0049e2, // nop                  ; mul24      r2, ra0.8b << 1,  r0 << 1     @ "mul_used", 0
+/* [0x000007a8] */ 0x40038031, 0xd800c9e3, // nop                  ; mul24.ifnz r3, ra0.8a << 8,  r1 << 8     @ "mul_used", 0
+/* [0x000007b0] */ 0x40037031, 0xda00c9e2, // nop                  ; mul24.ifnz r2, ra0.8b << 9,  r1 << 9     @ "mul_used", 0
+/* [0x000007b8] */ 0x4d03e4f0, 0xdc0248a3, // sub r2, r2, r3       ; mul24      r3, ra0.8c << 2,  r0 << 2     @ "mul_used", 0
+/* [0x000007c0] */ 0x40036031, 0xdc00c9e3, // nop                  ; mul24.ifnz r3, ra0.8c << 10, r1 << 10    @ "mul_used", 0
+/* [0x000007c8] */ 0x4c03d4f0, 0xde0248a3, // add r2, r2, r3       ; mul24      r3, ra0.8d << 3,  r0 << 3     @ "mul_used", 0
+/* [0x000007d0] */ 0x40035031, 0xde00c9e3, // nop                  ; mul24.ifnz r3, ra0.8d << 11, r1 << 11    @ "mul_used", 0
+/* [0x000007d8] */ 0x8d9df4ff, 0x10024823, // sub r0, r2, r3       ; mov r3, rb31
+/* [0x000007e0] */ 0x8d3447f6, 0xd00279cc, // sub.setf -, r3, 4    ; mov ra12, ra13
+/* [0x000007e8] */ 0xffffff50, 0xf06809e7, // brr.anyn -, r:uvloop_b
+/* [0x000007f0] */ 0x55389db7, 0x10024361, // mov ra13, ra14          ; mul24 r1, ra14, rb9
+/* [0x000007f8] */ 0x553cadb7, 0x100243a2, // mov ra14, ra15          ; mul24 r2, ra15, rb10
+/* [0x00000800] */ 0x55308037, 0x100243e0, // mov ra15, r0            ; mul24 r0, ra12, rb8
+/* [0x00000808] */ 0x8d1e7236, 0x10225848, // sub r1, r1, r0        ; mov ra8.16b, ra7
+/* [0x00000810] */ 0x4c3cb2b7, 0x10024860, // add r1, r1, r2        ; mul24 r0, ra15, rb11
+/* [0x00000818] */ 0x4d1ce237, 0x14024860, // sub r1, r1, r0        ; mul24 r0, ra7.16b, rb14
+/* [0x00000820] */ 0x55586fce, 0x100241e1, // mov ra7, rb6          ; mul24 r1, r1, ra_k256
+/* [0x00000828] */ 0x8f14e3f6, 0xd0024846, // asr r1, r1, 14        ; mov rb6, ra5
+/* [0x00000830] */ 0x55044fce, 0x12024161, // mov ra5, rb4          ; mul24 r1, r1, ra1.16a
+/* [0x00000838] */ 0x8c127236, 0x10024844, // add r1, r1, r0        ; mov rb4, ra4
+/* [0x00000840] */ 0x55585fce, 0x10024121, // mov ra4, rb5          ; mul24 r1, r1, ra_k256
+/* [0x00000848] */ 0x8c18c3f6, 0x10024845, // add r1, r1, rb12      ; mov rb5, ra6
+/* [0x00000850] */ 0x8d7c77bf, 0x100279c6, // sub.setf -, r3, ra31  ; mov ra6, rb7
+/* [0x00000858] */ 0x0f9cd3c0, 0x10c200e7, // asr ra3.8as, r1, rb13
+/* [0x00000860] */ 0x809f8009, 0xd00049e1, // nop                   ; mov r1, r1 << 8
+/* [0x00000868] */ 0xfffffed0, 0xf06809e7, // brr.anyn -, r:uvloop_b
+/* [0x00000870] */ 0x0f9cd3c0, 0x10d200e7, // asr ra3.8bs, r1, rb13
+/* [0x00000878] */ 0x95232ff6, 0x100049c7, // mov -, vw_wait        ; mov rb7, ra8
+/* [0x00000880] */ 0x150e7d80, 0x10020c27, // mov vpm, ra3
+/* [0x00000888] */ 0x00000000, 0xf0f7c9e7, // bra -, ra_link
+/* [0x00000890] */ 0x159dafc0, 0x10021c67, // mov vw_setup, rb26
+/* [0x00000898] */ 0x159ddfc0, 0x10021c67, // mov vw_setup, rb29
+/* [0x000008a0] */ 0x15827d80, 0x10021ca7, // mov vw_addr, unif
+// ::mc_interrupt_exit8c
+/* [0x000008a8] */ 0x009e7000, 0xa00009e7, // ldtmu0
+/* [0x000008b0] */ 0x009e7000, 0xb00009e7, // ldtmu1
+/* [0x000008b8] */ 0x009e7000, 0xb00009e7, // ldtmu1
+/* [0x000008c0] */ 0x159f2fc0, 0xa00009e7, // mov  -, vw_wait ; nop ; ldtmu0
+/* [0x000008c8] */ 0x00000010, 0xe80009e7, // mov -,sacq(0)
+/* [0x000008d0] */ 0x00000010, 0xe80009e7, // mov -,sacq(0)
+/* [0x000008d8] */ 0x00000010, 0xe80009e7, // mov -,sacq(0)
+/* [0x000008e0] */ 0x00000010, 0xe80009e7, // mov -,sacq(0)
+/* [0x000008e8] */ 0x00000010, 0xe80009e7, // mov -,sacq(0)
+/* [0x000008f0] */ 0x00000010, 0xe80009e7, // mov -,sacq(0)
+/* [0x000008f8] */ 0x00000010, 0xe80009e7, // mov -,sacq(0)
+/* [0x00000900] */ 0x009e7000, 0x300009e7, // nop        ; nop ; thrend
+/* [0x00000908] */ 0x00000001, 0xe00209a7, // mov interrupt, 1; nop
+/* [0x00000910] */ 0x009e7000, 0x100009e7, // nop        ; nop
+// ::mc_exit
+// ::mc_exit_c
+/* [0x00000918] */ 0x009e7000, 0xa00009e7, // ldtmu0
+/* [0x00000920] */ 0x009e7000, 0xb00009e7, // ldtmu1
+/* [0x00000928] */ 0x009e7000, 0xa00009e7, // ldtmu0
+/* [0x00000930] */ 0x159f2fc0, 0xb00009e7, // mov  -, vw_wait ; nop ; ldtmu1
+/* [0x00000938] */ 0x00000000, 0xe80009e7, // mov -,srel(0)
+/* [0x00000940] */ 0x009e7000, 0x300009e7, // nop        ; nop ; thrend
+/* [0x00000948] */ 0x009e7000, 0x100009e7, // nop        ; nop
+/* [0x00000950] */ 0x009e7000, 0x100009e7, // nop        ; nop
+// ::mc_interrupt_exit12
+/* [0x00000958] */ 0x009e7000, 0xa00009e7, // ldtmu0
+/* [0x00000960] */ 0x009e7000, 0xb00009e7, // ldtmu1
+/* [0x00000968] */ 0x009e7000, 0xa00009e7, // ldtmu0
+/* [0x00000970] */ 0x159f2fc0, 0xb00009e7, // mov  -, vw_wait ; nop ; ldtmu1
+/* [0x00000978] */ 0x00000010, 0xe80009e7, // mov -,sacq(0)
+/* [0x00000980] */ 0x00000010, 0xe80009e7, // mov -,sacq(0)
+/* [0x00000988] */ 0x00000010, 0xe80009e7, // mov -,sacq(0)
+/* [0x00000990] */ 0x00000010, 0xe80009e7, // mov -,sacq(0)
+/* [0x00000998] */ 0x00000010, 0xe80009e7, // mov -,sacq(0)
+/* [0x000009a0] */ 0x00000010, 0xe80009e7, // mov -,sacq(0)
+/* [0x000009a8] */ 0x00000010, 0xe80009e7, // mov -,sacq(0)
+/* [0x000009b0] */ 0x00000010, 0xe80009e7, // mov -,sacq(0)
+/* [0x000009b8] */ 0x00000010, 0xe80009e7, // mov -,sacq(0)
+/* [0x000009c0] */ 0x00000010, 0xe80009e7, // mov -,sacq(0)
+/* [0x000009c8] */ 0x00000010, 0xe80009e7, // mov -,sacq(0)
+/* [0x000009d0] */ 0x009e7000, 0x300009e7, // nop        ; nop ; thrend
+/* [0x000009d8] */ 0x00000001, 0xe00209a7, // mov interrupt, 1; nop
+/* [0x000009e0] */ 0x009e7000, 0x100009e7, // nop        ; nop
+// ::mc_exit1
+/* [0x000009e8] */ 0x159f2fc0, 0x100009e7, // mov  -, vw_wait
+/* [0x000009f0] */ 0x009e7000, 0xa00009e7, // ldtmu0
+/* [0x000009f8] */ 0x009e7000, 0xb00009e7, // ldtmu1
+/* [0x00000a00] */ 0x009e7000, 0xa00009e7, // ldtmu0
+/* [0x00000a08] */ 0x009e7000, 0xb00009e7, // ldtmu1
+/* [0x00000a10] */ 0x009e7000, 0x300009e7, // nop        ; nop ; thrend
+/* [0x00000a18] */ 0x00000001, 0xe00209a7, // mov interrupt, 1; nop
+/* [0x00000a20] */ 0x009e7000, 0x100009e7, // nop        ; nop
+// ::mc_setup
+/* [0x00000a28] */ 0x95801ff6, 0xd0025908, // mov tmurs, 1          ; mov ra8, unif
+/* [0x00000a30] */ 0x15827d80, 0x10020267, // mov ra9, unif
+/* [0x00000a38] */ 0x15827d80, 0x100202a7, // mov ra10, unif
+/* [0x00000a40] */ 0x15827d80, 0x100202e7, // mov ra11, unif
+/* [0x00000a48] */ 0x15827d80, 0x100200e7, // mov ra3, unif
+/* [0x00000a50] */ 0x15827d80, 0x10021527, // mov rb_xpitch, unif
+/* [0x00000a58] */ 0x0d0c1dc0, 0xd4021667, // sub rb_max_x, ra3.16b, 1
+/* [0x00000a60] */ 0x0d0c1dc0, 0xd20217a7, // sub rb_max_y, ra3.16a, 1
+/* [0x00000a68] */ 0x15827d80, 0x10021427, // mov rb_pitch, unif
+/* [0x00000a70] */ 0xc0000000, 0xe0020867, // mov r1, vdw_setup_1(0)
+/* [0x00000a78] */ 0x159d03c0, 0x10021627, // or  rb24, r1, rb_pitch
+/* [0x00000a80] */ 0x159a7d80, 0x100208e7, // mov r3, elem_num
+/* [0x00000a88] */ 0x0c227cc0, 0x12020827, // add r0, ra8.16a, r3
+/* [0x00000a90] */ 0x139c01c0, 0xd0020827, // max r0, r0, 0
+/* [0x00000a98] */ 0x129d91c0, 0x10020827, // min r0, r0, rb_max_x
+/* [0x00000aa0] */ 0x119c31c0, 0xd01204e7, // shl ra_xshift_next, r0, 3
+/* [0x00000aa8] */ 0xf49dc1d2, 0xd0024822, // and r0, r0, -4        ; v8subs r2, r2, r2
+/* [0x00000ab0] */ 0x0d9d05c0, 0x100208a7, // sub r2, r2, rb_pitch
+/* [0x00000ab8] */ 0x149e7080, 0x10020867, // and r1, r0, r2
+/* [0x00000ac0] */ 0x569d404f, 0x10024821, // xor r0, r0, r1        ; mul24 r1, r1, rb_xpitch
+/* [0x00000ac8] */ 0x0c9e7040, 0x10020827, // add r0, r0, r1
+/* [0x00000ad0] */ 0x0c267c00, 0x10020627, // add ra_base, ra9, r0
+/* [0x00000ad8] */ 0x15227d80, 0x14020867, // mov r1, ra8.16b
+/* [0x00000ae0] */ 0x0c9c13c0, 0xd0220467, // add ra_y, r1, 1
+/* [0x00000ae8] */ 0x139c03c0, 0xd0020867, // max r1, r1, 0
+/* [0x00000af0] */ 0x129de3c0, 0x10020867, // min r1, r1, rb_max_y
+/* [0x00000af8] */ 0x409d000f, 0x100049e1, // nop                   ; mul24 r1, r1, rb_pitch
+/* [0x00000b00] */ 0x0c627c40, 0x10020e27, // add t0s, ra_base, r1
+/* [0x00000b08] */ 0x0c2a7cc0, 0x12020827, // add r0, ra10.16a, r3
+/* [0x00000b10] */ 0x139c01c0, 0xd0020827, // max r0, r0, 0
+/* [0x00000b18] */ 0x129d91c0, 0x10020827, // min r0, r0, rb_max_x
+/* [0x00000b20] */ 0x119c31c0, 0xd0021067, // shl rb_xshift2_next, r0, 3
+/* [0x00000b28] */ 0x149dc1c0, 0xd0020827, // and r0, r0, -4
+/* [0x00000b30] */ 0x149e7080, 0x10020867, // and r1, r0, r2
+/* [0x00000b38] */ 0x569d404f, 0x10024821, // xor r0, r0, r1        ; mul24 r1, r1, rb_xpitch
+/* [0x00000b40] */ 0x0c9e7040, 0x10020827, // add r0, r0, r1
+/* [0x00000b48] */ 0x0c2e7c00, 0x10020667, // add ra_base2, ra11, r0
+/* [0x00000b50] */ 0x152a7d80, 0x14020867, // mov r1, ra10.16b
+/* [0x00000b58] */ 0x0c9c13c0, 0xd0120567, // add ra_y2, r1, 1
+/* [0x00000b60] */ 0x139c03c0, 0xd0020867, // max r1, r1, 0
+/* [0x00000b68] */ 0x129de3c0, 0x10020867, // min r1, r1, rb_max_y
+/* [0x00000b70] */ 0x409d000f, 0x100049e1, // nop                   ; mul24 r1, r1, rb_pitch
+/* [0x00000b78] */ 0x0c667c40, 0x10020f27, // add t1s, ra_base2, r1
+/* [0x00000b80] */ 0x00000001, 0xe0020527, // mov ra_k1, 1
+/* [0x00000b88] */ 0x00000100, 0xe00205a7, // mov ra_k256, 256
+/* [0x00000b90] */ 0x000000ff, 0xe00215a7, // mov rb_k255, 255
+/* [0x00000b98] */ 0x00000000, 0xe00205e7, // mov ra_k0, 0
+/* [0x00000ba0] */ 0x00000000, 0xe0024208, // mov ra8,  0           ; mov rb8,  0
+/* [0x00000ba8] */ 0x00000000, 0xe0024249, // mov ra9,  0           ; mov rb9,  0
+/* [0x00000bb0] */ 0x00000000, 0xe002428a, // mov ra10, 0           ; mov rb10, 0
+/* [0x00000bb8] */ 0x00000000, 0xe00242cb, // mov ra11, 0           ; mov rb11, 0
+/* [0x00000bc0] */ 0x159e6fc0, 0x100208a7, // mov r2, qpu_num
+/* [0x00000bc8] */ 0x0f9c25c0, 0xd0020867, // asr r1, r2, 2
+/* [0x00000bd0] */ 0x119c63c0, 0xd0020867, // shl r1, r1, 6
+/* [0x00000bd8] */ 0x149c35c0, 0xd0020827, // and r0, r2, 3
+/* [0x00000be0] */ 0x159e7040, 0x10020827, // or  r0, r0, r1
+/* [0x00000be8] */ 0x00004800, 0xe0020867, // mov r1, vpm_setup(0, 4, h8p(0, 0))
+/* [0x00000bf0] */ 0x0c9e7040, 0x10021727, // add r_vpm, r0, r1
+/* [0x00000bf8] */ 0x80004004, 0xe0020867, // mov r1, vdw_setup_0(0, 0, dma_h8p(0,0,0))
+/* [0x00000c00] */ 0x119c51c0, 0xd0020827, // shl r0, r0, 5
+/* [0x00000c08] */ 0x0c9e7040, 0x100216e7, // add r_dma, r0, r1
+/* [0x00000c10] */ 0x0c809dc0, 0xd0021367, // add rb13, unif, 9
+/* [0x00000c18] */ 0x13440dc0, 0xd4020867, // max r1, ra_y, 0
+/* [0x00000c20] */ 0x129de3c0, 0x10020867, // min r1, r1, rb_max_y
+/* [0x00000c28] */ 0x0c441dc0, 0xd4220467, // add ra_y, ra_y, 1
+/* [0x00000c30] */ 0x55810d8f, 0x100049e1, // mov -, unif           ; mul24 r1, r1, rb_pitch
+/* [0x00000c38] */ 0x0c627380, 0x10020e27, // add t0s, r1, ra_base
+/* [0x00000c40] */ 0x13540dc0, 0xd2020867, // max r1, ra_y2, 0
+/* [0x00000c48] */ 0x129de3c0, 0x10020867, // min r1, r1, rb_max_y
+/* [0x00000c50] */ 0x0c541dc0, 0xd2120567, // add ra_y2, ra_y2, 1
+/* [0x00000c58] */ 0x409d000f, 0x100049e1, // nop                   ; mul24 r1, r1, rb_pitch
+/* [0x00000c60] */ 0x0c667380, 0x10020f27, // add t1s, r1, ra_base2
+// :per_block_setup
+/* [0x00000c68] */ 0x0000ff00, 0xe20229e7, // mov.setf -, [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]
+/* [0x00000c70] */ 0x15827d80, 0x100207a7, // mov ra_link, unif
+/* [0x00000c78] */ 0x959a0ff6, 0x10024063, // mov ra1, unif         ; mov r3, elem_num
+/* [0x00000c80] */ 0x154e7d80, 0x12120467, // mov ra_xshift, ra_xshift_next
+/* [0x00000c88] */ 0x159c1fc0, 0x10021027, // mov rb_xshift2, rb_xshift2_next
+/* [0x00000c90] */ 0x0c067cc0, 0x12020827, // add r0, ra1.16a, r3
+/* [0x00000c98] */ 0x139c01c0, 0xd0020827, // max r0, r0, 0
+/* [0x00000ca0] */ 0x129d91c0, 0x10020827, // min r0, r0, rb_max_x
+/* [0x00000ca8] */ 0x119c31c0, 0xd01204e7, // shl ra_xshift_next, r0, 3
+/* [0x00000cb0] */ 0xf49dc1d2, 0xd0024822, // and r0, r0, -4        ; v8subs r2, r2, r2
+/* [0x00000cb8] */ 0x0d9d05c0, 0x100208a7, // sub r2, r2, rb_pitch
+/* [0x00000cc0] */ 0x149e7080, 0x10020867, // and r1, r0, r2
+/* [0x00000cc8] */ 0x569d404f, 0x10024821, // xor r0, r0, r1        ; mul24 r1, r1, rb_xpitch
+/* [0x00000cd0] */ 0x0c9e7040, 0x10020827, // add r0, r0, r1
+/* [0x00000cd8] */ 0x0c827c00, 0x100206a7, // add ra_base_next, unif, r0
+/* [0x00000ce0] */ 0x15067d80, 0x142204e7, // mov ra_y_next, ra1.16b
+/* [0x00000ce8] */ 0x15827d80, 0x10020067, // mov ra1, unif
+/* [0x00000cf0] */ 0x009e7000, 0x100009e7, // nop
+/* [0x00000cf8] */ 0x0c067cc0, 0x12020827, // add r0, ra1.16a, r3
+/* [0x00000d00] */ 0x139c01c0, 0xd0020827, // max r0, r0, 0
+/* [0x00000d08] */ 0x129d91c0, 0x10020827, // min r0, r0, rb_max_x
+/* [0x00000d10] */ 0x119c31c0, 0xd0021067, // shl rb_xshift2_next, r0, 3
+/* [0x00000d18] */ 0x149dc1c0, 0xd0020827, // and r0, r0, -4
+/* [0x00000d20] */ 0x149e7080, 0x10020867, // and r1, r0, r2
+/* [0x00000d28] */ 0x569d404f, 0x10024821, // xor r0, r0, r1        ; mul24 r1, r1, rb_xpitch
+/* [0x00000d30] */ 0x0c9e7040, 0x10020827, // add r0, r0, r1
+/* [0x00000d38] */ 0x0c827c00, 0x100214e7, // add rb_base2_next, unif, r0
+/* [0x00000d40] */ 0x15067d80, 0x14220567, // mov ra_y2_next, ra1.16b
+/* [0x00000d48] */ 0x15827d80, 0x10020427, // mov ra_width_height, unif
+/* [0x00000d50] */ 0x159dcfc0, 0x10021c67, // mov vw_setup, rb28
+/* [0x00000d58] */ 0x0d418f80, 0x14021767, // sub rb29, rb24, ra_width
+/* [0x00000d60] */ 0x8c405df6, 0xd2025460, // add rb17, ra_height, 5  ; mov r0, ra_height
+/* [0x00000d68] */ 0x00000010, 0xe0020867, // mov r1, 16
+/* [0x00000d70] */ 0x129e7040, 0x10020827, // min r0, r0, r1
+/* [0x00000d78] */ 0x0c9c71c0, 0xd00214a7, // add rb18, r0, 7
+/* [0x00000d80] */ 0x119c71c0, 0xd0020827, // shl r0,   r0, 7
+/* [0x00000d88] */ 0x0c427180, 0x14020827, // add r0,   r0, ra_width
+/* [0x00000d90] */ 0x119d01c0, 0xd0020827, // shl r0,   r0, i_shift16
+/* [0x00000d98] */ 0x8c81b1f6, 0x100256a0, // add rb26, r0, rb27                 ; mov r0, unif
+/* [0x00000da0] */ 0x918101f6, 0xd0045805, // shl.ifz r0, r0, i_shift16          ; mov ra5, unif
+/* [0x00000da8] */ 0x01040400, 0xe00208a7, // mov r2, 0x01040400
+/* [0x00000db0] */ 0x911431f6, 0xd202420e, // shl ra8, r0, 3                     ; mov rb14, ra5.16a
+/* [0x00000db8] */ 0x00010100, 0xe0020867, // mov r1,0x00010100
+/* [0x00000dc0] */ 0x10227380, 0x1e4200a7, // ror ra2.8a, r1, ra8.8d
+/* [0x00000dc8] */ 0x10227380, 0x1c420027, // ror ra0.8a, r1, ra8.8c
+/* [0x00000dd0] */ 0x10227580, 0x1e5200a7, // ror ra2.8b, r2, ra8.8d
+/* [0x00000dd8] */ 0x10227580, 0x1c520027, // ror ra0.8b, r2, ra8.8c
+/* [0x00000de0] */ 0x050b0a00, 0xe0020867, // mov r1,0x050b0a00
+/* [0x00000de8] */ 0x10227380, 0x1e6200a7, // ror ra2.8c, r1, ra8.8d
+/* [0x00000df0] */ 0x10227380, 0x1c620027, // ror ra0.8c, r1, ra8.8c
+/* [0x00000df8] */ 0x11283a40, 0xe0020867, // mov r1,0x11283a40
+/* [0x00000e00] */ 0x10227380, 0x1e7200a7, // ror ra2.8d, r1, ra8.8d
+/* [0x00000e08] */ 0x10227380, 0x1c720027, // ror ra0.8d, r1, ra8.8c
+/* [0x00000e10] */ 0x3a281100, 0xe0020867, // mov r1,0x3a281100
+/* [0x00000e18] */ 0x10227380, 0x1e4200e7, // ror ra3.8a, r1, ra8.8d
+/* [0x00000e20] */ 0x10227380, 0x1c420067, // ror ra1.8a, r1, ra8.8c
+/* [0x00000e28] */ 0x0a0b0500, 0xe0020867, // mov r1,0x0a0b0500
+/* [0x00000e30] */ 0x10227380, 0x1e5200e7, // ror ra3.8b, r1, ra8.8d
+/* [0x00000e38] */ 0x10227380, 0x1c520067, // ror ra1.8b, r1, ra8.8c
+/* [0x00000e40] */ 0x04040100, 0xe0020867, // mov r1,0x04040100
+/* [0x00000e48] */ 0x10227380, 0x1e6200e7, // ror ra3.8c, r1, ra8.8d
+/* [0x00000e50] */ 0x10227380, 0x1c620067, // ror ra1.8c, r1, ra8.8c
+/* [0x00000e58] */ 0x01010000, 0xe0020867, // mov r1,0x01010000
+/* [0x00000e60] */ 0x10227380, 0x1e7200e7, // ror ra3.8d, r1, ra8.8d
+/* [0x00000e68] */ 0x10227380, 0x1c720067, // ror ra1.8d, r1, ra8.8c
+/* [0x00000e70] */ 0x950e0dbf, 0x18025112, // mov rb4, ra3.8a            ; mov ra18, unif
+/* [0x00000e78] */ 0x150e7d80, 0x1a021167, // mov rb5, ra3.8b
+/* [0x00000e80] */ 0x150e7d80, 0x1c0211a7, // mov rb6, ra3.8c
+/* [0x00000e88] */ 0x154a7d80, 0x10060167, // mov.ifnz ra5, ra18
+/* [0x00000e90] */ 0x15827d80, 0x100215e7, // mov rb_dest, unif
+/* [0x00000e98] */ 0x00000000, 0xf0f7c9e7, // bra -, ra_link
+/* [0x00000ea0] */ 0x1114ddc0, 0x14020827, // shl r0, ra5.16b, rb13
+/* [0x00000ea8] */ 0x0f9c91c0, 0xd0021327, // asr rb12, r0, 9
+/* [0x00000eb0] */ 0x950c0ff6, 0xde0248c7, // mov r3, 0                  ; mov rb7, ra3.8d
+// ::mc_filter
+/* [0x00000eb8] */ 0x11141dc0, 0xd20213a7, // shl rb14, ra5.16a, 1
+// :yloop
+/* [0x00000ec0] */ 0xcd5117de, 0xa00269e3, // sub.setf -, r3, rb17      ; v8adds r3, r3, ra_k1                           ; ldtmu0
+/* [0x00000ec8] */ 0x8e4539bf, 0xb2029819, // shr r0, r4, ra_xshift     ; mov.ifz ra_base2, rb_base2_next    ; ldtmu1
+/* [0x00000ed0] */ 0x956a7d9b, 0x1004461f, // mov.ifz ra_base, ra_base_next ; mov rb31, r3
+/* [0x00000ed8] */ 0x954d0dbf, 0x14244463, // mov.ifz ra_y, ra_y_next   ; mov r3, rb_pitch
+/* [0x00000ee0] */ 0x8e5409f6, 0x14129855, // shr r1, r4, rb_xshift2    ; mov.ifz ra_y2, ra_y2_next
+/* [0x00000ee8] */ 0x13440dc0, 0xd40208a7, // max r2, ra_y, 0
+/* [0x00000ef0] */ 0x129de5c0, 0x100208a7, // min r2, r2, rb_max_y
+/* [0x00000ef8] */ 0x4c441dd3, 0xd4224462, // add ra_y, ra_y, 1            ; mul24 r2, r2, r3
+/* [0x00000f00] */ 0x8c616c87, 0x10024e20, // add t0s, ra_base, r2   ; v8min r0, r0, rb_k255
+/* [0x00000f08] */ 0x13540dc0, 0xd20208a7, // max r2, ra_y2, 0
+/* [0x00000f10] */ 0x129de5c0, 0x100208a7, // min r2, r2, rb_max_y
+/* [0x00000f18] */ 0x4c541dd3, 0xd2124562, // add ra_y2, ra_y2, 1          ; mul24 r2, r2, r3
+/* [0x00000f20] */ 0x8c656c8f, 0x10024f21, // add t1s, ra_base2, r2  ; v8min r1, r1, rb_k255
+/* [0x00000f28] */ 0x0000ff00, 0xe20229e7, // mov.setf -, [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]
+/* [0x00000f30] */ 0x40027030, 0x180049e3, // nop                  ; mul24      r3, ra0.8a,      r0
+/* [0x00000f38] */ 0x40038031, 0xd800c9e3, // nop                  ; mul24.ifnz r3, ra0.8a << 8, r1 << 8    @ "mul_used", 0
+/* [0x00000f40] */ 0x4003f030, 0xda0049e2, // nop                  ; mul24      r2, ra0.8b << 1, r0 << 1    @ "mul_used", 0
+/* [0x00000f48] */ 0x40037031, 0xda00c9e2, // nop                  ; mul24.ifnz r2, ra0.8b << 9, r1 << 9    @ "mul_used", 0
+/* [0x00000f50] */ 0x4d03e4f0, 0xdc0248a3, // sub r2, r2, r3       ; mul24      r3, ra0.8c << 2, r0 << 2    @ "mul_used", 0
+/* [0x00000f58] */ 0x40036031, 0xdc00c9e3, // nop                  ; mul24.ifnz r3, ra0.8c << 10, r1 << 10  @ "mul_used", 0
+/* [0x00000f60] */ 0x4d03d4f0, 0xde0248a3, // sub r2, r2, r3       ; mul24      r3, ra0.8d << 3, r0 << 3    @ "mul_used", 0
+/* [0x00000f68] */ 0x40035031, 0xde00c9e3, // nop                  ; mul24.ifnz r3, ra0.8d << 11, r1 << 11  @ "mul_used", 0
+/* [0x00000f70] */ 0x4c07c4f0, 0xd80248a3, // add r2, r2, r3       ; mul24      r3, ra1.8a << 4, r0 << 4    @ "mul_used", 0
+/* [0x00000f78] */ 0x40074031, 0xd800c9e3, // nop                  ; mul24.ifnz r3, ra1.8a << 12, r1 << 12  @ "mul_used", 0
+/* [0x00000f80] */ 0x4c07b4f0, 0xda0248a3, // add r2, r2, r3       ; mul24      r3, ra1.8b << 5, r0 << 5    @ "mul_used", 0
+/* [0x00000f88] */ 0x40073031, 0xda00c9e3, // nop                  ; mul24.ifnz r3, ra1.8b << 13, r1 << 13  @ "mul_used", 0
+/* [0x00000f90] */ 0x4d07a4f0, 0xdc0248a3, // sub r2, r2, r3       ; mul24      r3, ra1.8c << 6, r0 << 6    @ "mul_used", 0
+/* [0x00000f98] */ 0x40072031, 0xdc00c9e3, // nop                  ; mul24.ifnz r3, ra1.8c << 14, r1 << 14  @ "mul_used", 0
+/* [0x00000fa0] */ 0x4c0794f0, 0xde0248a3, // add r2, r2, r3       ; mul24      r3, ra1.8d << 7, r0 << 7    @ "mul_used", 0
+/* [0x00000fa8] */ 0x40071031, 0xde00c9e3, // nop                  ; mul24.ifnz r3, ra1.8d << 15, r1 << 15  @ "mul_used", 0
+/* [0x00000fb0] */ 0x8d9df4ff, 0x10024823, // sub r0, r2, r3       ; mov r3, rb31
+/* [0x00000fb8] */ 0x8d2087f6, 0xd00269e1, // sub.setf -, r3, 8       ; mov r1,   ra8
+/* [0x00000fc0] */ 0x95249dbf, 0x10024208, // mov ra8,  ra9           ; mov rb8,  rb9
+/* [0x00000fc8] */ 0xfffffed8, 0xf06809e7, // brr.anyn -, r:yloop
+/* [0x00000fd0] */ 0x9528adbf, 0x10024249, // mov ra9,  ra10          ; mov rb9,  rb10
+/* [0x00000fd8] */ 0x952cbdbf, 0x1002428a, // mov ra10, ra11          ; mov rb10, rb11
+/* [0x00000fe0] */ 0x959e7009, 0x100242cb, // mov ra11, r0            ; mov rb11, r1
+/* [0x00000fe8] */ 0x4008803e, 0x180049e0, // nop                     ; mul24 r0, rb8,  ra2.8a
+/* [0x00000ff0] */ 0x4008903e, 0x1a0049e1, // nop                     ; mul24 r1, rb9,  ra2.8b
+/* [0x00000ff8] */ 0x4d08a23e, 0x1c024860, // sub r1, r1, r0          ; mul24 r0, rb10, ra2.8c
+/* [0x00001000] */ 0x4d08b23e, 0x1e024860, // sub r1, r1, r0          ; mul24 r0, rb11, ra2.8d
+/* [0x00001008] */ 0x4c204237, 0x10024860, // add r1, r1, r0          ; mul24 r0, ra8,  rb4
+/* [0x00001010] */ 0x4c245237, 0x10024860, // add r1, r1, r0          ; mul24 r0, ra9,  rb5
+/* [0x00001018] */ 0x4d286237, 0x10024860, // sub r1, r1, r0          ; mul24 r0, ra10, rb6
+/* [0x00001020] */ 0x4c2c7237, 0x10024860, // add r1, r1, r0          ; mul24 r0, ra11, rb7
+/* [0x00001028] */ 0x8d9f223f, 0x10020867, // sub r1, r1, r0          ; mov -, vw_wait
+/* [0x00001030] */ 0x4d5927ce, 0x100269e1, // sub.setf -, r3, rb18    ; mul24 r1, r1, ra_k256
+/* [0x00001038] */ 0x0f9ce3c0, 0xd0020867, // asr r1, r1, 14
+/* [0x00001040] */ 0x409ce00f, 0x100049e1, // nop                     ; mul24 r1, r1, rb14
+/* [0x00001048] */ 0x0c9cc3c0, 0x10020867, // add r1, r1, rb12
+/* [0x00001050] */ 0x119c83c0, 0xd0020867, // shl r1, r1, 8
+/* [0x00001058] */ 0xfffffe48, 0xf06809e7, // brr.anyn -, r:yloop
+/* [0x00001060] */ 0x0f9cd3c0, 0x10020867, // asr r1, r1, rb13
+/* [0x00001068] */ 0x129d63c0, 0x10020867, // min r1, r1, rb_k255
+/* [0x00001070] */ 0x139c03c0, 0xd0020c27, // max vpm, r1, 0
+/* [0x00001078] */ 0x00000010, 0xe0020867, // mov r1, 16
+/* [0x00001080] */ 0x0d427c40, 0x12020827, // sub r0, ra_height, r1
+/* [0x00001088] */ 0x159e7000, 0x10120427, // mov ra_height, r0
+/* [0x00001090] */ 0x139c01c0, 0xd0022827, // max.setf r0, r0, 0
+/* [0x00001098] */ 0xfffffbb0, 0xf02809e7, // brr.anyz -, r:per_block_setup
+/* [0x000010a0] */ 0x159dafc0, 0x10021c67, // mov vw_setup, rb26
+/* [0x000010a8] */ 0x159ddfc0, 0x10021c67, // mov vw_setup, rb29
+/* [0x000010b0] */ 0x159d7fc0, 0x10021ca7, // mov vw_addr, rb_dest
+/* [0x000010b8] */ 0x129e7040, 0x10020827, // min r0, r0, r1
+/* [0x000010c0] */ 0x0c9d2e00, 0x100214a7, // add rb18, rb18, r0
+/* [0x000010c8] */ 0x0d9e7040, 0x10020827, // sub r0, r0, r1
+/* [0x000010d0] */ 0x119d71c0, 0xd0020827, // shl r0, r0, i_shift23
+/* [0x000010d8] */ 0x0c9dae00, 0x100216a7, // add rb26, rb26, r0
+/* [0x000010e0] */ 0x409d000f, 0x100049e0, // nop ; mul24 r0, r1, rb_pitch
+/* [0x000010e8] */ 0x0c9d7e00, 0x100215e7, // add rb_dest, rb_dest, r0
+/* [0x000010f0] */ 0x159dcfc0, 0x10021c67, // mov vw_setup, rb28
+/* [0x000010f8] */ 0xfffffda8, 0xf0f809e7, // brr -, r:yloop
+/* [0x00001100] */ 0x009e7000, 0x100009e7, // nop
+/* [0x00001108] */ 0x009e7000, 0x100009e7, // nop
+/* [0x00001110] */ 0x009e7000, 0x100009e7, // nop
+// ::mc_filter_b
+// :yloopb
+/* [0x00001118] */ 0xcd5117de, 0xa00269e3, // sub.setf -, r3, rb17      ; v8adds r3, r3, ra_k1                           ; ldtmu0
+/* [0x00001120] */ 0x8e4539bf, 0xb2029819, // shr r0, r4, ra_xshift     ; mov.ifz ra_base2, rb_base2_next    ; ldtmu1
+/* [0x00001128] */ 0x956a7d9b, 0x1004461f, // mov.ifz ra_base, ra_base_next ; mov rb31, r3
+/* [0x00001130] */ 0x954d0dbf, 0x14244463, // mov.ifz ra_y, ra_y_next   ; mov r3, rb_pitch
+/* [0x00001138] */ 0x8e5409f6, 0x14129855, // shr r1, r4, rb_xshift2    ; mov.ifz ra_y2, ra_y2_next
+/* [0x00001140] */ 0x13440dc0, 0xd40208a7, // max r2, ra_y, 0
+/* [0x00001148] */ 0x129de5c0, 0x100208a7, // min r2, r2, rb_max_y
+/* [0x00001150] */ 0x4c441dd3, 0xd4224462, // add ra_y, ra_y, 1            ; mul24 r2, r2, r3
+/* [0x00001158] */ 0x8c616c87, 0x10024e20, // add t0s, ra_base, r2   ; v8min r0, r0, rb_k255
+/* [0x00001160] */ 0x13540dc0, 0xd20208a7, // max r2, ra_y2, 0
+/* [0x00001168] */ 0x129de5c0, 0x100208a7, // min r2, r2, rb_max_y
+/* [0x00001170] */ 0x4c541dd3, 0xd2124562, // add ra_y2, ra_y2, 1          ; mul24 r2, r2, r3
+/* [0x00001178] */ 0x8c656c8f, 0x10024f21, // add t1s, ra_base2, r2  ; v8min r1, r1, rb_k255
+/* [0x00001180] */ 0x0000ff00, 0xe20229e7, // mov.setf -, [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]
+/* [0x00001188] */ 0x40027030, 0x180049e3, // nop                  ; mul24      r3, ra0.8a,      r0
+/* [0x00001190] */ 0x40038031, 0xd800c9e3, // nop                  ; mul24.ifnz r3, ra0.8a << 8, r1 << 8    @ "mul_used", 0
+/* [0x00001198] */ 0x4003f030, 0xda0049e2, // nop                  ; mul24      r2, ra0.8b << 1, r0 << 1    @ "mul_used", 0
+/* [0x000011a0] */ 0x40037031, 0xda00c9e2, // nop                  ; mul24.ifnz r2, ra0.8b << 9, r1 << 9    @ "mul_used", 0
+/* [0x000011a8] */ 0x4d03e4f0, 0xdc0248a3, // sub r2, r2, r3       ; mul24      r3, ra0.8c << 2, r0 << 2    @ "mul_used", 0
+/* [0x000011b0] */ 0x40036031, 0xdc00c9e3, // nop                  ; mul24.ifnz r3, ra0.8c << 10, r1 << 10  @ "mul_used", 0
+/* [0x000011b8] */ 0x4d03d4f0, 0xde0248a3, // sub r2, r2, r3       ; mul24      r3, ra0.8d << 3, r0 << 3    @ "mul_used", 0
+/* [0x000011c0] */ 0x40035031, 0xde00c9e3, // nop                  ; mul24.ifnz r3, ra0.8d << 11, r1 << 11  @ "mul_used", 0
+/* [0x000011c8] */ 0x4c07c4f0, 0xd80248a3, // add r2, r2, r3       ; mul24      r3, ra1.8a << 4, r0 << 4    @ "mul_used", 0
+/* [0x000011d0] */ 0x40074031, 0xd800c9e3, // nop                  ; mul24.ifnz r3, ra1.8a << 12, r1 << 12  @ "mul_used", 0
+/* [0x000011d8] */ 0x4c07b4f0, 0xda0248a3, // add r2, r2, r3       ; mul24      r3, ra1.8b << 5, r0 << 5    @ "mul_used", 0
+/* [0x000011e0] */ 0x40073031, 0xda00c9e3, // nop                  ; mul24.ifnz r3, ra1.8b << 13, r1 << 13  @ "mul_used", 0
+/* [0x000011e8] */ 0x4d07a4f0, 0xdc0248a3, // sub r2, r2, r3       ; mul24      r3, ra1.8c << 6, r0 << 6    @ "mul_used", 0
+/* [0x000011f0] */ 0x40072031, 0xdc00c9e3, // nop                  ; mul24.ifnz r3, ra1.8c << 14, r1 << 14  @ "mul_used", 0
+/* [0x000011f8] */ 0x4c0794f0, 0xde0248a3, // add r2, r2, r3       ; mul24      r3, ra1.8d << 7, r0 << 7    @ "mul_used", 0
+/* [0x00001200] */ 0x40071031, 0xde00c9e3, // nop                  ; mul24.ifnz r3, ra1.8d << 15, r1 << 15  @ "mul_used", 0
+/* [0x00001208] */ 0x8d9df4ff, 0x10024823, // sub r0, r2, r3       ; mov r3, rb31
+/* [0x00001210] */ 0x8d2087f6, 0xd00269e1, // sub.setf -, r3, 8       ; mov r1,   ra8
+/* [0x00001218] */ 0x95249dbf, 0x10024208, // mov ra8,  ra9           ; mov rb8,  rb9
+/* [0x00001220] */ 0xfffffed8, 0xf06809e7, // brr.anyn -, r:yloopb
+/* [0x00001228] */ 0x9528adbf, 0x10024249, // mov ra9,  ra10          ; mov rb9,  rb10
+/* [0x00001230] */ 0x952cbdbf, 0x1002428a, // mov ra10, ra11          ; mov rb10, rb11
+/* [0x00001238] */ 0x959e7009, 0x100242cb, // mov ra11, r0            ; mov rb11, r1
+/* [0x00001240] */ 0x4008803e, 0x180049e0, // nop                     ; mul24 r0, rb8,  ra2.8a
+/* [0x00001248] */ 0x4008903e, 0x1a0049e1, // nop                     ; mul24 r1, rb9,  ra2.8b
+/* [0x00001250] */ 0x4d08a23e, 0x1c024860, // sub r1, r1, r0          ; mul24 r0, rb10, ra2.8c
+/* [0x00001258] */ 0x4d08b23e, 0x1e024860, // sub r1, r1, r0          ; mul24 r0, rb11, ra2.8d
+/* [0x00001260] */ 0x4c204237, 0x10024860, // add r1, r1, r0          ; mul24 r0, ra8,  rb4
+/* [0x00001268] */ 0x4c245237, 0x10024860, // add r1, r1, r0          ; mul24 r0, ra9,  rb5
+/* [0x00001270] */ 0x4d286237, 0x10024860, // sub r1, r1, r0          ; mul24 r0, ra10, rb6
+/* [0x00001278] */ 0x4c2c7237, 0x10024860, // add r1, r1, r0          ; mul24 r0, ra11, rb7
+/* [0x00001280] */ 0x8d9cc23f, 0x10024862, // sub r1, r1, r0          ; mov r2, rb12
+/* [0x00001288] */ 0x4d5927ce, 0x100269e1, // sub.setf -, r3, rb18    ; mul24 r1, r1, ra_k256
+/* [0x00001290] */ 0x0f9ce3c0, 0xd0020867, // asr r1, r1, 14
+/* [0x00001298] */ 0x409ce00f, 0x100049e0, // nop                     ; mul24 r0, r1, rb14
+/* [0x000012a0] */ 0x4c4b808e, 0xd2024821, // add r0, r0, r2          ; mul24 r1, r1 << 8, ra18.16a << 8    @ "mul_used", 0
+/* [0x000012a8] */ 0x8c9f223f, 0x10020867, // add r1, r1, r0          ; mov -, vw_wait
+/* [0x000012b0] */ 0x119c83c0, 0xd0020867, // shl r1, r1, 8
+/* [0x000012b8] */ 0xfffffe40, 0xf06809e7, // brr.anyn -, r:yloopb
+/* [0x000012c0] */ 0x0f9cd3c0, 0x10020867, // asr r1, r1, rb13
+/* [0x000012c8] */ 0x129d63c0, 0x10020867, // min r1, r1, rb_k255
+/* [0x000012d0] */ 0x139c03c0, 0xd0020c27, // max vpm, r1, 0
+/* [0x000012d8] */ 0x00000010, 0xe0020867, // mov r1, 16
+/* [0x000012e0] */ 0x0d427c40, 0x12020827, // sub r0, ra_height, r1
+/* [0x000012e8] */ 0x159e7000, 0x10120427, // mov ra_height, r0
+/* [0x000012f0] */ 0x139c01c0, 0xd0022827, // max.setf r0, r0, 0
+/* [0x000012f8] */ 0xfffff950, 0xf02809e7, // brr.anyz -, r:per_block_setup
+/* [0x00001300] */ 0x159dafc0, 0x10021c67, // mov vw_setup, rb26
+/* [0x00001308] */ 0x159ddfc0, 0x10021c67, // mov vw_setup, rb29
+/* [0x00001310] */ 0x159d7fc0, 0x10021ca7, // mov vw_addr, rb_dest
+/* [0x00001318] */ 0x129e7040, 0x10020827, // min r0, r0, r1
+/* [0x00001320] */ 0x0c9d2e00, 0x100214a7, // add rb18, rb18, r0
+/* [0x00001328] */ 0x0d9e7040, 0x10020827, // sub r0, r0, r1
+/* [0x00001330] */ 0x119d71c0, 0xd0020827, // shl r0, r0, i_shift23
+/* [0x00001338] */ 0x0c9dae00, 0x100216a7, // add rb26, rb26, r0
+/* [0x00001340] */ 0x409d000f, 0x100049e0, // nop ; mul24 r0, r1, rb_pitch
+/* [0x00001348] */ 0x0c9d7e00, 0x100215e7, // add rb_dest, rb_dest, r0
+/* [0x00001350] */ 0x159dcfc0, 0x10021c67, // mov vw_setup, rb28
+/* [0x00001358] */ 0xfffffda0, 0xf0f809e7, // brr -, r:yloopb
+/* [0x00001360] */ 0x009e7000, 0x100009e7, // nop
+/* [0x00001368] */ 0x009e7000, 0x100009e7, // nop
+/* [0x00001370] */ 0x009e7000, 0x100009e7, // nop
+// ::mc_end
+};
+#ifdef __HIGHC__
+#pragma Align_to(8, rpi_shader)
+#endif
diff -Naur ffmpeg-3.2.4/libavcodec/rpi_shader_cmd.h ffmpeg-3.2.4.patch/libavcodec/rpi_shader_cmd.h
--- ffmpeg-3.2.4/libavcodec/rpi_shader_cmd.h	1970-01-01 01:00:00.000000000 +0100
+++ ffmpeg-3.2.4.patch/libavcodec/rpi_shader_cmd.h	2017-05-28 20:42:45.755088727 +0200
@@ -0,0 +1,88 @@
+#ifndef RPI_SHADER_CMD_H
+#define RPI_SHADER_CMD_H
+
+#pragma pack(push, 4)
+
+typedef struct qpu_mc_pred_c_s {
+    uint32_t next_fn;
+    int16_t next_src_y;
+    int16_t next_src_x;
+    uint32_t next_src_base_c;
+    union {
+        struct {
+            uint16_t h;
+            uint16_t w;
+            uint32_t coeffs_x;
+            uint32_t coeffs_y;
+            uint32_t wo_u;
+            uint32_t wo_v;
+            uint32_t dst_addr_c;
+        } p;
+        struct {
+            uint16_t h;
+            uint16_t w;
+            uint32_t coeffs_x;
+            uint32_t coeffs_y;
+            uint32_t weight_u;
+            uint32_t weight_v;
+            uint32_t dummy0;
+        } b0;
+        struct {
+            uint32_t dummy0;
+            uint32_t coeffs_x;
+            uint32_t coeffs_y;
+            uint32_t wo_u;
+            uint32_t wo_v;
+            uint32_t dst_addr_c;
+        } b1;
+        struct {
+            uint32_t pic_cw;            // C Width (== Y width / 2)
+            uint32_t pic_ch;            // C Height (== Y Height / 2)
+            uint32_t stride2;
+            uint32_t stride1;
+            uint32_t wdenom;
+            uint32_t dummy0;
+        } s0;
+        struct {
+            uint32_t dummy0;
+            uint32_t dummy1;
+            uint32_t dummy2;
+            uint32_t dummy3;
+            uint32_t dummy4;
+            uint32_t dummy5;
+        } s1;
+    };
+} qpu_mc_pred_c_t;
+
+typedef struct qpu_mc_pred_y_s {
+    int16_t next_src1_x;
+    int16_t next_src1_y;
+    uint32_t next_src1_base;
+    int16_t next_src2_x;
+    int16_t next_src2_y;
+    uint32_t next_src2_base;
+    union {
+        struct {
+            uint16_t h;
+            uint16_t w;
+            uint32_t mymx21;
+            uint32_t wo1;
+            uint32_t wo2;
+            uint32_t dst_addr;
+        } p;
+        struct {
+            uint16_t pic_h;
+            uint16_t pic_w;
+            uint32_t stride2;
+            uint32_t stride1;
+            uint32_t wdenom;
+            uint32_t dummy0;
+        } s;
+    };
+    uint32_t next_fn;
+} qpu_mc_pred_y_t;
+
+#pragma pack(pop)
+
+#endif
+
diff -Naur ffmpeg-3.2.4/libavcodec/rpi_shader.h ffmpeg-3.2.4.patch/libavcodec/rpi_shader.h
--- ffmpeg-3.2.4/libavcodec/rpi_shader.h	1970-01-01 01:00:00.000000000 +0100
+++ ffmpeg-3.2.4.patch/libavcodec/rpi_shader.h	2017-05-28 20:42:45.753088719 +0200
@@ -0,0 +1,19 @@
+#ifndef rpi_shader_H
+#define rpi_shader_H
+
+extern unsigned int rpi_shader[];
+
+#define mc_setup_c (rpi_shader + 0)
+#define mc_filter_uv (rpi_shader + 152)
+#define mc_filter_uv_b0 (rpi_shader + 280)
+#define mc_interrupt_exit8c (rpi_shader + 554)
+#define mc_exit (rpi_shader + 582)
+#define mc_exit_c (rpi_shader + 582)
+#define mc_interrupt_exit12 (rpi_shader + 598)
+#define mc_exit1 (rpi_shader + 634)
+#define mc_setup (rpi_shader + 650)
+#define mc_filter (rpi_shader + 942)
+#define mc_filter_b (rpi_shader + 1094)
+#define mc_end (rpi_shader + 1246)
+
+#endif
diff -Naur ffmpeg-3.2.4/libavcodec/rpi_shader.qasm ffmpeg-3.2.4.patch/libavcodec/rpi_shader.qasm
--- ffmpeg-3.2.4/libavcodec/rpi_shader.qasm	1970-01-01 01:00:00.000000000 +0100
+++ ffmpeg-3.2.4.patch/libavcodec/rpi_shader.qasm	2017-05-28 20:42:45.754088723 +0200
@@ -0,0 +1,1259 @@
+
+# The @ "mul_used", 0 annotations that occur by various mul blocks suppress
+# the warning that we are using rotation & ra/rb registers. r0..3 can be
+# rotated through all 16 elems ra regs can only be rotated through their
+# local 4.  As it happens this is what is wanted here as we do not want the
+# constants from the other half of the calc.
+
+# register allocation
+#
+# ra0...ra7                                     eight horizontal filter coefficients
+#
+# rb0 rx_shift2
+# rb1 rb_y2_next
+#
+# rb4...rb7
+#
+# rb8..rb11, ra8...ra11                         Y: eight filtered rows of context (ra11 == most recent)
+#
+#                                               (ra15 isn't clamped to zero - this happens during the
+#                                                copy to ra14, and during its use in the vertical filter)
+#
+# rb8...rb11                                    eight vertical filter coefficients
+
+# ra4                                           y: Fiter, UV: part -of b0 -> b stash
+
+# rb12                                          offset to add before shift (round + weighting offsets)
+# rb13                                          shift: denom + 6 + 9
+# rb14                                          L0 weight (U on left, V on right)
+# rb15                                          -- free --
+#
+# ra16                                          width:height
+# ra17                                          ra_y:ra_xshift
+# ra18                                          L1 weight (Y)
+# ra19                                          ra_y_next:ra_xshift_next
+#
+# rb16                                          pitch
+# rb17                                          height + 1
+# rb18                                          max(height,16) + 3
+# rb19                                          frame_base2_next
+#
+# ra20                                          1
+# ra21                                          ra_y2_next:ra_y2 (luma); free (chroma)
+# ra22 ra_k256                                  256
+# ra23                                          0
+#
+# rb20                                          -- free --
+# rb21                                          -- free --
+# rb22 rb_k255                                  255
+# rb23                                          dest (Y)
+#
+# rb24                                          vdw_setup_1(dst_pitch)
+# rb25                                          frame width-1
+# rb26                                          height<<23 + width<<16 + vdw_setup_0
+# rb27                                          vdw_setup_0 (depends on QPU number)
+# rb28                                          vpm_setup (depends on QPU number) for writing 8bit results into VPM
+# rb29                                          vdw_setup_1(dst_pitch-width)
+# rb30                                          frame height-1
+# rb31                                          used as temp to count loop iterations
+#
+# ra24                                          src frame base
+# ra25                                          src frame base 2
+# ra26                                          next ra24
+# ra27                                          next ra25
+# ra28                                          -- free --
+# ra29                                          -- free --
+#
+# Use an even numbered register as a link register to avoid corrupting flags
+# ra30                                          next kernel address
+# ra31                                          chroma-B height+3; free otherwise
+
+.set rb_max_x,                     rb25
+.set rb_max_y,                     rb30
+.set rb_pitch,                     rb16
+.set ra_width_height,              ra16
+.set ra_width,                     ra16.16b
+.set ra_height,                    ra16.16a
+.set ra_y2,                        ra21.16a
+.set ra_y2_next,                   ra21.16b
+
+.set rb_base2_next,                rb19
+
+.set rb_dest,                      rb23
+.set ra_base,                      ra24
+.set ra_base_next,                 ra26
+.set ra_xshift,                    ra17.16a
+
+.set ra_base2,                     ra25
+
+# Note ra_xy & ra_xy_next should have same structure!
+.set ra_xshift_next,               ra19.16a
+.set rb_xshift2,                   rb0
+.set rb_xshift2_next,              rb1
+
+.set ra_y_next,                    ra19.16b
+.set ra_y,                         ra17.16b
+
+.set ra_k1,                        ra20
+.set rb_xpitch,                    rb20
+.set rb_k255,                      rb22
+.set ra_k256,                      ra22
+.set ra_k0,                        ra23
+
+.set ra_link,                      ra30
+
+# With shifts only the bottom 5 bits are considered so -16=16, -15=17 etc.
+.set i_shift16,                    -16
+.set i_shift21,                    -11
+.set i_shift23,                     -9
+.set i_shift30,                     -2
+
+# Much of the setup code is common between Y & C
+# Macros that express this - obviously these can't be overlapped
+# so are probably unsuitable for loop code
+
+.macro m_calc_dma_regs, r_vpm, r_dma
+  mov r2, qpu_num
+  asr r1, r2, 2
+  shl r1, r1, 6
+  and r0, r2, 3
+  or  r0, r0, r1
+
+  mov r1, vpm_setup(0, 4, h8p(0, 0))   # 4 is stride - stride acts on ADDR which is Y[5:0],B[1:0] for 8 bit
+  add r_vpm, r0, r1  # VPM 8bit storage
+
+  mov r1, vdw_setup_0(0, 0, dma_h8p(0,0,0)) # height,width added later
+  shl r0, r0, 5
+  add r_dma, r0, r1  # DMA out
+.endm
+
+# For chroma use packed H = (qpu_num & 1), Y = (qpu_num >> 1) * 16
+.macro m_calc_dma_regs_c, r_vpm, r_dma
+  mov r2, qpu_num
+  asr r1, r2, 1
+  shl r1, r1, 5
+  and r0, r2, 1
+  or  r0, r0, r1
+
+  mov r1, vpm_setup(0, 2, h16p(0, 0))   # 2 is stride - stride acts on ADDR which is Y[5:0],B[1:0] for 8 bit
+  add r_vpm, r0, r1  # VPM 8bit storage
+
+  # X = H * 8 so the YH from VPMVCD_WR_SETUP[ADDR] drops into
+  # XY VPMVCD_WR_SETUP[VPMBASE] if shifted left 3 (+ 3 for pos of field in reg)
+  mov r1, vdw_setup_0(0, 0, dma_h16p(0,0,0)) # height,width added later
+  shl r0, r0, 6
+  add r_dma, r0, r1  # DMA out
+.endm
+
+
+################################################################################
+# mc_setup_uv(next_kernel, x, y, ref_c_base, frame_width, frame_height, pitch, dst_pitch, offset, denom, vpm_id)
+::mc_setup_c
+  mov tmurs, 1          ; mov -, unif        # No swap TMUs ; Next fn (ignored)
+
+# Load first request location
+  mov ra0, unif         # next_x_y
+
+  mov ra_base, unif                             # Store frame c base
+
+# Read image dimensions
+  sub rb_max_x, unif, 1     # pic c width
+  sub rb_max_y, unif, 1     # pic c height
+
+# load constants
+  mov ra_k1, 1
+  mov ra_k256, 256
+  mov rb_k255, 255
+  mov ra_k0, 0
+
+# touch registers to keep simulator happy
+
+  # ra/b4..7: B0 -> B stash registers
+  mov ra4, 0 ; mov rb4, 0
+  mov ra5, 0 ; mov rb5, 0
+  mov ra6, 0 ; mov rb6, 0
+  mov ra7, 0 ; mov rb7, 0
+
+  mov r1, vdw_setup_1(0)  # Merged with dst_stride shortly, delay slot for ra_base
+
+# ; ra12..15: vertical scroll registers
+# get source pitch
+  mov rb_xpitch, unif   ; mov ra12, 0           # stride2
+  mov rb_pitch, unif    ; mov ra13, 0           # stride1
+  mov r0, elem_num      ; mov ra14, 0
+# get destination vdw setup
+  add rb24, r1, rb_pitch ; mov ra15, ra_k0 # vdw_setup_1
+
+# Compute base address for first and second access
+# ra_base ends up with t0s base
+# ra_base2 ends up with t1s base
+
+  add r0, r0, ra0.16b                           # Add elem no to x to get X for this slice
+  max r0, r0, 0         ; mov ra_y, ra0.16a     # ; stash Y
+  min r0, r0, rb_max_x
+
+# Get shift
+  and r1, r0, 1
+  shl ra_xshift_next, r1, 4
+
+# In a single 32 bit word we get 2 UV pairs so mask bottom bit of xs
+
+  and r0, r0, -2
+  add r0, r0, r0        ; v8subs r1, r1, r1
+  sub r1, r1, rb_pitch
+  and r1, r0, r1
+  xor r0, r0, r1        ; mul24 r1, r1, rb_xpitch
+  add r0, r0, r1        ; mov r1, ra_y
+  add ra_base, ra_base, r0
+
+  max r0, r1, 0
+  min r0, r0, rb_max_y
+
+# submit texture requests for first line
+  add r1, r1, ra_k1     ; mul24 r0, r0, rb_pitch
+  add t0s, ra_base, r0
+
+# submit texture requests for 2nd line
+
+  max r0, r1, 0
+  min r0, r0, rb_max_y
+
+  add ra_y, r1, ra_k1   ; mul24 r0, r0, rb_pitch
+  add t0s, ra_base, r0
+
+  add rb13, 9, unif     # denominator
+  mov -, unif           # Unused
+
+# Compute part of VPM to use for DMA output
+  m_calc_dma_regs_c rb28, rb27
+
+# -----------------
+# And again for L1, but only worrying about frame2 stuff
+
+  mov ra_link, unif        # Next fn
+
+# Load first request location
+  mov ra0, unif            # next_x_y
+
+  mov ra_base2, unif # Store frame c base
+
+# Compute base address for first and second access
+# ra_base ends up with t0s base
+# ra_base2 ends up with t1s base
+
+  mov ra_y2, ra0.16a       # Store y
+  mov r0, ra0.16b          # Load x
+  add r0, r0, elem_num     # Add QPU slice
+  max r0, r0, 0         ; mov -, unif           # Unused 0
+  min r0, r0, rb_max_x  ; mov -, unif           # Unused 1
+
+# Get shift
+  and r1, r0, 1         ; mov -, unif           # Unused 2
+  shl rb_xshift2_next, r1, 4
+
+# In a single 32 bit word we get 2 UV pairs so mask bottom bit of xs
+
+  and r0, r0, -2
+  add r0, r0, r0        ; v8subs r1, r1, r1
+  sub r1, r1, rb_pitch
+  and r1, r0, r1
+  xor r0, r0, r1        ; mul24 r1, r1, rb_xpitch
+  add r0, r0, r1        ; mov r1, ra_y2
+  add ra_base2, ra_base2, r0
+
+  max r0, r1, 0
+  min r0, r0, rb_max_y
+
+# submit texture requests for first line
+  add r1, r1, ra_k1     ; mul24 r0, r0, rb_pitch
+  add t1s, ra_base2, r0 ; mov -, unif           # Unused 3
+
+# submit texture requests for 2nd line
+
+  max r0, r1, 0         ; mov -, unif           # Unused 4
+
+  bra -, ra_link
+
+  min r0, r0, rb_max_y  ; mov -, unif           # Unused 5
+  add ra_y2, r1, ra_k1   ; mul24 r0, r0, rb_pitch
+  add t1s, ra_base2, r0
+
+# >>> ra_link
+
+
+.macro setf_nz_if_v
+  mov.setf -, [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]
+.endm
+
+
+################################################################################
+
+# mc_filter_uv(next_kernel, x, y, frame_u_base, frame_v_base, width_height, hcoeffs, vcoeffs, offset_weight_u, offset_weight_v, this_u_dst, this_v_dst)
+
+# At this point we have already issued two pairs of texture requests for the current block
+# ra_x, ra_x16_base point to the current coordinates for this block
+::mc_filter_uv
+  mov ra_link, unif     ; mov vw_setup, rb28    # ; x_y
+
+# per-channel shifts were calculated on the *previous* invocation
+
+# get base addresses and per-channel shifts for *next* invocation
+  mov ra2, unif         ; mov r0, elem_num
+
+  setf_nz_if_v                                  # Also acts as delay slot for ra2
+
+  add r0, ra2.16b, r0   ; v8subs r1, r1, r1     # x ; r1=0
+  sub r1, r1, rb_pitch  ; mov r3, unif          # r1=pitch2 mask ; r3=base
+  max r0, r0, 0         ; mov rb_xshift2, ra_xshift_next # ; xshift2 used because B
+  min r0, r0, rb_max_x  ; mov ra1, unif         # ; width_height
+
+  shl ra_xshift_next, r0, 4
+
+  and r0, r0, -2        ; mov ra0, unif         # H filter coeffs
+  add r0, r0, r0        ; mov ra_y_next, ra2.16a
+  and r1, r0, r1        ; mul24 r2, ra1.16b, 2  # r2=x*2 (we are working in pel pairs)
+  xor r0, r0, r1        ; mul24 r1, r1, rb_xpitch
+  add r0, r0, r1        ; mov r1, ra1.16a       # Add stripe offsets ; r1=height
+  add ra_base_next, r3, r0 ; mul24 r0, r1, ra_k256
+
+# set up VPM write
+
+  sub rb29, rb24, r2    ; mov ra3, unif         # Compute vdw_setup1(dst_pitch-width) ; V filter coeffs
+  add rb17, r1, 1       ; mov ra1, unif         # ; U offset/weight
+  add rb18, r1, 3       ; mov.ifnz ra1, unif    # ; V offset/weight
+
+# ; unpack filter coefficients
+
+  add r0,   r0, r2      ; mov rb8,  ra3.8a      # Combine width and height of destination area
+  shl r0,   r0, 15      ; mov rb9,  ra3.8b      # Shift into bits 16 upwards of the vdw_setup0 register
+  add rb26, r0, rb27    ; mov r1, ra1.16b       # ; r1=weight
+
+  shl r1, r1, rb13      ; mov rb10, ra3.8c
+  mov r3, 0             ; mov rb11, ra3.8d   # Loop count
+
+  asr rb12, r1, 1
+  shl rb14, ra1.16a, 1  # b14 = weight*2
+
+# rb14 - weight L0 * 2
+# rb13 = weight denom + 6 + 9
+# rb12 = (((is P) ? offset L0 * 2 : offset L1 + offset L0) + 1) << (rb13 - 1)
+
+# retrieve texture results and pick out bytes
+# then submit two more texture requests
+
+# r3 = 0
+:uvloop
+# retrieve texture results and pick out bytes
+# then submit two more texture requests
+
+  sub.setf -, r3, rb17  ; v8adds rb31, r3, ra_k1 ; ldtmu0     # loop counter increment
+  shr r0, r4, rb_xshift2 ; mov.ifz r3, ra_y_next
+  shr r1, r0, 8         ; mov.ifnz r3, ra_y
+
+  max r2, r3, 0         ; mov.ifz ra_base, ra_base_next
+  min r2, r2, rb_max_y
+  add ra_y, r3, ra_k1   ; mul24 r2, r2, rb_pitch
+  add t0s, ra_base, r2  ; v8min r0, r0, rb_k255  # v8subs masks out all but bottom byte
+
+# generate seven shifted versions
+# interleave with scroll of vertical context
+
+  setf_nz_if_v
+
+# apply horizontal filter
+# The filter coeffs for the two halves of this are the same (unlike in the
+# Y case) so it doesn't matter which ra0 we get them from
+
+  and r1, r1, rb_k255   ; mul24      r3, ra0.8a,       r0
+  nop                   ; mul24      r2, ra0.8b << 1,  r0 << 1  @ "mul_used", 0
+  nop                   ; mul24.ifnz r3, ra0.8a << 8,  r1 << 8  @ "mul_used", 0
+  nop                   ; mul24.ifnz r2, ra0.8b << 9,  r1 << 9  @ "mul_used", 0
+  sub r2, r2, r3        ; mul24      r3, ra0.8c << 2,  r0 << 2  @ "mul_used", 0
+  nop                   ; mul24.ifnz r3, ra0.8c << 10, r1 << 10 @ "mul_used", 0
+  add r2, r2, r3        ; mul24      r3, ra0.8d << 3,  r0 << 3  @ "mul_used", 0
+  nop                   ; mul24.ifnz r3, ra0.8d << 11, r1 << 11 @ "mul_used", 0
+  sub r0, r2, r3        ; mov r3, rb31
+  sub.setf -, r3, 4     ; mov ra12, ra13
+  brr.anyn -, r:uvloop
+  mov ra13, ra14        ; mul24 r1, ra14, rb9
+  mov ra14, ra15
+  mov ra15, r0          ; mul24 r0, ra12, rb8
+# >>> .anyn uvloop
+
+# apply vertical filter and write to VPM
+
+  sub r1, r1, r0        ; mul24 r0, ra14, rb10
+  add r1, r1, r0        ; mul24 r0, ra15, rb11
+  sub r1, r1, r0
+  sub.setf -, r3, rb18  ; mul24 r1, r1, ra_k256
+  asr r1, r1, 14
+  nop                   ; mul24 r1, r1, rb14
+  shl r1, r1, 8
+
+  add r1, r1, rb12
+  asr ra1.8as, r1, rb13
+  nop                   ; mov r1, r1 << 8
+  brr.anyn -, r:uvloop
+  asr ra1.8bs, r1, rb13
+  mov -, vw_wait
+  mov vpm, ra1
+
+# >>>
+
+# DMA out for U & stash for V
+  bra -, ra_link
+  mov vw_setup, rb26
+  mov vw_setup, rb29
+  mov vw_addr, unif     # u_dst_addr
+# >>>
+
+################################################################################
+
+# mc_filter_uv_b0(next_kernel, x, y, frame_c_base, height, hcoeffs[0], hcoeffs[1], vcoeffs[0], vcoeffs[1], this_u_dst, this_v_dst)
+
+# At this point we have already issued two pairs of texture requests for the current block
+# ra_x, ra_x16_base point to the current coordinates for this block
+::mc_filter_uv_b0
+  mov -, unif           ; mov vw_setup, rb28    # next_fn ignored - always uv_b
+
+# per-channel shifts were calculated on the *previous* invocation
+
+# get base addresses and per-channel shifts for *next* invocation
+  mov ra2, unif         ; mov r0, elem_num
+
+  setf_nz_if_v                                  # Also acts as delay slot for ra2
+
+  add r0, ra2.16b, r0   ; v8subs r1, r1, r1     # x ; r1=0
+  sub r1, r1, rb_pitch  ; mov r3, unif          # r1=pitch2 mask ; r3=base
+  max r0, r0, 0         ; mov rb_xshift2, ra_xshift_next # ; xshift2 used because B
+  min r0, r0, rb_max_x  ; mov ra1, unif         # ; width_height
+
+  shl ra_xshift_next, r0, 4
+
+  and r0, r0, -2        ; mov ra0, unif         # H filter coeffs
+  add r0, r0, r0        ; mov ra_y_next, ra2.16a
+  and r1, r0, r1        ; mul24 r2, ra1.16b, 2  # r2=x*2 (we are working in pel pairs)
+  xor r0, r0, r1        ; mul24 r1, r1, rb_xpitch
+  add r0, r0, r1        ; mov r1, ra1.16a       # Add stripe offsets ; r1=height
+  add ra_base_next, r3, r0 ; mul24 r0, r1, ra_k256
+
+# set up VPM write
+
+  sub rb29, rb24, r2    ; mov ra3, unif         # Compute vdw_setup1(dst_pitch-width) ; V filter coeffs
+  add rb17, r1, 1
+  add ra31, r1, 3       ; mov rb8,  ra3.8a      # Combine width and height of destination area
+
+# ; unpack filter coefficients
+
+  add r0,   r0, r2      ; mov rb9,  ra3.8b
+  shl r0,   r0, 15      ; mov rb10, ra3.8c      # Shift into bits 16 upwards of the vdw_setup0 register
+  add rb26, r0, rb27
+
+  mov r3, 0             ; mov rb11, ra3.8d      # Loop count
+
+  mov rb14, unif                                # U weight
+  mov.ifnz rb14, unif                           # V weight
+
+# rb14 unused in b0 but will hang around till the second pass
+
+# retrieve texture results and pick out bytes
+# then submit two more texture requests
+
+# r3 = 0
+:uvloop_b0
+# retrieve texture results and pick out bytes
+# then submit two more texture requests
+
+  sub.setf -, r3, rb17  ; v8adds rb31, r3, ra_k1 ; ldtmu0     # loop counter increment
+  shr r0, r4, rb_xshift2 ; mov.ifz r3, ra_y_next
+  shr r1, r0, 8         ; mov.ifnz r3, ra_y
+
+  max r2, r3, 0         ; mov.ifz ra_base, ra_base_next
+  min r2, r2, rb_max_y
+  add ra_y, r3, ra_k1   ; mul24 r2, r2, rb_pitch
+  add t0s, ra_base, r2  ; v8min r0, r0, rb_k255  # v8subs masks out all but bottom byte
+
+# generate seven shifted versions
+# interleave with scroll of vertical context
+
+  mov.setf -, [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]
+
+  and r1, r1, rb_k255   ; mul24      r3, ra0.8a,       r0
+  nop                   ; mul24      r2, ra0.8b << 1,  r0 << 1  @ "mul_used", 0
+  nop                   ; mul24.ifnz r3, ra0.8a << 8,  r1 << 8  @ "mul_used", 0  # Need to wait 1 cycle for rotated r1
+  nop                   ; mul24.ifnz r2, ra0.8b << 9,  r1 << 9  @ "mul_used", 0
+  sub r2, r2, r3        ; mul24      r3, ra0.8c << 2,  r0 << 2  @ "mul_used", 0
+  nop                   ; mul24.ifnz r3, ra0.8c << 10, r1 << 10 @ "mul_used", 0
+  add r2, r2, r3        ; mul24      r3, ra0.8d << 3,  r0 << 3  @ "mul_used", 0
+  nop                   ; mul24.ifnz r3, ra0.8d << 11, r1 << 11 @ "mul_used", 0
+  sub r0, r2, r3        ; mov r3, rb31
+  sub.setf -, r3, 4     ; mov ra12, ra13
+  brr.anyn -, r:uvloop_b0
+  mov ra13, ra14        ; mul24 r1, ra14, rb9   # ra14 is about to be ra13
+  mov ra14, ra15        ; mul24 r2, ra15, rb10  # ra15 is about to be ra14
+  mov ra15, r0          ; mul24 r0, ra12, rb8
+# >>> .anyn uvloop_b0
+
+# apply vertical filter and write to B-FIFO
+
+  sub r1, r1, r0        ; mov ra8.16b, ra7      # start of B FIFO writes
+  add r1, r1, r2        ; mul24 r0, ra15, rb11  # N.B. ra15 write gap
+  sub r1, r1, r0        ; mov ra7, rb6
+
+# FIFO goes:
+# b7a, a6a, b5a, a4a, b4a, a5a, b6a, a7a : b7b, a6b, b5b, a4b, b4b, a5b, b6b, a7b
+# This arrangement optimizes the inner loop FIFOs at the expense of making the
+# bulk shift between loops quite a bit nastier
+# a8 used as temp
+
+  sub.setf -, r3, ra31
+  asr ra8.16a, r1, 6    ; mov rb6, ra5          # This discards the high bits that might be bad
+  brr.anyn -, r:uvloop_b0
+  mov ra5, rb4          ; mov rb4, ra4
+  mov ra4, rb5          ; mov rb5, ra6
+  mov ra6, rb7          ; mov rb7, ra8
+# >>>
+
+# 1st half done all results now in the a/b4..7 fifo
+
+# Need to bulk rotate FIFO for heights other than 16
+# plausible heights are 16, 12, 8, 6, 4, 2 and that is all we deal with
+# we are allowed 3/4 cb_size w/h :-(
+
+# Destination uniforms discarded
+# At the end drop through to _b - we will always do b after b0
+
+  sub.setf -, 15, r3    # 12 + 3 of preroll
+  brr.anyn -, r:uv_b0_post_fin                  # h > 12 (n) => 16 (do nothing)
+  sub r3, 11, r3        ; mov -, unif           # r3 = shifts wanted ; Discard u_dst_addr
+  mov r0, i_shift16     ; mov ra_link, unif
+  mov r1, 0x10000
+# >>>
+  brr.anyz -, r:uv_b0_post12                    # h == 12 deal with specially
+# If h != 16 && h != 12 then h <= 8 so
+# shift 8 with discard (.16b = .16a on all regs)
+  shl.ifnz ra7, ra7, r0 ; mul24.ifnz rb7, rb7, r1
+  shl.ifnz ra6, ra6, r0 ; mul24.ifnz rb6, rb6, r1
+  shl.ifnz ra5, ra5, r0 ; mul24.ifnz rb5, rb5, r1
+# >>>
+  shl ra4, ra4, r0      ; mul24 rb4, rb4, r1
+
+  shl.setf -, r3, i_shift30  # b2 -> C, b1 -> N
+# Shift 4
+  mov.ifc ra7, ra4      ; mov.ifc rb6, rb5
+  mov.ifc ra5, ra6      ; mov.ifc rb4, rb7
+  # If we shifted by 4 here then the max length remaining is 4
+  # so that is it
+
+  brr -, r:uv_b0_post_fin
+# Shift 2
+  mov.ifn ra7, ra5      ; mov.ifn rb6, rb4
+  mov.ifn ra5, ra4      ; mov.ifn rb4, rb5
+  mov.ifn ra4, ra6      ; mov.ifn rb5, rb7
+  # 6 / 2 so need 6 outputs
+# >>>
+
+:uv_b0_post12
+# this one is annoying as we need to swap halves of things that don't
+# really want to be swapped
+
+# b7a, a6a, b5a, a4a
+# b4a, a5a, b6a, a7a
+# b7b, a6b, b5b, a4b
+# b4b, a5b, b6b, a7b
+
+  mov r2, ra6           ; mov r3, rb7
+  shl ra6, ra5, r0      ; mul24 rb7, rb4, r1
+  mov ra5, r2           ; mov rb4, r3
+
+  mov r2,  ra4          ; mov r3,  rb5
+  shl ra4, ra7, r0      ; mul24 rb5, rb6, r1
+  mov ra7, r2           ; mov rb6, r3
+
+:uv_b0_post_fin
+
+##### L1 B processing
+
+# per-channel shifts were calculated on the *previous* invocation
+
+# get base addresses and per-channel shifts for *next* invocation
+  mov ra2, unif         ; mov r0, elem_num
+
+  setf_nz_if_v                                  # Also acts as delay slot for ra2
+
+  add r0, ra2.16b, r0   ; v8subs r1, r1, r1     # x ; r1=0
+  sub r1, r1, rb_pitch  ; mov r3, unif          # r1=pitch2 mask ; r3=base
+  max r0, r0, ra_k0     ; mov rb_xshift2, rb_xshift2_next # ; xshift2 used because B
+  min r0, r0, rb_max_x  ; mov -, unif           # ; width_height
+
+  shl rb_xshift2_next, r0, 4
+
+  and r0, r0, -2        ; mov ra0, unif         # H filter coeffs
+  add r0, r0, r0        ; mov ra_y2_next, ra2.16a
+  and r1, r0, r1        ; mov ra3, unif         # ; V filter coeffs
+  xor r0, r0, r1        ; mul24 r1, r1, rb_xpitch
+  add r0, r0, r1        ; mov rb8,  ra3.8a      # Add stripe offsets ; start unpacking filter coeffs
+  add rb_base2_next, r3, r0
+
+  mov ra1, unif         ; mov rb9,  ra3.8b      # U offset/weight
+  mov.ifnz ra1, unif    ; mov rb10, ra3.8c      # V offset/weight
+
+  nop                   ; mov rb11, ra3.8d
+  shl r1, ra1.16b, rb13 ; v8subs r3, r3, r3     # ; r3 (loop counter)  = 0
+  asr rb12, r1, 1
+
+# ra1.16a used directly in the loop
+
+# retrieve texture results and pick out bytes
+# then submit two more texture requests
+
+# r3 = 0
+
+:uvloop_b
+# retrieve texture results and pick out bytes
+# then submit two more texture requests
+
+  sub.setf -, r3, rb17  ; v8adds rb31, r3, ra_k1 ; ldtmu1     # loop counter increment
+  shr r0, r4, rb_xshift2 ; mov.ifz r3, ra_y2_next
+  shr r1, r0, 8         ; mov.ifnz r3, ra_y2
+
+  max r2, r3, ra_k0     ; mov.ifz ra_base2, rb_base2_next
+  min r2, r2, rb_max_y
+  add ra_y2, r3, ra_k1  ; mul24 r2, r2, rb_pitch
+  add t1s, ra_base2, r2 ; v8min r0, r0, rb_k255  # v8subs masks out all but bottom byte
+
+# generate seven shifted versions
+# interleave with scroll of vertical context
+
+mov.setf -, [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]
+
+  and r1, r1, rb_k255  ; mul24      r3, ra0.8a,       r0
+  nop                  ; mul24      r2, ra0.8b << 1,  r0 << 1     @ "mul_used", 0
+  nop                  ; mul24.ifnz r3, ra0.8a << 8,  r1 << 8     @ "mul_used", 0
+  nop                  ; mul24.ifnz r2, ra0.8b << 9,  r1 << 9     @ "mul_used", 0
+  sub r2, r2, r3       ; mul24      r3, ra0.8c << 2,  r0 << 2     @ "mul_used", 0
+  nop                  ; mul24.ifnz r3, ra0.8c << 10, r1 << 10    @ "mul_used", 0
+  add r2, r2, r3       ; mul24      r3, ra0.8d << 3,  r0 << 3     @ "mul_used", 0
+  nop                  ; mul24.ifnz r3, ra0.8d << 11, r1 << 11    @ "mul_used", 0
+  sub r0, r2, r3       ; mov r3, rb31
+  sub.setf -, r3, 4    ; mov ra12, ra13
+  brr.anyn -, r:uvloop_b
+  mov ra13, ra14          ; mul24 r1, ra14, rb9
+  mov ra14, ra15          ; mul24 r2, ra15, rb10
+  mov ra15, r0            ; mul24 r0, ra12, rb8
+# >>> .anyn uvloop_b
+
+# apply vertical filter and write to VPM
+
+  sub r1, r1, r0        ; mov ra8.16b, ra7      # FIFO rotate (all ra/b4..7)
+  add r1, r1, r2        ; mul24 r0, ra15, rb11
+  sub r1, r1, r0        ; mul24 r0, ra7.16b, rb14
+  mov ra7, rb6          ; mul24 r1, r1, ra_k256
+  asr r1, r1, 14        ; mov rb6, ra5 # shift2=6
+
+  mov ra5, rb4          ; mul24 r1, r1, ra1.16a
+  add r1, r1, r0        ; mov rb4, ra4
+
+  mov ra4, rb5          ; mul24 r1, r1, ra_k256 # Lose bad top 8 bits & sign extend
+  add r1, r1, rb12      ; mov rb5, ra6          # rb12 = (offsetL0 + offsetL1 + 1) << (rb13 - 1)
+
+  sub.setf -, r3, ra31  ; mov ra6, rb7
+  asr ra3.8as, r1, rb13
+  nop                   ; mov r1, r1 << 8
+  brr.anyn -, r:uvloop_b
+  asr ra3.8bs, r1, rb13
+  mov -, vw_wait        ; mov rb7, ra8          #  vw_wait is B-reg (annoyingly) ; Final FIFO mov
+  mov vpm, ra3
+# >>>
+
+# DMA out
+
+  bra -, ra_link
+  mov vw_setup, rb26
+  mov vw_setup, rb29
+  mov vw_addr, unif     # c_dst_addr
+
+
+################################################################################
+
+# mc_exit()
+
+::mc_interrupt_exit8c
+  ldtmu0
+  ldtmu1
+  ldtmu1
+  mov  -, vw_wait ; nop ; ldtmu0  # wait on the VDW
+
+  mov -,sacq(0) # 1
+  mov -,sacq(0) # 2
+  mov -,sacq(0) # 3
+  mov -,sacq(0) # 4
+  mov -,sacq(0) # 5
+  mov -,sacq(0) # 6
+  mov -,sacq(0) # 7
+#  mov -,sacq(0) # 8
+#  mov -,sacq(0) # 9
+#  mov -,sacq(0) # 10
+#  mov -,sacq(0) # 11
+
+  nop        ; nop ; thrend
+  mov interrupt, 1; nop # delay slot 1
+  nop        ; nop # delay slot 2
+
+# Chroma & Luma the same now
+::mc_exit_c
+::mc_exit
+  ldtmu0
+  ldtmu1
+  ldtmu0
+  mov  -, vw_wait ; nop ; ldtmu1 # wait on the VDW
+
+  mov -,srel(0)
+
+  nop        ; nop ; thrend
+  nop        ; nop # delay slot 1
+  nop        ; nop # delay slot 2
+
+
+# mc_interrupt_exit12()
+::mc_interrupt_exit12
+  ldtmu0
+  ldtmu1
+  ldtmu0
+  mov  -, vw_wait ; nop ; ldtmu1  # wait on the VDW
+
+  mov -,sacq(0) # 1
+  mov -,sacq(0) # 2
+  mov -,sacq(0) # 3
+  mov -,sacq(0) # 4
+  mov -,sacq(0) # 5
+  mov -,sacq(0) # 6
+  mov -,sacq(0) # 7
+  mov -,sacq(0) # 8
+  mov -,sacq(0) # 9
+  mov -,sacq(0) # 10
+  mov -,sacq(0) # 11
+
+  nop        ; nop ; thrend
+  mov interrupt, 1; nop # delay slot 1
+  nop        ; nop # delay slot 2
+
+
+::mc_exit1
+  mov  -, vw_wait # wait on the VDW
+
+  ldtmu0
+  ldtmu1
+  ldtmu0
+  ldtmu1
+  nop        ; nop ; thrend
+  mov interrupt, 1; nop # delay slot 1
+  nop        ; nop # delay slot 2
+
+# LUMA CODE
+
+# The idea is to form B predictions by doing 8 pixels from ref0 in parallel with 8 pixels from ref1.
+# For P frames we make the second x,y coordinates offset by +8
+
+################################################################################
+# mc_setup(y_x, ref_y_base, y2_x2, ref_y2_base, frame_width_height, pitch, dst_pitch, offset_shift, tbd, next_kernel)
+::mc_setup
+  # Need to save these because we need to know the frame dimensions before computing texture coordinates
+  mov tmurs, 1          ; mov ra8, unif         # No TMU swap ; y_x
+  mov ra9, unif         # ref_y_base
+  mov ra10, unif        # y2_x2
+  mov ra11, unif        # ref_y2_base
+
+# Read image dimensions
+  mov ra3, unif         # width_height
+  mov rb_xpitch, unif   # stride2
+  sub rb_max_x, ra3.16b, 1
+  sub rb_max_y, ra3.16a, 1
+  mov rb_pitch, unif    # stride1
+
+# get destination pitch
+  mov r1, vdw_setup_1(0)
+  or  rb24, r1, rb_pitch
+
+# Compute base address for first and second access
+  mov r3, elem_num
+  add r0, ra8.16a, r3   # Load x + elem_num
+  max r0, r0, 0
+  min r0, r0, rb_max_x
+  shl ra_xshift_next, r0, 3 # Compute shifts
+
+
+# In a single 32 bit word we get 4 Y Pels so mask 2 bottom bits of xs
+
+  and r0, r0, -4        ; v8subs r2, r2, r2
+  sub r2, r2, rb_pitch
+  and r1, r0, r2
+  xor r0, r0, r1        ; mul24 r1, r1, rb_xpitch
+  add r0, r0, r1        # Add stripe offsets
+  add ra_base, ra9, r0
+
+  mov r1, ra8.16b       # Load y
+  add ra_y, r1, 1       # Set for next
+  max r1, r1, 0
+  min r1, r1, rb_max_y
+
+# submit texture requests for first line
+  nop                   ; mul24 r1, r1, rb_pitch
+  add t0s, ra_base, r1
+
+
+  # r3 still contains elem_num
+  add r0, ra10.16a, r3  # Load x
+  max r0, r0, 0
+  min r0, r0, rb_max_x
+  shl rb_xshift2_next, r0, 3 # Compute shifts
+
+  # r2 still contains mask
+  and r0, r0, -4
+  and r1, r0, r2
+  xor r0, r0, r1        ; mul24 r1, r1, rb_xpitch
+  add r0, r0, r1        # Add stripe offsets
+  add ra_base2, ra11, r0
+
+  mov r1, ra10.16b       # Load y
+  add ra_y2, r1, 1       # Set for next
+  max r1, r1, 0
+  min r1, r1, rb_max_y
+
+# submit texture requests for first line
+  nop                   ; mul24 r1, r1, rb_pitch
+  add t1s, ra_base2, r1
+
+# load constants
+
+  mov ra_k1, 1
+  mov ra_k256, 256
+  mov rb_k255, 255
+  mov ra_k0, 0
+
+# touch vertical context to keep simulator happy
+
+  mov ra8,  0           ; mov rb8,  0
+  mov ra9,  0           ; mov rb9,  0
+  mov ra10, 0           ; mov rb10, 0
+  mov ra11, 0           ; mov rb11, 0
+
+# Compute part of VPM to use
+  m_calc_dma_regs rb28, rb27
+
+# Weighted prediction denom
+  add rb13, unif, 9     # unif = weight denom + 6
+
+# submit texture requests for second line
+  max r1, ra_y, 0
+  min r1, r1, rb_max_y
+  add ra_y, ra_y, 1
+  mov -, unif           ; mul24 r1, r1, rb_pitch  # unused ;
+  add t0s, r1, ra_base
+
+  max r1, ra_y2, 0
+  min r1, r1, rb_max_y
+  add ra_y2, ra_y2, 1
+  nop                   ; mul24 r1, r1, rb_pitch
+  add t1s, r1, ra_base2
+
+# FALL THROUGHT TO PER-BLOCK SETUP
+
+# Start of per-block setup code
+# P and B blocks share the same setup code to save on Icache space
+:per_block_setup
+  mov.setf -, [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]
+  mov ra_link, unif
+#### We do all the setup even if we are about to exit - reading junk from unif....
+
+  mov ra1, unif         ; mov r3, elem_num  # y_x ; elem_num has implicit unpack??
+
+# per-channel shifts were calculated on the *previous* invocation
+  mov ra_xshift, ra_xshift_next
+  mov rb_xshift2, rb_xshift2_next
+
+# get base addresses and per-channel shifts for *next* invocation
+
+  add r0, ra1.16a, r3   # Load x
+  max r0, r0, 0
+  min r0, r0, rb_max_x
+
+  shl ra_xshift_next, r0, 3         # Compute shifts
+  and r0, r0, -4        ; v8subs r2, r2, r2
+  sub r2, r2, rb_pitch
+  and r1, r0, r2
+  xor r0, r0, r1        ; mul24 r1, r1, rb_xpitch
+  add r0, r0, r1        # Add stripe offsets
+  add ra_base_next, unif, r0              # Base1
+  mov ra_y_next, ra1.16b                      # Load y
+  mov ra1, unif         # x2_y2
+  nop                   # ra1 delay
+
+  add r0, ra1.16a, r3   # Load x2
+  max r0, r0, 0
+  min r0, r0, rb_max_x
+
+  shl rb_xshift2_next, r0, 3         # Compute shifts
+  and r0, r0, -4
+  and r1, r0, r2
+  xor r0, r0, r1        ; mul24 r1, r1, rb_xpitch
+  add r0, r0, r1        # Add stripe offsets
+  add rb_base2_next, unif, r0              # Base1
+  mov ra_y2_next, ra1.16b                      # Load y
+  mov ra_width_height, unif         # width_height
+
+# set up VPM write
+  mov vw_setup, rb28    # [ra1 delay]
+
+# get width,height of block (unif load above)
+  sub rb29, rb24, ra_width # Compute vdw_setup1(dst_pitch-width)
+  add rb17, ra_height, 5  ; mov r0, ra_height
+  mov r1, 16
+  min r0, r0, r1
+  add rb18, r0, 7
+  shl r0,   r0, 7
+  add r0,   r0, ra_width                        # Combine width and height of destination area
+  shl r0,   r0, i_shift16                       # Shift into bits 16 upwards of the vdw_setup0 register
+  add rb26, r0, rb27                 ; mov r0, unif   # Packed filter offsets
+
+# get filter coefficients and discard unused B frame values
+  shl.ifz r0, r0, i_shift16          ; mov ra5, unif    #  Pick half to use ; L0 offset/weight
+  mov r2, 0x01040400                 # [ra5 delay]
+  shl ra8, r0, 3                     ; mov rb14, ra5.16a
+
+# Pack the 1st 4 filter coefs for H & V tightly
+
+  mov r1,0x00010100  # -ve
+  ror ra2.8a, r1, ra8.8d
+  ror ra0.8a, r1, ra8.8c
+
+  ror ra2.8b, r2, ra8.8d
+  ror ra0.8b, r2, ra8.8c
+
+  mov r1,0x050b0a00  # -ve
+  ror ra2.8c, r1, ra8.8d
+  ror ra0.8c, r1, ra8.8c
+
+  mov r1,0x11283a40
+  ror ra2.8d, r1, ra8.8d
+  ror ra0.8d, r1, ra8.8c
+
+# In the 2nd vertical half we use b registers due to
+# using a-side fifo regs. The easiest way to achieve this to pack it
+# and then unpack!
+
+  mov r1,0x3a281100
+  ror ra3.8a, r1, ra8.8d
+  ror ra1.8a, r1, ra8.8c
+
+  mov r1,0x0a0b0500  # -ve
+  ror ra3.8b, r1, ra8.8d
+  ror ra1.8b, r1, ra8.8c
+
+  mov r1,0x04040100
+  ror ra3.8c, r1, ra8.8d
+  ror ra1.8c, r1, ra8.8c
+
+  mov r1,0x01010000  # -ve
+  ror ra3.8d, r1, ra8.8d
+  ror ra1.8d, r1, ra8.8c
+
+# Extract weighted prediction information in parallel
+# We are annoyingly A src limited here
+
+  mov rb4, ra3.8a            ; mov ra18, unif
+  mov rb5, ra3.8b
+  mov rb6, ra3.8c
+  mov.ifnz ra5, ra18
+
+  mov rb_dest, unif     # Destination address
+
+  bra -, ra_link
+
+  shl r0, ra5.16b, rb13      # Offset calc
+  asr rb12, r0, 9            # For B l1 & L0 offsets should be identical so it doesn't matter which we use
+  mov r3, 0                  ; mov rb7, ra3.8d
+# >>> branch ra_link
+#
+# r3 = 0
+# ra18.16a = weight L1
+# ra5.16a  = weight L0/L1 depending on side (wanted for 2x mono-pred)
+# rb12     = (((is P) ? offset L0/L1 * 2 : offset L1 + offset L0) + 1) << (rb13 - 1)
+# rb13     = weight denom + 6 + 9
+# rb14     = weight L0
+
+
+################################################################################
+# mc_filter(y_x, base, y2_x2, base2, width_height, my2_mx2_my_mx, offsetweight0, this_dst, next_kernel)
+# In a P block, y2_x2 should be y_x+8
+# At this point we have already issued two pairs of texture requests for the current block
+
+::mc_filter
+# ra5.16a = weight << 16; We want weight * 2 in rb14
+
+  shl rb14, ra5.16a, 1
+
+# r3 = 0
+
+:yloop
+# retrieve texture results and pick out bytes
+# then submit two more texture requests
+
+# If we knew there was no clipping then this code would get simpler.
+# Perhaps we could add on the pitch and clip using larger values?
+
+# N.B. Whilst y == y2 as far as this loop is concerned we will start
+# the grab for the next block before we finish with this block and that
+# might be B where y != y2 so we must do full processing on both y and y2
+
+  sub.setf -, r3, rb17      ; v8adds r3, r3, ra_k1                           ; ldtmu0
+  shr r0, r4, ra_xshift     ; mov.ifz ra_base2, rb_base2_next    ; ldtmu1
+  mov.ifz ra_base, ra_base_next ; mov rb31, r3
+  mov.ifz ra_y, ra_y_next   ; mov r3, rb_pitch
+  shr r1, r4, rb_xshift2    ; mov.ifz ra_y2, ra_y2_next
+
+  max r2, ra_y, 0  # y
+  min r2, r2, rb_max_y
+  add ra_y, ra_y, 1            ; mul24 r2, r2, r3
+  add t0s, ra_base, r2   ; v8min r0, r0, rb_k255 # v8subs masks out all but bottom byte
+
+  max r2, ra_y2, 0  # y
+  min r2, r2, rb_max_y
+  add ra_y2, ra_y2, 1          ; mul24 r2, r2, r3
+  add t1s, ra_base2, r2  ; v8min r1, r1, rb_k255
+
+# generate seven shifted versions
+# interleave with scroll of vertical context
+
+  mov.setf -, [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]
+
+# apply horizontal filter
+  nop                  ; mul24      r3, ra0.8a,      r0
+  nop                  ; mul24.ifnz r3, ra0.8a << 8, r1 << 8    @ "mul_used", 0
+  nop                  ; mul24      r2, ra0.8b << 1, r0 << 1    @ "mul_used", 0
+  nop                  ; mul24.ifnz r2, ra0.8b << 9, r1 << 9    @ "mul_used", 0
+  sub r2, r2, r3       ; mul24      r3, ra0.8c << 2, r0 << 2    @ "mul_used", 0
+  nop                  ; mul24.ifnz r3, ra0.8c << 10, r1 << 10  @ "mul_used", 0
+  sub r2, r2, r3       ; mul24      r3, ra0.8d << 3, r0 << 3    @ "mul_used", 0
+  nop                  ; mul24.ifnz r3, ra0.8d << 11, r1 << 11  @ "mul_used", 0
+  add r2, r2, r3       ; mul24      r3, ra1.8a << 4, r0 << 4    @ "mul_used", 0
+  nop                  ; mul24.ifnz r3, ra1.8a << 12, r1 << 12  @ "mul_used", 0
+  add r2, r2, r3       ; mul24      r3, ra1.8b << 5, r0 << 5    @ "mul_used", 0
+  nop                  ; mul24.ifnz r3, ra1.8b << 13, r1 << 13  @ "mul_used", 0
+  sub r2, r2, r3       ; mul24      r3, ra1.8c << 6, r0 << 6    @ "mul_used", 0
+  nop                  ; mul24.ifnz r3, ra1.8c << 14, r1 << 14  @ "mul_used", 0
+  add r2, r2, r3       ; mul24      r3, ra1.8d << 7, r0 << 7    @ "mul_used", 0
+  nop                  ; mul24.ifnz r3, ra1.8d << 15, r1 << 15  @ "mul_used", 0
+  sub r0, r2, r3       ; mov r3, rb31
+
+  sub.setf -, r3, 8       ; mov r1,   ra8
+  mov ra8,  ra9           ; mov rb8,  rb9
+  brr.anyn -, r:yloop
+  mov ra9,  ra10          ; mov rb9,  rb10
+  mov ra10, ra11          ; mov rb10, rb11
+  mov ra11, r0            ; mov rb11, r1
+  # >>> .anyn yloop
+
+  # apply vertical filter and write to VPM
+
+  nop                     ; mul24 r0, rb8,  ra2.8a
+  nop                     ; mul24 r1, rb9,  ra2.8b
+  sub r1, r1, r0          ; mul24 r0, rb10, ra2.8c
+  sub r1, r1, r0          ; mul24 r0, rb11, ra2.8d
+  add r1, r1, r0          ; mul24 r0, ra8,  rb4
+  add r1, r1, r0          ; mul24 r0, ra9,  rb5
+  sub r1, r1, r0          ; mul24 r0, ra10, rb6
+  add r1, r1, r0          ; mul24 r0, ra11, rb7
+  sub r1, r1, r0          ; mov -, vw_wait
+# At this point r1 is a 22-bit signed quantity: 8 (original sample),
+#  +6, +6 (each pass), +1 (the passes can overflow slightly), +1 (sign)
+# The top 8 bits have rubbish in them as mul24 is unsigned
+# The low 6 bits need discard before weighting
+  sub.setf -, r3, rb18    ; mul24 r1, r1, ra_k256  # x256 - sign extend & discard rubbish
+  asr r1, r1, 14
+  nop                     ; mul24 r1, r1, rb14
+  add r1, r1, rb12
+
+  shl r1, r1, 8
+  brr.anyn -, r:yloop
+  asr r1, r1, rb13
+# We have a saturating pack unit - I can't help feeling it should be useful here
+  min r1, r1, rb_k255       # Delay 2  rb_k255 = 255
+  max vpm, r1, 0         # Delay 3
+# >>> branch.anyn yloop
+
+# If looping again the we consumed 16 height last loop
+  # rb29 (stride) remains constant
+  # rb17 remains const (based on total height)
+  # recalc rb26, rb18 based on new segment height
+  # N.B. r3 is loop counter still
+
+  mov r1, 16
+  sub r0, ra_height, r1
+  mov ra_height, r0
+  max.setf r0, r0, 0    # Done if Z now
+
+# DMA out
+  brr.anyz -, r:per_block_setup
+  mov vw_setup, rb26 # VDW setup 0    Delay 1
+  mov vw_setup, rb29 # Stride         Delay 2
+  mov vw_addr, rb_dest # start the VDW   Delay 3
+# >>> .anyz per_block_setup
+
+  min r0, r0, r1
+  add rb18, rb18, r0
+  sub r0, r0, r1
+  shl r0, r0, i_shift23
+  add rb26, rb26, r0
+
+  nop ; mul24 r0, r1, rb_pitch  # r0 = pitch*16
+  add rb_dest, rb_dest, r0
+
+  mov vw_setup, rb28    # Reset our VDM write pointer
+
+  brr -, r:yloop
+  nop
+  nop
+  nop
+# >>>
+
+
+
+
+
+################################################################################
+
+# mc_filter_b(y_x, base, y2_x2, base2, width_height, my2_mx2_my_mx, offsetweight0, this_dst, next_kernel)
+# In a P block, only the first half of coefficients contain used information.
+# At this point we have already issued two pairs of texture requests for the current block
+# May be better to just send 16.16 motion vector and figure out the coefficients inside this block (only 4 cases so can compute hcoeffs in around 24 cycles?)
+# Can fill in the coefficients so only
+# Can also assume default weighted prediction for B frames.
+# Perhaps can unpack coefficients in a more efficient manner by doing H/V for a and b at the same time?
+# Or possibly by taking advantage of symmetry?
+# From 19->7 32bits per command.
+
+::mc_filter_b
+  # r0 = weightL0 << 16, we want it in rb14
+#  asr rb14, r0, i_shift16
+
+:yloopb
+# retrieve texture results and pick out bytes
+# then submit two more texture requests
+
+# If we knew there was no clipping then this code would get simpler.
+# Perhaps we could add on the pitch and clip using larger values?
+
+  sub.setf -, r3, rb17      ; v8adds r3, r3, ra_k1                           ; ldtmu0
+  shr r0, r4, ra_xshift     ; mov.ifz ra_base2, rb_base2_next    ; ldtmu1
+  mov.ifz ra_base, ra_base_next ; mov rb31, r3
+  mov.ifz ra_y, ra_y_next   ; mov r3, rb_pitch
+  shr r1, r4, rb_xshift2    ; mov.ifz ra_y2, ra_y2_next
+
+  max r2, ra_y, 0  # y
+  min r2, r2, rb_max_y
+  add ra_y, ra_y, 1            ; mul24 r2, r2, r3
+  add t0s, ra_base, r2   ; v8min r0, r0, rb_k255 # v8subs masks out all but bottom byte
+
+  max r2, ra_y2, 0  # y
+  min r2, r2, rb_max_y
+  add ra_y2, ra_y2, 1          ; mul24 r2, r2, r3
+  add t1s, ra_base2, r2  ; v8min r1, r1, rb_k255
+
+# generate seven shifted versions
+# interleave with scroll of vertical context
+
+  mov.setf -, [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]
+
+# apply horizontal filter
+  nop                  ; mul24      r3, ra0.8a,      r0
+  nop                  ; mul24.ifnz r3, ra0.8a << 8, r1 << 8    @ "mul_used", 0
+  nop                  ; mul24      r2, ra0.8b << 1, r0 << 1    @ "mul_used", 0
+  nop                  ; mul24.ifnz r2, ra0.8b << 9, r1 << 9    @ "mul_used", 0
+  sub r2, r2, r3       ; mul24      r3, ra0.8c << 2, r0 << 2    @ "mul_used", 0
+  nop                  ; mul24.ifnz r3, ra0.8c << 10, r1 << 10  @ "mul_used", 0
+  sub r2, r2, r3       ; mul24      r3, ra0.8d << 3, r0 << 3    @ "mul_used", 0
+  nop                  ; mul24.ifnz r3, ra0.8d << 11, r1 << 11  @ "mul_used", 0
+  add r2, r2, r3       ; mul24      r3, ra1.8a << 4, r0 << 4    @ "mul_used", 0
+  nop                  ; mul24.ifnz r3, ra1.8a << 12, r1 << 12  @ "mul_used", 0
+  add r2, r2, r3       ; mul24      r3, ra1.8b << 5, r0 << 5    @ "mul_used", 0
+  nop                  ; mul24.ifnz r3, ra1.8b << 13, r1 << 13  @ "mul_used", 0
+  sub r2, r2, r3       ; mul24      r3, ra1.8c << 6, r0 << 6    @ "mul_used", 0
+  nop                  ; mul24.ifnz r3, ra1.8c << 14, r1 << 14  @ "mul_used", 0
+  add r2, r2, r3       ; mul24      r3, ra1.8d << 7, r0 << 7    @ "mul_used", 0
+  nop                  ; mul24.ifnz r3, ra1.8d << 15, r1 << 15  @ "mul_used", 0
+  sub r0, r2, r3       ; mov r3, rb31
+
+  sub.setf -, r3, 8       ; mov r1,   ra8
+  mov ra8,  ra9           ; mov rb8,  rb9
+  brr.anyn -, r:yloopb
+  mov ra9,  ra10          ; mov rb9,  rb10
+  mov ra10, ra11          ; mov rb10, rb11
+  mov ra11, r0            ; mov rb11, r1
+  # >>> .anyn yloopb
+
+  # apply vertical filter and write to VPM
+  nop                     ; mul24 r0, rb8,  ra2.8a
+  nop                     ; mul24 r1, rb9,  ra2.8b
+  sub r1, r1, r0          ; mul24 r0, rb10, ra2.8c
+  sub r1, r1, r0          ; mul24 r0, rb11, ra2.8d
+  add r1, r1, r0          ; mul24 r0, ra8,  rb4
+  add r1, r1, r0          ; mul24 r0, ra9,  rb5
+  sub r1, r1, r0          ; mul24 r0, ra10, rb6
+  add r1, r1, r0          ; mul24 r0, ra11, rb7
+  sub r1, r1, r0          ; mov r2, rb12
+# As with P-pred r1 is a 22-bit signed quantity in 32-bits
+# Top 8 bits are bad - low 6 bits should be discarded
+  sub.setf -, r3, rb18    ; mul24 r1, r1, ra_k256
+
+  asr r1, r1, 14
+  nop                     ; mul24 r0, r1, rb14
+  add r0, r0, r2          ; mul24 r1, r1 << 8, ra18.16a << 8    @ "mul_used", 0
+
+  add r1, r1, r0          ; mov -, vw_wait
+  shl r1, r1, 8
+
+  brr.anyn -, r:yloopb
+  asr r1, r1, rb13         # Delay 1
+  min r1, r1, rb_k255       # Delay 2
+  max vpm, r1, 0         # Delay 3
+
+
+# If looping again the we consumed 16 height last loop
+  # rb29 (stride) remains constant
+  # rb17 remains const (based on total height)
+  # recalc rb26, rb18 based on new segment height
+  # N.B. r3 is loop counter still
+
+  mov r1, 16
+  sub r0, ra_height, r1
+  mov ra_height, r0
+  max.setf r0, r0, 0    # Done if Z now
+
+# DMA out
+  brr.anyz -, r:per_block_setup
+  mov vw_setup, rb26 # VDW setup 0    Delay 1
+  mov vw_setup, rb29 # Stride         Delay 2
+  mov vw_addr, rb_dest # start the VDW   Delay 3
+# >>> .anyz per_block_setup
+
+  min r0, r0, r1
+  add rb18, rb18, r0
+  sub r0, r0, r1
+  shl r0, r0, i_shift23
+  add rb26, rb26, r0
+
+  nop ; mul24 r0, r1, rb_pitch  # r0 = pitch*16
+  add rb_dest, rb_dest, r0
+
+  mov vw_setup, rb28    # Reset our VDM write pointer
+
+  brr -, r:yloopb
+  nop
+  nop
+  nop
+
+################################################################################
+
+::mc_end
+# Do not add code here because mc_end must appear after all other code.
diff -Naur ffmpeg-3.2.4/libavcodec/rpi_zc.c ffmpeg-3.2.4.patch/libavcodec/rpi_zc.c
--- ffmpeg-3.2.4/libavcodec/rpi_zc.c	1970-01-01 01:00:00.000000000 +0100
+++ ffmpeg-3.2.4.patch/libavcodec/rpi_zc.c	2017-05-28 20:42:45.755088727 +0200
@@ -0,0 +1,581 @@
+#include "config.h"
+#ifdef RPI
+#include "rpi_qpu.h"
+#include "rpi_mailbox.h"
+#include "rpi_zc.h"
+#include "libavutil/avassert.h"
+#include <pthread.h>
+
+#include "libavutil/buffer_internal.h"
+#include <interface/vctypes/vc_image_types.h>
+
+#define TRACE_ALLOC 0
+
+struct ZcPoolEnt;
+
+typedef struct ZcPool
+{
+    int numbytes;
+    unsigned int n;
+    struct ZcPoolEnt * head;
+    pthread_mutex_t lock;
+} ZcPool;
+
+typedef struct ZcPoolEnt
+{
+    // It is important that we start with gmem as other bits of code will expect to see that
+    GPU_MEM_PTR_T gmem;
+    unsigned int n;
+    struct ZcPoolEnt * next;
+    struct ZcPool * pool;
+} ZcPoolEnt;
+
+#if 1
+//#define ALLOC_PAD       0x1000
+#define ALLOC_PAD       0
+#define ALLOC_ROUND     0x1000
+//#define ALLOC_N_OFFSET  0x100
+#define ALLOC_N_OFFSET  0
+#define STRIDE_ROUND    0x80
+#define STRIDE_OR       0x80
+#else
+#define ALLOC_PAD       0
+#define ALLOC_ROUND     0x1000
+#define ALLOC_N_OFFSET  0
+#define STRIDE_ROUND    32
+#define STRIDE_OR       0
+#endif
+
+#define DEBUG_ZAP0_BUFFERS 0
+
+
+static ZcPoolEnt * zc_pool_ent_alloc(ZcPool * const pool, const unsigned int req_size)
+{
+    ZcPoolEnt * const zp = av_malloc(sizeof(ZcPoolEnt));
+
+    // Round up to 4k & add 4k
+    const unsigned int alloc_size = (req_size + ALLOC_PAD + ALLOC_ROUND - 1) & ~(ALLOC_ROUND - 1);
+
+    if (zp == NULL) {
+        av_log(NULL, AV_LOG_ERROR, "av_malloc(ZcPoolEnt) failed\n");
+        goto fail0;
+    }
+
+    if (gpu_malloc_cached(alloc_size, &zp->gmem) != 0)
+    {
+        av_log(NULL, AV_LOG_ERROR, "av_gpu_malloc_cached(%d) failed\n", alloc_size);
+        goto fail1;
+    }
+
+#if TRACE_ALLOC
+    printf("%s: Alloc %#x bytes @ %p\n", __func__, zp->gmem.numbytes, zp->gmem.arm);
+#endif
+
+    pool->numbytes = zp->gmem.numbytes;
+    zp->next = NULL;
+    zp->pool = pool;
+    zp->n = pool->n++;
+    return zp;
+
+fail1:
+    av_free(zp);
+fail0:
+    return NULL;
+}
+
+static void zc_pool_ent_free(ZcPoolEnt * const zp)
+{
+#if TRACE_ALLOC
+    printf("%s: Free %#x bytes @ %p\n", __func__, zp->gmem.numbytes, zp->gmem.arm);
+#endif
+
+    gpu_free(&zp->gmem);
+    av_free(zp);
+}
+
+static void zc_pool_flush(ZcPool * const pool)
+{
+    ZcPoolEnt * p = pool->head;
+    pool->head = NULL;
+    pool->numbytes = -1;
+
+    while (p != NULL)
+    {
+        ZcPoolEnt * const zp = p;
+        p = p->next;
+        zc_pool_ent_free(zp);
+    }
+}
+
+static ZcPoolEnt * zc_pool_alloc(ZcPool * const pool, const int req_bytes)
+{
+    ZcPoolEnt * zp;
+    int numbytes;
+
+    pthread_mutex_lock(&pool->lock);
+
+    numbytes = pool->numbytes;
+
+    // If size isn't close then dump the pool
+    // Close in this context means within 128k
+    if (req_bytes > numbytes || req_bytes + 0x20000 < numbytes)
+    {
+        zc_pool_flush(pool);
+        numbytes = req_bytes;
+    }
+
+    if (pool->head != NULL)
+    {
+        zp = pool->head;
+        pool->head = zp->next;
+    }
+    else
+    {
+        zp = zc_pool_ent_alloc(pool, numbytes);
+    }
+
+    pthread_mutex_unlock(&pool->lock);
+
+    // Start with our buffer empty of preconceptions
+//    rpi_cache_flush_one_gm_ptr(&zp->gmem, RPI_CACHE_FLUSH_MODE_INVALIDATE);
+
+    return zp;
+}
+
+static void zc_pool_free(ZcPoolEnt * const zp)
+{
+    ZcPool * const pool = zp == NULL ? NULL : zp->pool;
+    if (zp != NULL)
+    {
+        pthread_mutex_lock(&pool->lock);
+#if TRACE_ALLOC
+        printf("%s: Recycle %#x, %#x\n", __func__, pool->numbytes, zp->gmem.numbytes);
+#endif
+
+        if (pool->numbytes == zp->gmem.numbytes)
+        {
+            zp->next = pool->head;
+            pool->head = zp;
+            pthread_mutex_unlock(&pool->lock);
+        }
+        else
+        {
+            pthread_mutex_unlock(&pool->lock);
+            zc_pool_ent_free(zp);
+        }
+    }
+}
+
+static void
+zc_pool_init(ZcPool * const pool)
+{
+    pool->numbytes = -1;
+    pool->head = NULL;
+    pthread_mutex_init(&pool->lock, NULL);
+}
+
+static void
+zc_pool_destroy(ZcPool * const pool)
+{
+    pool->numbytes = -1;
+    zc_pool_flush(pool);
+    pthread_mutex_destroy(&pool->lock);
+}
+
+typedef struct ZcOldCtxVals
+{
+    int thread_safe_callbacks;
+    int (*get_buffer2)(struct AVCodecContext *s, AVFrame *frame, int flags);
+    void * get_buffer_context;
+} ZcOldCtxVals;
+
+typedef struct AVZcEnv
+{
+    unsigned int refcount;
+    ZcPool pool;
+    ZcOldCtxVals old;
+} ZcEnv;
+
+// Callback when buffer unrefed to zero
+static void rpi_free_display_buffer(void *opaque, uint8_t *data)
+{
+    ZcPoolEnt *const zp = opaque;
+//    printf("%s: data=%p\n", __func__, data);
+    zc_pool_free(zp);
+}
+
+static inline GPU_MEM_PTR_T * pic_gm_ptr(AVBufferRef * const buf)
+{
+    // Kludge where we check the free fn to check this is really
+    // one of our buffers - can't think of a better way
+    return buf == NULL || buf->buffer->free != rpi_free_display_buffer ? NULL :
+        av_buffer_get_opaque(buf);
+}
+
+AVRpiZcFrameGeometry av_rpi_zc_frame_geometry(
+    const int format, const unsigned int video_width, const unsigned int video_height)
+{
+    AVRpiZcFrameGeometry geo;
+
+    switch (format)
+    {
+        case AV_PIX_FMT_YUV420P:
+            geo.stride_y = ((video_width + 32 + STRIDE_ROUND - 1) & ~(STRIDE_ROUND - 1)) | STRIDE_OR;
+        //    geo.stride_y = ((video_width + 32 + 31) & ~31);
+            geo.stride_c = geo.stride_y / 2;
+        //    geo.height_y = (video_height + 15) & ~15;
+            geo.height_y = (video_height + 32 + 31) & ~31;
+            geo.height_c = geo.height_y / 2;
+            geo.planes_c = 2;
+            geo.stripes = 1;
+            break;
+
+        case AV_PIX_FMT_SAND128:
+        {
+            const unsigned int stripe_w = 128;
+
+            static pthread_mutex_t sand_lock = PTHREAD_MUTEX_INITIALIZER;
+            static VC_IMAGE_T img = {0};
+
+            // Given the overhead of calling the mailbox keep a stashed
+            // copy as we will almost certainly just want the same numbers again
+            // but that means we need a lock
+            pthread_mutex_lock(&sand_lock);
+
+            if (img.width != video_width || img.height != video_height)
+            {
+                VC_IMAGE_T new_img = {
+                    .type = VC_IMAGE_YUV_UV,
+                    .width = video_width,
+                    .height = video_height
+                };
+
+                gpu_ref();
+                mbox_get_image_params(gpu_get_mailbox(), &new_img);
+                gpu_unref();
+                img = new_img;
+            }
+
+            geo.stride_y = stripe_w;
+            geo.stride_c = stripe_w;
+            geo.height_y = ((intptr_t)img.extra.uv.u - (intptr_t)img.image_data) / stripe_w;
+            geo.height_c = img.pitch / stripe_w - geo.height_y;
+            geo.planes_c = 1;
+            geo.stripes = (video_width + stripe_w - 1) / stripe_w;
+
+            pthread_mutex_unlock(&sand_lock);
+
+            av_assert0((int)geo.height_y > 0 && (int)geo.height_c > 0);
+            av_assert0(geo.height_y >= video_height && geo.height_c >= video_height / 2);
+            break;
+        }
+
+        default:
+            memset(&geo, 0, sizeof(geo));
+            break;
+    }
+    return geo;
+}
+
+
+static AVBufferRef * rpi_buf_pool_alloc(ZcPool * const pool, int size)
+{
+    ZcPoolEnt *const zp = zc_pool_alloc(pool, size);
+    AVBufferRef * buf;
+    intptr_t idata = (intptr_t)zp->gmem.arm;
+#if ALLOC_N_OFFSET != 0
+    intptr_t noff = (zp->n * ALLOC_N_OFFSET) & (ALLOC_PAD - 1);
+#endif
+
+    if (zp == NULL) {
+        av_log(NULL, AV_LOG_ERROR, "zc_pool_alloc(%d) failed\n", size);
+        goto fail0;
+    }
+
+#if ALLOC_N_OFFSET != 0
+    idata = ((idata & ~(ALLOC_PAD - 1)) | noff) + (((idata & (ALLOC_PAD - 1)) > noff) ? ALLOC_PAD : 0);
+#endif
+
+#if DEBUG_ZAP0_BUFFERS
+    memset((void*)idata, 0, size);
+#endif
+
+    if ((buf = av_buffer_create((void *)idata, size, rpi_free_display_buffer, zp, AV_BUFFER_FLAG_READONLY)) == NULL)
+    {
+        av_log(NULL, AV_LOG_ERROR, "av_buffer_create() failed\n");
+        goto fail2;
+    }
+
+    return buf;
+
+fail2:
+    zc_pool_free(zp);
+fail0:
+    return NULL;
+}
+
+static int rpi_get_display_buffer(ZcEnv *const zc, AVFrame * const frame)
+{
+    const AVRpiZcFrameGeometry geo = av_rpi_zc_frame_geometry(frame->format, frame->width, frame->height);
+    const unsigned int size_y = geo.stride_y * geo.height_y;
+    const unsigned int size_c = geo.stride_c * geo.height_c;
+    const unsigned int size_pic = (size_y + size_c * geo.planes_c) * geo.stripes;
+    AVBufferRef * buf;
+    unsigned int i;
+
+//    printf("Do local alloc: format=%#x, %dx%d: %u\n", frame->format, frame->width, frame->height, size_pic);
+
+    if ((buf = rpi_buf_pool_alloc(&zc->pool, size_pic)) == NULL)
+    {
+        av_log(NULL, AV_LOG_ERROR, "rpi_get_display_buffer: Failed to get buffer from pool\n");
+        return AVERROR(ENOMEM);
+    }
+
+    for (i = 0; i < AV_NUM_DATA_POINTERS; i++) {
+        frame->buf[i] = NULL;
+        frame->data[i] = NULL;
+        frame->linesize[i] = 0;
+    }
+
+    frame->buf[0] = buf;
+
+    frame->linesize[0] = geo.stride_y;
+    frame->linesize[1] = geo.stride_c;
+    frame->linesize[2] = geo.stride_c;
+    if (geo.stripes > 1)
+        frame->linesize[3] = geo.height_y + geo.height_c;      // abuse: linesize[3] = stripe stride
+
+    frame->data[0] = buf->data;
+    frame->data[1] = frame->data[0] + size_y;
+    if (geo.planes_c > 1)
+        frame->data[2] = frame->data[1] + size_c;
+
+    frame->extended_data = frame->data;
+    // Leave extended buf alone
+
+    return 0;
+}
+
+#define RPI_GET_BUFFER2 1
+
+int av_rpi_zc_get_buffer2(struct AVCodecContext *s, AVFrame *frame, int flags)
+{
+#if !RPI_GET_BUFFER2
+    return avcodec_default_get_buffer2(s, frame, flags);
+#else
+    int rv;
+
+    if ((s->codec->capabilities & AV_CODEC_CAP_DR1) == 0)
+    {
+//        printf("Do default alloc: format=%#x\n", frame->format);
+        rv = avcodec_default_get_buffer2(s, frame, flags);
+    }
+    else if (frame->format == AV_PIX_FMT_YUV420P ||
+             frame->format == AV_PIX_FMT_SAND128)
+    {
+        rv = rpi_get_display_buffer(s->get_buffer_context, frame);
+    }
+    else
+    {
+        rv = avcodec_default_get_buffer2(s, frame, flags);
+    }
+
+#if 0
+    printf("%s: fmt:%d, %dx%d lsize=%d/%d/%d/%d data=%p/%p/%p bref=%p/%p/%p opaque[0]=%p\n", __func__,
+        frame->format, frame->width, frame->height,
+        frame->linesize[0], frame->linesize[1], frame->linesize[2], frame->linesize[3],
+        frame->data[0], frame->data[1], frame->data[2],
+        frame->buf[0], frame->buf[1], frame->buf[2],
+        av_buffer_get_opaque(frame->buf[0]));
+#endif
+    return rv;
+#endif
+}
+
+
+static AVBufferRef * zc_copy(struct AVCodecContext * const s,
+    const AVFrame * const src)
+{
+    AVFrame dest_frame;
+    AVFrame * const dest = &dest_frame;
+    unsigned int i;
+    uint8_t * psrc, * pdest;
+
+    dest->width = src->width;
+    dest->height = src->height;
+
+    if (rpi_get_display_buffer(s->get_buffer_context, dest) != 0)
+    {
+        return NULL;
+    }
+
+    for (i = 0, psrc = src->data[0], pdest = dest->data[0];
+         i != dest->height;
+         ++i, psrc += src->linesize[0], pdest += dest->linesize[0])
+    {
+        memcpy(pdest, psrc, dest->width);
+    }
+    for (i = 0, psrc = src->data[1], pdest = dest->data[1];
+         i != dest->height / 2;
+         ++i, psrc += src->linesize[1], pdest += dest->linesize[1])
+    {
+        memcpy(pdest, psrc, dest->width / 2);
+    }
+    for (i = 0, psrc = src->data[2], pdest = dest->data[2];
+         i != dest->height / 2;
+         ++i, psrc += src->linesize[2], pdest += dest->linesize[2])
+    {
+        memcpy(pdest, psrc, dest->width / 2);
+    }
+
+    return dest->buf[0];
+}
+
+
+AVRpiZcRefPtr av_rpi_zc_ref(struct AVCodecContext * const s,
+    const AVFrame * const frame, const int maycopy)
+{
+    assert(s != NULL);
+
+    if (frame->format != AV_PIX_FMT_YUV420P &&
+        frame->format != AV_PIX_FMT_SAND128)
+    {
+        av_log(s, AV_LOG_WARNING, "%s: *** Format not SAND/YUV420P: %d\n", __func__, frame->format);
+        return NULL;
+    }
+
+    if (frame->buf[1] != NULL)
+    {
+        av_assert0(frame->format == AV_PIX_FMT_YUV420P);
+        if (maycopy)
+        {
+            av_log(s, AV_LOG_INFO, "%s: *** Not a single buf frame: copying\n", __func__);
+            return zc_copy(s, frame);
+        }
+        else
+        {
+            av_log(s, AV_LOG_WARNING, "%s: *** Not a single buf frame: NULL\n", __func__);
+            return NULL;
+        }
+    }
+
+    if (pic_gm_ptr(frame->buf[0]) == NULL)
+    {
+        if (maycopy)
+        {
+            av_log(s, AV_LOG_INFO, "%s: *** Not one of our buffers: copying\n", __func__);
+            return zc_copy(s, frame);
+        }
+        else
+        {
+            av_log(s, AV_LOG_WARNING, "%s: *** Not one of our buffers: NULL\n", __func__);
+            return NULL;
+        }
+    }
+
+    return av_buffer_ref(frame->buf[0]);
+}
+
+int av_rpi_zc_vc_handle(const AVRpiZcRefPtr fr_ref)
+{
+    const GPU_MEM_PTR_T * const p = pic_gm_ptr(fr_ref);
+    return p == NULL ? -1 : p->vc_handle;
+}
+
+int av_rpi_zc_offset(const AVRpiZcRefPtr fr_ref)
+{
+    const GPU_MEM_PTR_T * const p = pic_gm_ptr(fr_ref);
+    return p == NULL ? 0 : fr_ref->data - p->arm;
+}
+
+int av_rpi_zc_length(const AVRpiZcRefPtr fr_ref)
+{
+    return fr_ref == NULL ? 0 : fr_ref->size;
+}
+
+
+int av_rpi_zc_numbytes(const AVRpiZcRefPtr fr_ref)
+{
+    const GPU_MEM_PTR_T * const p = pic_gm_ptr(fr_ref);
+    return p == NULL ? 0 : p->numbytes;
+}
+
+void av_rpi_zc_unref(AVRpiZcRefPtr fr_ref)
+{
+    if (fr_ref != NULL)
+    {
+        av_buffer_unref(&fr_ref);
+    }
+}
+
+AVZcEnvPtr av_rpi_zc_env_alloc(void)
+{
+    ZcEnv * const zc = av_mallocz(sizeof(ZcEnv));
+    if (zc == NULL)
+    {
+        av_log(NULL, AV_LOG_ERROR, "av_rpi_zc_env_alloc: Context allocation failed\n");
+        return NULL;
+    }
+
+    zc_pool_init(&zc->pool);
+    return zc;
+}
+
+void av_rpi_zc_env_free(AVZcEnvPtr zc)
+{
+    if (zc != NULL)
+    {
+        zc_pool_destroy(&zc->pool); ;
+        av_free(zc);
+    }
+}
+
+int av_rpi_zc_in_use(const struct AVCodecContext * const s)
+{
+    return s->get_buffer2 == av_rpi_zc_get_buffer2;
+}
+
+int av_rpi_zc_init(struct AVCodecContext * const s)
+{
+    if (av_rpi_zc_in_use(s))
+    {
+        ZcEnv * const zc = s->get_buffer_context;
+        ++zc->refcount;
+    }
+    else
+    {
+        ZcEnv *const zc = av_rpi_zc_env_alloc();
+        if (zc == NULL)
+        {
+            return AVERROR(ENOMEM);
+        }
+
+        zc->refcount = 1;
+        zc->old.get_buffer_context = s->get_buffer_context;
+        zc->old.get_buffer2 = s->get_buffer2;
+        zc->old.thread_safe_callbacks = s->thread_safe_callbacks;
+
+        s->get_buffer_context = zc;
+        s->get_buffer2 = av_rpi_zc_get_buffer2;
+        s->thread_safe_callbacks = 1;
+    }
+    return 0;
+}
+
+void av_rpi_zc_uninit(struct AVCodecContext * const s)
+{
+    if (av_rpi_zc_in_use(s))
+    {
+        ZcEnv * const zc = s->get_buffer_context;
+        if (--zc->refcount == 0)
+        {
+            s->get_buffer2 = zc->old.get_buffer2;
+            s->get_buffer_context = zc->old.get_buffer_context;
+            s->thread_safe_callbacks = zc->old.thread_safe_callbacks;
+            av_rpi_zc_env_free(zc);
+        }
+    }
+}
+
+#endif  // RPI
+
diff -Naur ffmpeg-3.2.4/libavcodec/rpi_zc.h ffmpeg-3.2.4.patch/libavcodec/rpi_zc.h
--- ffmpeg-3.2.4/libavcodec/rpi_zc.h	1970-01-01 01:00:00.000000000 +0100
+++ ffmpeg-3.2.4.patch/libavcodec/rpi_zc.h	2017-05-28 20:42:45.756088730 +0200
@@ -0,0 +1,137 @@
+#ifndef LIBAVCODEC_RPI_ZC_H
+#define LIBAVCODEC_RPI_ZC_H
+
+// Zero-Copy frame code for RPi
+// RPi needs Y/U/V planes to be contiguous for display.  By default
+// ffmpeg will allocate separated planes so a memcpy is needed before
+// display.  This code provides a method a making ffmpeg allocate a single
+// bit of memory for the frame when can then be reference counted until
+// display has finished with it.
+
+#include "libavutil/frame.h"
+#include "libavcodec/avcodec.h"
+
+// "Opaque" pointer to whatever we are using as a buffer reference
+typedef AVBufferRef * AVRpiZcRefPtr;
+
+struct AVZcEnv;
+typedef struct AVZcEnv * AVZcEnvPtr;
+
+typedef struct AVRpiZcFrameGeometry
+{
+    unsigned int stride_y;
+    unsigned int height_y;
+    unsigned int stride_c;
+    unsigned int height_c;
+    unsigned int planes_c;
+    unsigned int stripes;
+} AVRpiZcFrameGeometry;
+
+
+AVRpiZcFrameGeometry av_rpi_zc_frame_geometry(
+    const int format,
+    const unsigned int video_width, const unsigned int video_height);
+
+// Replacement fn for avctx->get_buffer2
+// Should be set before calling avcodec_decode_open2
+//
+// N.B. in addition to to setting avctx->get_buffer2, avctx->refcounted_frames
+// must be set to 1 as otherwise the buffer info is killed before being returned
+// by avcodec_decode_video2.  Note also that this means that the AVFrame that is
+// returned must be manually derefed with av_frame_unref.  This should be done
+// after av_rpi_zc_ref has been called.
+int av_rpi_zc_get_buffer2(struct AVCodecContext *s, AVFrame *frame, int flags);
+
+// Generate a ZC reference to the buffer(s) in this frame
+// If the buffer doesn't appear to be one allocated by _get_buffer_2
+// then the behaviour depends on maycopy:
+//   If maycopy=0 then return NULL
+//   If maycopy=1 && the src frame is in a form where we can easily copy
+//     the data, then allocate a new buffer and copy the data into it
+//   Otherwise return NULL
+AVRpiZcRefPtr av_rpi_zc_ref(struct AVCodecContext * const s,
+    const AVFrame * const frame, const int maycopy);
+
+// Get the vc_handle from the frame ref
+// Returns -1 if ref doesn't look valid
+int av_rpi_zc_vc_handle(const AVRpiZcRefPtr fr_ref);
+// Get offset from the start of the memory referenced
+// by the vc_handle to valid data
+int av_rpi_zc_offset(const AVRpiZcRefPtr fr_ref);
+// Length of buffer data
+int av_rpi_zc_length(const AVRpiZcRefPtr fr_ref);
+// Get the number of bytes allocated from the frame ref
+// Returns 0 if ref doesn't look valid
+int av_rpi_zc_numbytes(const AVRpiZcRefPtr fr_ref);
+
+// Unreference the buffer refed/allocated by _zc_ref
+// If fr_ref is NULL then this will NOP
+void av_rpi_zc_unref(AVRpiZcRefPtr fr_ref);
+
+// Allocate an environment for the buffer pool used by the ZC code
+// This should be put in avctx->get_buffer_context so it can be found by
+// av_rpi_zc_get_buffer2 when it is called from ffmpeg
+AVZcEnvPtr av_rpi_zc_env_alloc(void);
+
+// Allocate the environment used by the ZC code
+void av_rpi_zc_env_free(AVZcEnvPtr);
+
+// Test to see if the context is using zc (checks get_buffer2)
+int av_rpi_zc_in_use(const struct AVCodecContext * const s);
+
+// Init ZC into a context
+// There is nothing magic in this fn - it just packages setting
+// get_buffer2 & get_buffer_context
+int av_rpi_zc_init(struct AVCodecContext * const s);
+
+// Free ZC from a context
+// There is nothing magic in this fn - it just packages unsetting
+// get_buffer2 & get_buffer_context
+void av_rpi_zc_uninit(struct AVCodecContext * const s);
+
+
+
+static inline unsigned int rpi_sliced_frame_stride2(const AVFrame * const frame)
+{
+    return frame->linesize[3];
+}
+
+static inline unsigned int rpi_sliced_frame_off_y(const AVFrame * const frame, const unsigned int x, const unsigned int y)
+{
+    const unsigned int stride1 = frame->linesize[0];
+    const unsigned int stride2 = rpi_sliced_frame_stride2(frame);
+    const unsigned int x1 = x & (stride1 - 1);
+    const unsigned int x2 = x ^ x1;
+
+    return x1 + stride1 * y + stride2 * x2;
+}
+
+static inline unsigned int rpi_sliced_frame_off_c(const AVFrame * const frame, const unsigned int x_c, const unsigned int y_c)
+{
+    const unsigned int stride1 = frame->linesize[0];
+    const unsigned int stride2 = rpi_sliced_frame_stride2(frame);
+    const unsigned int x = x_c * 2;
+    const unsigned int x1 = x & (stride1 - 1);
+    const unsigned int x2 = x ^ x1;
+
+    return x1 + stride1 * y_c + stride2 * x2;
+}
+
+static inline uint8_t * rpi_sliced_frame_pos_y(const AVFrame * const frame, const unsigned int x, const unsigned int y)
+{
+    return frame->data[0] + rpi_sliced_frame_off_y(frame, x, y);
+}
+
+static inline uint8_t * rpi_sliced_frame_pos_c(const AVFrame * const frame, const unsigned int x, const unsigned int y)
+{
+    return frame->data[1] + rpi_sliced_frame_off_c(frame, x, y);
+}
+
+static inline int rpi_sliced_frame(const AVFrame * const frame)
+{
+    return frame->format == AV_PIX_FMT_SAND128;
+}
+
+
+#endif
+
diff -Naur ffmpeg-3.2.4/libavcodec/utils.c ffmpeg-3.2.4.patch/libavcodec/utils.c
--- ffmpeg-3.2.4/libavcodec/utils.c	2017-02-10 14:25:27.000000000 +0100
+++ ffmpeg-3.2.4.patch/libavcodec/utils.c	2017-05-28 20:42:45.757088734 +0200
@@ -26,6 +26,12 @@
  */
 
 #include "config.h"
+
+#ifdef RPI
+// Move video buffers to GPU memory
+#define RPI_GPU_BUFFERS
+#endif
+
 #include "libavutil/atomic.h"
 #include "libavutil/attributes.h"
 #include "libavutil/avassert.h"
@@ -64,6 +70,10 @@
 #include "libavutil/ffversion.h"
 const char av_codec_ffversion[] = "FFmpeg version " FFMPEG_VERSION;
 
+#ifdef RPI_GPU_BUFFERS
+#include "rpi_qpu.h"
+#endif
+
 #if HAVE_PTHREADS || HAVE_W32THREADS || HAVE_OS2THREADS
 static int default_lockmgr_cb(void **arg, enum AVLockOp op)
 {
@@ -508,6 +518,47 @@
     return ret;
 }
 
+#ifdef RPI_GPU_BUFFERS
+static void rpi_buffer_default_free(void *opaque, uint8_t *data)
+{
+    GPU_MEM_PTR_T *p = opaque;
+    gpu_free(p);
+    av_free(p);
+}
+
+static AVBufferRef *rpi_buffer_alloc(int size)
+{
+    AVBufferRef *ret = NULL;
+    uint8_t    *data = NULL;
+    GPU_MEM_PTR_T *p;
+
+    static int total=0;
+    total+=size;
+
+    p = av_malloc(sizeof *p);
+    if (!p)
+        return NULL;
+
+    if (gpu_malloc_cached(size,p)<0)  // Change this line to choose cached or uncached memory.  The caching here refers to the ARM data cache.
+        return NULL;
+
+    data = p->arm;
+    printf("Rpi alloc %d/%d ARM=%p VC=%x->%x\n",size,total,p->arm,p->vc,p->vc+size);
+    //memset(data, 64, size);
+
+    if (!data)
+        return NULL;
+
+    ret = av_buffer_create(data, size, rpi_buffer_default_free, p, 0);
+    if (!ret) {
+        gpu_free(p);
+        av_freep(&p);
+    }
+
+    return ret;
+}
+#endif
+
 static int update_frame_pool(AVCodecContext *avctx, AVFrame *frame)
 {
     FramePool *pool = avctx->internal->pool;
@@ -555,6 +606,14 @@
             av_buffer_pool_uninit(&pool->pools[i]);
             pool->linesize[i] = linesize[i];
             if (size[i]) {
+#ifdef RPI_GPU_BUFFERS
+                if (avctx->codec_id == AV_CODEC_ID_HEVC)
+                    pool->pools[i] = av_buffer_pool_init(size[i] + 16 + STRIDE_ALIGN - 1,
+                                                     CONFIG_MEMORY_POISONING ?
+                                                        NULL :
+                                                        rpi_buffer_alloc);
+                else
+#endif
                 pool->pools[i] = av_buffer_pool_init(size[i] + 16 + STRIDE_ALIGN - 1,
                                                      CONFIG_MEMORY_POISONING ?
                                                         NULL :
@@ -732,6 +791,11 @@
     if (avctx->hw_frames_ctx)
         return av_hwframe_get_buffer(avctx->hw_frames_ctx, frame, 0);
 
+#ifdef RPI
+    // This is going to end badly if we let it continue
+    av_assert0(frame->format != AV_PIX_FMT_SAND128);
+#endif
+
     if ((ret = update_frame_pool(avctx, frame)) < 0)
         return ret;
 
diff -Naur ffmpeg-3.2.4/libavfilter/avfilter.c ffmpeg-3.2.4.patch/libavfilter/avfilter.c
--- ffmpeg-3.2.4/libavfilter/avfilter.c	2017-02-10 14:25:27.000000000 +0100
+++ ffmpeg-3.2.4.patch/libavfilter/avfilter.c	2017-05-28 20:42:45.758088737 +0200
@@ -924,6 +924,7 @@
                    "options, but options were provided: %s.\n", args);
             return AVERROR(EINVAL);
         }
+        printf("=== args='%s'\n", args);
 
 #if FF_API_OLD_FILTER_OPTS || FF_API_OLD_FILTER_OPTS_ERROR
             if (   !strcmp(filter->filter->name, "format")     ||
diff -Naur ffmpeg-3.2.4/libavformat/mpegts.c ffmpeg-3.2.4.patch/libavformat/mpegts.c
--- ffmpeg-3.2.4/libavformat/mpegts.c	2017-02-10 14:25:27.000000000 +0100
+++ ffmpeg-3.2.4.patch/libavformat/mpegts.c	2017-05-28 20:42:45.759088741 +0200
@@ -701,7 +701,7 @@
 #endif
     { 0x1b, AVMEDIA_TYPE_VIDEO, AV_CODEC_ID_H264       },
     { 0x1c, AVMEDIA_TYPE_AUDIO, AV_CODEC_ID_AAC        },
-    { 0x20, AVMEDIA_TYPE_VIDEO, AV_CODEC_ID_H264       },
+    { 0x20, AVMEDIA_TYPE_VIDEO, AV_CODEC_ID_H264_MVC   },
     { 0x21, AVMEDIA_TYPE_VIDEO, AV_CODEC_ID_JPEG2000   },
     { 0x24, AVMEDIA_TYPE_VIDEO, AV_CODEC_ID_HEVC       },
     { 0x42, AVMEDIA_TYPE_VIDEO, AV_CODEC_ID_CAVS       },
diff -Naur ffmpeg-3.2.4/libavformat/utils.c ffmpeg-3.2.4.patch/libavformat/utils.c
--- ffmpeg-3.2.4/libavformat/utils.c	2017-02-10 14:25:27.000000000 +0100
+++ ffmpeg-3.2.4.patch/libavformat/utils.c	2017-05-28 20:42:45.760088744 +0200
@@ -733,7 +733,7 @@
         int default_stream_index = av_find_default_stream_index(s);
         if (s->streams[default_stream_index]->pts_wrap_reference == AV_NOPTS_VALUE) {
             for (i = 0; i < s->nb_streams; i++) {
-                if (av_find_program_from_stream(s, NULL, i))
+                if (0 && av_find_program_from_stream(s, NULL, i))
                     continue;
                 s->streams[i]->pts_wrap_reference = pts_wrap_reference;
                 s->streams[i]->pts_wrap_behavior = pts_wrap_behavior;
diff -Naur ffmpeg-3.2.4/libavutil/buffer.c ffmpeg-3.2.4.patch/libavutil/buffer.c
--- ffmpeg-3.2.4/libavutil/buffer.c	2017-02-10 14:25:28.000000000 +0100
+++ ffmpeg-3.2.4.patch/libavutil/buffer.c	2017-05-28 20:42:45.760088744 +0200
@@ -425,3 +425,9 @@
 
     return ret;
 }
+
+// Return the opaque for the underlying frame (gives us a GPU_MEM_PTR_T)
+void *av_buffer_pool_opaque(AVBufferRef *ref) {
+  BufferPoolEntry *buf = av_buffer_get_opaque(ref);
+  return buf->opaque;
+}
diff -Naur ffmpeg-3.2.4/libavutil/buffer.h ffmpeg-3.2.4.patch/libavutil/buffer.h
--- ffmpeg-3.2.4/libavutil/buffer.h	2017-02-10 14:25:28.000000000 +0100
+++ ffmpeg-3.2.4.patch/libavutil/buffer.h	2017-05-28 20:42:45.760088744 +0200
@@ -283,6 +283,9 @@
  */
 AVBufferRef *av_buffer_pool_get(AVBufferPool *pool);
 
+// Return the opaque for the underlying frame
+void *av_buffer_pool_opaque(AVBufferRef *ref);
+
 /**
  * @}
  */
diff -Naur ffmpeg-3.2.4/libavutil/pixdesc.c ffmpeg-3.2.4.patch/libavutil/pixdesc.c
--- ffmpeg-3.2.4/libavutil/pixdesc.c	2017-02-10 14:25:28.000000000 +0100
+++ ffmpeg-3.2.4.patch/libavutil/pixdesc.c	2017-05-28 20:42:45.761088748 +0200
@@ -2092,6 +2092,18 @@
         .flags = AV_PIX_FMT_FLAG_BE | AV_PIX_FMT_FLAG_PLANAR |
                  AV_PIX_FMT_FLAG_RGB | AV_PIX_FMT_FLAG_ALPHA,
     },
+    [AV_PIX_FMT_SAND128] = {
+        .name = "sand128",
+        .nb_components = 3,
+        .log2_chroma_w = 1,
+        .log2_chroma_h = 1,
+        .comp = {
+            { 0, 1, 0, 0, 8, 0, 7, 1 },        /* Y */
+            { 1, 2, 0, 0, 8, 1, 7, 1 },        /* U */
+            { 1, 2, 1, 0, 8, 1, 7, 2 },        /* V */
+        },
+        .flags = 0,
+    }
 };
 #if FF_API_PLUS1_MINUS1
 FF_ENABLE_DEPRECATION_WARNINGS
diff -Naur ffmpeg-3.2.4/libavutil/pixfmt.h ffmpeg-3.2.4.patch/libavutil/pixfmt.h
--- ffmpeg-3.2.4/libavutil/pixfmt.h	2017-02-10 14:25:28.000000000 +0100
+++ ffmpeg-3.2.4.patch/libavutil/pixfmt.h	2017-05-28 20:47:06.209006953 +0200
@@ -306,6 +306,9 @@
 
     AV_PIX_FMT_MEDIACODEC, ///< hardware decoding through MediaCodec
 
+// RPI - not on ifdef so can be got at by calling progs
+    AV_PIX_FMT_SAND128,   ///< 4:2:0 128x*Y stripe, 64x*UV stripe, then next x stripe, mysterious padding
+
     AV_PIX_FMT_NB         ///< number of pixel formats, DO NOT USE THIS if you want to link with shared libav* because the number of formats might differ between versions
 };
 
diff -Naur ffmpeg-3.2.4/libswscale/input.c ffmpeg-3.2.4.patch/libswscale/input.c
--- ffmpeg-3.2.4/libswscale/input.c	2017-02-10 14:25:28.000000000 +0100
+++ ffmpeg-3.2.4.patch/libswscale/input.c	2017-05-28 20:42:45.762088751 +0200
@@ -719,6 +719,14 @@
     }
 }
 
+
+static void sand128ToUV_c(uint8_t *dstU, uint8_t *dstV,
+                       const uint8_t *unused0, const uint8_t *src1, const uint8_t *src2,
+                       int width, uint32_t *unused)
+{
+    // NIF
+}
+
 #define input_pixel(pos) (isBE(origin) ? AV_RB16(pos) : AV_RL16(pos))
 
 static void bgr24ToY_c(uint8_t *_dst, const uint8_t *src, const uint8_t *unused1, const uint8_t *unused2,
@@ -1085,6 +1093,9 @@
     case AV_PIX_FMT_P010BE:
         c->chrToYV12 = p010BEToUV_c;
         break;
+    case AV_PIX_FMT_SAND128:
+        c->chrToYV12 = sand128ToUV_c;
+        break;
     }
     if (c->chrSrcHSubSample) {
         switch (srcFormat) {
diff -Naur ffmpeg-3.2.4/libswscale/utils.c ffmpeg-3.2.4.patch/libswscale/utils.c
--- ffmpeg-3.2.4/libswscale/utils.c	2017-02-10 14:25:28.000000000 +0100
+++ ffmpeg-3.2.4.patch/libswscale/utils.c	2017-05-28 20:48:08.013211613 +0200
@@ -248,6 +248,9 @@
     [AV_PIX_FMT_AYUV64LE]    = { 1, 1},
     [AV_PIX_FMT_P010LE]      = { 1, 1 },
     [AV_PIX_FMT_P010BE]      = { 1, 1 },
+#ifdef RPI
+    [AV_PIX_FMT_SAND128]     = { 1, 0 },
+#endif
 };
 
 int sws_isSupportedInput(enum AVPixelFormat pix_fmt)
diff -Naur ffmpeg-3.2.4/pi-util/conf1.sh ffmpeg-3.2.4.patch/pi-util/conf1.sh
--- ffmpeg-3.2.4/pi-util/conf1.sh	1970-01-01 01:00:00.000000000 +0100
+++ ffmpeg-3.2.4.patch/pi-util/conf1.sh	2017-05-28 20:42:45.764088759 +0200
@@ -0,0 +1,34 @@
+echo "Configure for Pi1"
+
+RPI_BUILDROOT=`pwd`/build
+RPI_ROOTFS=$RPI_BUILDROOT/linux/raspian_jessie_pi1-sysroot
+RPI_TOOLROOT=$RPI_BUILDROOT/tools/arm-bcm2708/arm-rpi-4.9.3-linux-gnueabihf
+RPI_OPT_VC=$RPI_ROOTFS/opt/vc
+#RPI_INCLUDES="-I$RPI_OPT_VC/include -I$RPI_ROOTFS/usr/include -I$RPI_OPT_VC/include/interface/vcos/pthreads -I$RPI_OPT_VC/include/interface/vmcs_host/linux"
+RPI_INCLUDES="-I$RPI_OPT_VC/include -I$RPI_OPT_VC/include/interface/vcos/pthreads -I$RPI_OPT_VC/include/interface/vmcs_host/linux"
+RPI_DEFS="-D__VCCOREVER__=0x04000000 -DRPI=1"
+#RPI_DEFS="-D__VCCOREVER__=0x04000000"
+RPI_LIBDIRS="-L$RPI_ROOTFS/lib -L$RPI_ROOTFS/usr/lib -L$RPI_OPT_VC/lib"
+#RPI_KEEPS="-save-temps=obj"
+RPI_KEEPS=""
+
+./configure --enable-cross-compile\
+ --cpu=arm1176jzf-s\
+ --arch=armv\
+ --disable-neon\
+ --target-os=linux\
+ --disable-stripping\
+ --enable-mmal\
+ --extra-cflags="-g $RPI_KEEPS $RPI_DEFS $RPI_INCLUDES"\
+ --extra-cxxflags="$RPI_DEFS $RPI_INCLUDES"\
+ --extra-ldflags="$RPI_LIBDIRS -Wl,-rpath=/opt/vc/lib,-rpath-link=$RPI_OPT_VC/lib,-rpath=/lib,-rpath=/usr/lib,-rpath-link=$RPI_ROOTFS/lib,-rpath-link=$RPI_ROOTFS/usr/lib"\
+ --extra-libs="-Wl,--start-group -lbcm_host -lmmal -lmmal_util -lmmal_core -lvcos -lvcsm -lvchostif -lvchiq_arm"\
+ --cross-prefix=$RPI_TOOLROOT/bin/arm-linux-gnueabihf-
+
+
+# --enable-extra-warnings\
+# --arch=armv71\
+# --enable-shared\
+
+# gcc option for getting asm listing
+# -Wa,-ahls
diff -Naur ffmpeg-3.2.4/pi-util/conf_h265.csv ffmpeg-3.2.4.patch/pi-util/conf_h265.csv
--- ffmpeg-3.2.4/pi-util/conf_h265.csv	1970-01-01 01:00:00.000000000 +0100
+++ ffmpeg-3.2.4.patch/pi-util/conf_h265.csv	2017-05-28 20:42:45.764088759 +0200
@@ -0,0 +1,144 @@
+1,ADJUST_IPRED_ANGLE_A_RExt_Mitsubishi_1,ADJUST_IPRED_ANGLE_A_RExt_Mitsubishi_1.bit,ADJUST_IPRED_ANGLE_A_RExt_Mitsubishi_1.md5
+1,AMP_A_Samsung_6,AMP_A_Samsung_6.bin,AMP_A_Samsung_6.md5
+1,AMP_B_Samsung_6,AMP_B_Samsung_6.bin,AMP_B_Samsung_6.md5
+1,AMP_D_Hisilicon_3,AMP_D_Hisilicon.bit,AMP_D_Hisilicon_3.yuv.md5
+1,AMP_E_Hisilicon_3,AMP_E_Hisilicon.bit,AMP_E_Hisilicon_3.yuv.md5
+1,AMP_F_Hisilicon_3,AMP_F_Hisilicon_3.bit,AMP_F_Hisilicon_3.yuv.md5
+1,AMVP_A_MTK_4,AMVP_A_MTK_4.bit,AMVP_A_MTK_4.md5
+1,AMVP_B_MTK_4,AMVP_B_MTK_4.bit,AMVP_B_MTK_4.md5
+1,AMVP_C_Samsung_6,AMVP_C_Samsung_6.bin,AMVP_C_Samsung_6.md5
+1,BUMPING_A_ericsson_1,BUMPING_A_ericsson_1.bit,BUMPING_A_ericsson_1.md5
+1,CAINIT_A_SHARP_4,CAINIT_A_SHARP_4.bit,CAINIT_A_SHARP_4.md5
+1,CAINIT_B_SHARP_4,CAINIT_B_SHARP_4.bit,CAINIT_B_SHARP_4.md5
+1,CAINIT_C_SHARP_3,CAINIT_C_SHARP_3.bit,CAINIT_C_SHARP_3.md5
+1,CAINIT_D_SHARP_3,CAINIT_D_SHARP_3.bit,CAINIT_D_SHARP_3.md5
+1,CAINIT_E_SHARP_3,CAINIT_E_SHARP_3.bit,CAINIT_E_SHARP_3.md5
+1,CAINIT_F_SHARP_3,CAINIT_F_SHARP_3.bit,CAINIT_F_SHARP_3.md5
+1,CAINIT_G_SHARP_3,CAINIT_G_SHARP_3.bit,CAINIT_G_SHARP_3.md5
+1,CAINIT_H_SHARP_3,CAINIT_H_SHARP_3.bit,CAINIT_H_SHARP_3.md5
+1,CIP_A_Panasonic_3,CIP_A_Panasonic_3.bit,CIP_A_Panasonic_3_yuv.md5
+1,cip_B_NEC_3,cip_B_NEC_3.bit,cip_B_NEC_3.md5
+1,CIP_C_Panasonic_2,CIP_C_Panasonic_2.bit,CIP_C_Panasonic_2_yuv.md5
+1,CONFWIN_A_Sony_1,CONFWIN_A_Sony_1.bit,CONFWIN_A_Sony_1.md5
+1,DBLK_A_MAIN10_VIXS_3,DBLK_A_MAIN10_VIXS_3.bit,DBLK_A_MAIN10_VIXS_3.md5
+1,DBLK_A_SONY_3,DBLK_A_SONY_3.bit,DBLK_A_SONY_3.bit.yuv.md5
+1,DBLK_B_SONY_3,DBLK_B_SONY_3.bit,DBLK_B_SONY_3.bit.yuv.md5
+1,DBLK_C_SONY_3,DBLK_C_SONY_3.bit,DBLK_C_SONY_3.bit.yuv.md5
+1,DBLK_D_VIXS_2,DBLK_D_VIXS_2.bit,DBLK_D_VIXS_2_yuv.md5
+1,DBLK_E_VIXS_2,DBLK_E_VIXS_2.bit,DBLK_E_VIXS_2_yuv.md5
+1,DBLK_F_VIXS_2,DBLK_F_VIXS_2.bit,DBLK_F_VIXS_2_yuv.md5
+1,DBLK_G_VIXS_2,DBLK_G_VIXS_2.bit,DBLK_G_VIXS_2_yuv.md5
+1,DELTAQP_A_BRCM_4,DELTAQP_A_BRCM_4.bit,DELTAQP_A_BRCM_4_yuv.md5
+1,DELTAQP_B_SONY_3,DELTAQP_B_SONY_3.bit,DELTAQP_B_SONY_3.bit.yuv.md5
+1,DELTAQP_C_SONY_3,DELTAQP_C_SONY_3.bit,DELTAQP_C_SONY_3.bit.yuv.md5
+1,DSLICE_A_HHI_5,DSLICE_A_HHI_5.bin,DSLICE_A_HHI_5.md5
+1,DSLICE_B_HHI_5,DSLICE_B_HHI_5.bin,DSLICE_B_HHI_5.md5
+1,DSLICE_C_HHI_5,DSLICE_C_HHI_5.bin,DSLICE_C_HHI_5.md5
+1,ENTP_A_QUALCOMM_1,ENTP_A_Qualcomm_1.bit,ENTP_A_Qualcomm_1.md5
+1,ENTP_B_Qualcomm_1,ENTP_B_Qualcomm_1.bit,ENTP_B_Qualcomm_1.md5
+1,ENTP_C_Qualcomm_1,ENTP_C_Qualcomm_1.bit,ENTP_C_Qualcomm_1.md5
+1,EXT_A_ericsson_4,EXT_A_ericsson_4.bit,EXT_A_ericsson_4.md5
+1,FILLER_A_Sony_1,FILLER_A_Sony_1.bit,FILLER_A_Sony_1.md5
+1,HRD_A_Fujitsu_3,HRD_A_Fujitsu_3.bin,HRD_A_Fujitsu_3.md5
+1,INITQP_A_Sony_1,INITQP_A_Sony_1.bit,INITQP_A_Sony_1.md5
+1,INITQP_B_Main10_Sony_1,INITQP_B_Main10_Sony_1.bit,INITQP_B_Main10_Sony_1.md5
+1,ipcm_A_NEC_3,ipcm_A_NEC_3.bit,ipcm_A_NEC_3.md5
+1,ipcm_B_NEC_3,ipcm_B_NEC_3.bit,ipcm_B_NEC_3.md5
+1,ipcm_C_NEC_3,ipcm_C_NEC_3.bit,ipcm_C_NEC_3.md5
+1,ipcm_D_NEC_3,ipcm_D_NEC_3.bit,ipcm_D_NEC_3.md5
+1,ipcm_E_NEC_2,ipcm_E_NEC_2.bit,ipcm_E_NEC_2.md5
+1,IPRED_A_docomo_2,IPRED_A_docomo_2.bit,IPRED_A_docomo_2.md5
+1,IPRED_B_Nokia_3,IPRED_B_Nokia_3.bit,IPRED_B_Nokia_3_yuv.md5
+1,IPRED_C_Mitsubishi_3,IPRED_C_Mitsubishi_3.bit,IPRED_C_Mitsubishi_3_yuv.md5
+1,LS_A_Orange_2,LS_A_Orange_2.bit,LS_A_Orange_2_yuv.md5
+1,LS_B_Orange_4,LS_B_Orange_4.bit,LS_B_Orange_4_yuv.md5
+1,LTRPSPS_A_Qualcomm_1,LTRPSPS_A_Qualcomm_1.bit,LTRPSPS_A_Qualcomm_1.md5
+1,MAXBINS_A_TI_4,MAXBINS_A_TI_4.bit,MAXBINS_A_TI_4.md5
+1,MAXBINS_B_TI_4,MAXBINS_B_TI_4.bit,MAXBINS_B_TI_4.md5
+1,MAXBINS_C_TI_4,MAXBINS_C_TI_4.bit,MAXBINS_C_TI_4.md5
+1,MERGE_A_TI_3,MERGE_A_TI_3.bit,MERGE_A_TI_3.md5
+1,MERGE_B_TI_3,MERGE_B_TI_3.bit,MERGE_B_TI_3.md5
+1,MERGE_C_TI_3,MERGE_C_TI_3.bit,MERGE_C_TI_3.md5
+1,MERGE_D_TI_3,MERGE_D_TI_3.bit,MERGE_D_TI_3.md5
+1,MERGE_E_TI_3,MERGE_E_TI_3.bit,MERGE_E_TI_3.md5
+1,MERGE_F_MTK_4,MERGE_F_MTK_4.bit,MERGE_F_MTK_4.md5
+1,MERGE_G_HHI_4,MERGE_G_HHI_4.bit,MERGE_G_HHI_4.md5
+1,MVCLIP_A_qualcomm_3,MVCLIP_A_qualcomm_3.bit,MVCLIP_A_qualcomm_3.yuv.md5
+1,MVDL1ZERO_A_docomo_4,MVDL1ZERO_A_docomo_4.bit,MVDL1ZERO_A_docomo_4.md5
+1,MVEDGE_A_qualcomm_3,MVEDGE_A_qualcomm_3.bit,MVEDGE_A_qualcomm_3.yuv.md5
+1,NoOutPrior_A_Qualcomm_1,NoOutPrior_A_Qualcomm_1.bit,NoOutPrior_A_Qualcomm_1.md5
+1,NoOutPrior_B_Qualcomm_1,NoOutPrior_B_Qualcomm_1.bit,NoOutPrior_B_Qualcomm_1.md5
+1,NUT_A_ericsson_5,NUT_A_ericsson_5.bit,NUT_A_ericsson_5.md5
+1,OPFLAG_A_Qualcomm_1,OPFLAG_A_Qualcomm_1.bit,OPFLAG_A_Qualcomm_1.md5
+1,OPFLAG_B_Qualcomm_1,OPFLAG_B_Qualcomm_1.bit,OPFLAG_B_Qualcomm_1.md5
+1,OPFLAG_C_Qualcomm_1,OPFLAG_C_Qualcomm_1.bit,OPFLAG_C_Qualcomm_1.md5
+1,PICSIZE_A_Bossen_1,PICSIZE_A_Bossen_1.bin,PICSIZE_A_Bossen_1.md5
+1,PICSIZE_B_Bossen_1,PICSIZE_B_Bossen_1.bin,PICSIZE_B_Bossen_1.md5
+1,PICSIZE_C_Bossen_1,PICSIZE_C_Bossen_1.bin,PICSIZE_C_Bossen_1.md5
+1,PICSIZE_D_Bossen_1,PICSIZE_D_Bossen_1.bin,PICSIZE_D_Bossen_1.md5
+1,PMERGE_A_TI_3,PMERGE_A_TI_3.bit,PMERGE_A_TI_3.md5
+1,PMERGE_B_TI_3,PMERGE_B_TI_3.bit,PMERGE_B_TI_3.md5
+1,PMERGE_C_TI_3,PMERGE_C_TI_3.bit,PMERGE_C_TI_3.md5
+1,PMERGE_D_TI_3,PMERGE_D_TI_3.bit,PMERGE_D_TI_3.md5
+1,PMERGE_E_TI_3,PMERGE_E_TI_3.bit,PMERGE_E_TI_3.md5
+1,POC_A_Bossen_3,POC_A_Bossen_3.bin,POC_A_Bossen_3.md5
+1,PPS_A_qualcomm_7,PPS_A_qualcomm_7.bit,PPS_A_qualcomm_7.yuv.md5
+1,PS_B_VIDYO_3,PS_B_VIDYO_3.bit,PS_B_VIDYO_3_yuv.md5
+1,RAP_A_docomo_6,RAP_A_docomo_6.bit,RAP_A_docomo_6.md5
+1,RAP_B_Bossen_2,RAP_B_Bossen_2.bit,RAP_B_Bossen_2.md5
+1,RPLM_A_qualcomm_4,RPLM_A_qualcomm_4.bit,RPLM_A_qualcomm_4.yuv.md5
+1,RPLM_B_qualcomm_4,RPLM_B_qualcomm_4.bit,RPLM_B_qualcomm_4.yuv.md5
+1,RPS_A_docomo_5,RPS_A_docomo_5.bit,RPS_A_docomo_5.md5
+1,RPS_B_qualcomm_5,RPS_B_qualcomm_5.bit,RPS_B_qualcomm_5.yuv.md5
+1,RPS_C_ericsson_5,RPS_C_ericsson_5.bit,RPS_C_ericsson_5.md5
+1,RPS_D_ericsson_6,RPS_D_ericsson_6.bit,RPS_D_ericsson_6.md5
+1,RPS_E_qualcomm_5,RPS_E_qualcomm_5.bit,RPS_E_qualcomm_5.yuv.md5
+1,RPS_F_docomo_2,RPS_F_docomo_2.bit,RPS_F_docomo_2.md5
+1,RQT_A_HHI_4,RQT_A_HHI_4.bit,RQT_A_HHI_4.md5
+1,RQT_B_HHI_4,RQT_B_HHI_4.bit,RQT_B_HHI_4.md5
+1,RQT_C_HHI_4,RQT_C_HHI_4.bit,RQT_C_HHI_4.md5
+1,RQT_D_HHI_4,RQT_D_HHI_4.bit,RQT_D_HHI_4.md5
+1,RQT_E_HHI_4,RQT_E_HHI_4.bit,RQT_E_HHI_4.md5
+1,RQT_F_HHI_4,RQT_F_HHI_4.bit,RQT_F_HHI_4.md5
+1,RQT_G_HHI_4,RQT_G_HHI_4.bit,RQT_G_HHI_4.md5
+1,SAO_A_MediaTek_4,SAO_A_MediaTek_4.bit,SAO_A_MediaTek_4.md5
+1,SAO_B_MediaTek_5,SAO_B_MediaTek_5.bit,SAO_B_MediaTek_5.md5
+1,SAO_C_Samsung_5,SAO_C_Samsung_5.bin,SAO_C_Samsung_5.md5
+1,SAO_D_Samsung_5,SAO_D_Samsung_5.bin,SAO_D_Samsung_5.md5
+1,SAO_E_Canon_4,SAO_E_Canon_4.bit,SAO_E_Canon_4.md5
+1,SAO_F_Canon_3,SAO_F_Canon_3.bit,SAO_F_Canon_3.md5
+1,SAO_G_Canon_3,SAO_G_Canon_3.bit,SAO_G_Canon_3.md5
+1,SDH_A_Orange_4,SDH_A_Orange_4.bit,SDH_A_Orange_4_yuv.md5
+1,SLICES_A_Rovi_3,SLICES_A_Rovi_3.bin,SLICES_A_Rovi_3.md5
+1,SLIST_A_Sony_4,str.bin,SLIST_A_Sony_4_yuv.md5
+1,SLIST_B_Sony_8,str.bin,SLIST_B_Sony_8_yuv.md5
+1,SLIST_C_Sony_3,str.bin,SLIST_C_Sony_3_yuv.md5
+1,SLIST_D_Sony_9,str.bin,SLIST_D_Sony_9_yuv.md5
+1,SLPPLP_A_VIDYO_2,SLPPLP_A_VIDYO_2.bit,SLPPLP_A_VIDYO_2_yuv.md5
+1,STRUCT_A_Samsung_6,STRUCT_A_Samsung_6.bin,STRUCT_A_Samsung_6.md5
+1,STRUCT_B_Samsung_6,STRUCT_B_Samsung_6.bin,STRUCT_B_Samsung_6.md5
+1,TILES_A_Cisco_2,TILES_A_Cisco_2.bin,TILES_A_Cisco_2_yuv.md5
+1,TILES_B_Cisco_1,TILES_B_Cisco_1.bin,TILES_B_Cisco_1_yuv.md5
+1,TMVP_A_MS_3,TMVP_A_MS_3.bit,TMVP_A_MS_3.yuv.md5
+1,TSCL_A_VIDYO_5,TSCL_A_VIDYO_5.bit,TSCL_A_VIDYO_5_yuv.md5
+1,TSCL_B_VIDYO_4,TSCL_B_VIDYO_4.bit,TSCL_B_VIDYO_4_yuv.md5
+1,TSKIP_A_MS_3,TSKIP_A_MS_3.bit,TSKIP_A_MS_3.yuv.md5
+0,TSUNEQBD_A_MAIN10_Technicolor_2,TSUNEQBD_A_MAIN10_Technicolor_2.bit,TSUNEQBD_A_MAIN10_Technicolor_2_yuv.md5, # Y/C bit depth unmatched
+1,TUSIZE_A_Samsung_1,TUSIZE_A_Samsung_1.bin,TUSIZE_A_Samsung_1.md5
+1,VPSID_A_VIDYO_2,VPSID_A_VIDYO_2.bit,VPSID_A_VIDYO_2_yuv.md5
+1,WP_A_MAIN10_Toshiba_3,WP_A_MAIN10_Toshiba_3.bit,WP_A_MAIN10_Toshiba_3_yuv.md5
+1,WP_A_Toshiba_3,WP_A_Toshiba_3.bit,WP_A_Toshiba_3_yuv.md5
+1,WP_B_Toshiba_3,WP_B_Toshiba_3.bit,WP_B_Toshiba_3_yuv.md5
+1,WP_MAIN10_B_Toshiba_3,WP_MAIN10_B_Toshiba_3.bit,WP_MAIN10_B_Toshiba_3_yuv.md5
+1,WPP_A_ericsson_MAIN10_2,WPP_A_ericsson_MAIN10_2.bit,WPP_A_ericsson_MAIN10_yuv.md5
+1,WPP_A_ericsson_MAIN_2,WPP_A_ericsson_MAIN_2.bit,WPP_A_ericsson_MAIN_2_yuv.md5
+1,WPP_B_ericsson_MAIN10_2,WPP_B_ericsson_MAIN10_2.bit,WPP_B_ericsson_MAIN10_yuv.md5
+1,WPP_B_ericsson_MAIN_2,WPP_B_ericsson_MAIN_2.bit,WPP_B_ericsson_MAIN_2_yuv.md5
+1,WPP_C_ericsson_MAIN10_2,WPP_C_ericsson_MAIN10_2.bit,WPP_C_ericsson_MAIN10_yuv.md5
+1,WPP_C_ericsson_MAIN_2,WPP_C_ericsson_MAIN_2.bit,WPP_C_ericsson_MAIN_2_yuv.md5
+1,WPP_D_ericsson_MAIN10_2,WPP_D_ericsson_MAIN10_2.bit,WPP_D_ericsson_MAIN10_yuv.md5
+1,WPP_D_ericsson_MAIN_2,WPP_D_ericsson_MAIN_2.bit,WPP_D_ericsson_MAIN_2_yuv.md5
+1,WPP_E_ericsson_MAIN10_2,WPP_E_ericsson_MAIN10_2.bit,WPP_E_ericsson_MAIN10_yuv.md5
+1,WPP_E_ericsson_MAIN_2,WPP_E_ericsson_MAIN_2.bit,WPP_E_ericsson_MAIN_2_yuv.md5
+1,WPP_F_ericsson_MAIN10_2,WPP_F_ericsson_MAIN10_2.bit,WPP_F_ericsson_MAIN10_yuv.md5
+1,WPP_F_ericsson_MAIN_2,WPP_F_ericsson_MAIN_2.bit,WPP_F_ericsson_MAIN_2_yuv.md5
diff -Naur ffmpeg-3.2.4/pi-util/conf.sh ffmpeg-3.2.4.patch/pi-util/conf.sh
--- ffmpeg-3.2.4/pi-util/conf.sh	1970-01-01 01:00:00.000000000 +0100
+++ ffmpeg-3.2.4.patch/pi-util/conf.sh	2017-05-28 20:42:45.764088759 +0200
@@ -0,0 +1,33 @@
+echo "Configure for Pi2/3"
+
+RPI_BUILDROOT=`pwd`/build
+RPI_ROOTFS=$RPI_BUILDROOT/linux/raspian_jessie_pi1-sysroot
+RPI_TOOLROOT=$RPI_BUILDROOT/tools/arm-bcm2708/arm-rpi-4.9.3-linux-gnueabihf
+RPI_OPT_VC=$RPI_ROOTFS/opt/vc
+#RPI_INCLUDES="-I$RPI_OPT_VC/include -I$RPI_ROOTFS/usr/include -I$RPI_OPT_VC/include/interface/vcos/pthreads -I$RPI_OPT_VC/include/interface/vmcs_host/linux"
+RPI_INCLUDES="-I$RPI_OPT_VC/include -I$RPI_OPT_VC/include/interface/vcos/pthreads -I$RPI_OPT_VC/include/interface/vmcs_host/linux"
+RPI_DEFS="-D__VCCOREVER__=0x04000000 -DRPI=1"
+#RPI_DEFS="-D__VCCOREVER__=0x04000000"
+RPI_LIBDIRS="-L$RPI_ROOTFS/lib -L$RPI_ROOTFS/usr/lib -L$RPI_OPT_VC/lib"
+#RPI_KEEPS="-save-temps=obj"
+RPI_KEEPS=""
+
+./configure --enable-cross-compile\
+ --arch=armv6t2\
+ --cpu=cortex-a7\
+ --target-os=linux\
+ --disable-stripping\
+ --disable-thumb\
+ --enable-mmal\
+ --extra-cflags="-g $RPI_KEEPS $RPI_DEFS $RPI_INCLUDES"\
+ --extra-cxxflags="$RPI_DEFS $RPI_INCLUDES"\
+ --extra-ldflags="$RPI_LIBDIRS -Wl,-rpath=/opt/vc/lib,-rpath-link=$RPI_OPT_VC/lib,-rpath=/lib,-rpath=/usr/lib,-rpath-link=$RPI_ROOTFS/lib,-rpath-link=$RPI_ROOTFS/usr/lib"\
+ --extra-libs="-Wl,--start-group -lbcm_host -lmmal -lmmal_util -lmmal_core -lvcos -lvcsm -lvchostif -lvchiq_arm"\
+ --cross-prefix=$RPI_TOOLROOT/bin/arm-linux-gnueabihf-
+
+# --enable-extra-warnings\
+# --arch=armv71\
+# --enable-shared\
+
+# gcc option for getting asm listing
+# -Wa,-ahls
diff -Naur ffmpeg-3.2.4/pi-util/ffconf.py ffmpeg-3.2.4.patch/pi-util/ffconf.py
--- ffmpeg-3.2.4/pi-util/ffconf.py	1970-01-01 01:00:00.000000000 +0100
+++ ffmpeg-3.2.4.patch/pi-util/ffconf.py	2017-05-28 20:42:45.765088762 +0200
@@ -0,0 +1,154 @@
+#!/usr/bin/env python
+
+import os
+import subprocess
+import re
+import argparse
+import sys
+import csv
+from stat import *
+
+conf_root = "/opt/conform/h265"
+ffmpeg_exec = "./ffmpeg"
+
+def testone(fileroot, name, es_file, md5_file):
+    tmp_root = "/tmp"
+
+    dec_file = os.path.join(tmp_root, name + ".dec.md5")
+    try:
+        os.remove(dec_file)
+    except:
+        pass
+
+    flog = open(os.path.join(tmp_root, name + ".log"), "wt")
+
+    # Unaligned needed for cropping conformance
+    rstr = subprocess.call(
+        [ffmpeg_exec, "-flags", "unaligned", "-vcodec", "hevc", "-i", os.path.join(fileroot, es_file), "-f", "md5", dec_file],
+        stdout=flog, stderr=subprocess.STDOUT)
+
+    try:
+        m1 = None
+        m2 = None
+        with open(os.path.join(fileroot, md5_file)) as f:
+            for line in f:
+                m1 = re.search("[0-9a-f]{32}", line.lower())
+                if m1:
+                    break
+
+        with open(dec_file) as f:
+            m2 = re.search("[0-9a-f]{32}", f.readline())
+    except:
+        pass
+
+    if  m1 and m2 and m1.group() == m2.group():
+        print >> flog, "Match: " + m1.group()
+        rv = 0
+    elif not m1:
+        print >> flog, "****** Cannot find m1"
+        rv = 3
+    elif not m2:
+        print >> flog, "****** Cannot find m2"
+        rv = 2
+    else:
+        print >> flog, "****** Mismatch: " + m1.group() + " != " + m2.group()
+        rv = 1
+    flog.close()
+    return rv
+
+def scandir(root):
+    aconf = []
+    ents = os.listdir(conf_root)
+    ents.sort(key=str.lower)
+    for name in ents:
+        test_path = os.path.join(conf_root, name)
+        if S_ISDIR(os.stat(test_path).st_mode):
+            files = os.listdir(test_path)
+            es_file = "?"
+            md5_file = "?"
+            for f in files:
+                (base, ext) = os.path.splitext(f)
+                if base[0] == '.':
+                    pass
+                elif ext == ".bit" or ext == ".bin":
+                    es_file = f
+                elif ext == ".md5":
+                    if md5_file == "?":
+                        md5_file = f
+                    elif base[-3:] == "yuv":
+                        md5_file = f
+            aconf.append((1, name, es_file, md5_file))
+    return aconf
+
+def runtest(name, tests):
+    if not tests:
+        return True
+    for t in tests:
+        if name[0:len(t)] == t:
+            return True
+        return False
+
+def doconf(csva, tests):
+    failures = []
+    unx_success = []
+    for a in csva:
+        exp_test = int(a[0])
+        if (exp_test and runtest(a[1], tests)):
+            name = a[1]
+            print "==== ", name,
+            sys.stdout.flush()
+
+            rv = testone(os.path.join(conf_root, name), name, a[2], a[3])
+            if (rv == 0):
+                if exp_test == 2:
+                    print ": * OK *"
+                    unx_success.append(name)
+                else:
+                    print ": ok"
+            elif exp_test > 1 and rv == 1:
+                print ": fail"
+            else:
+                failures.append(name)
+                if rv == 1:
+                    print ": * FAIL *"
+                elif (rv == 2) :
+                    print ": * CRASH *"
+                elif (rv == 3) :
+                    print ": * MD5 MISSING *"
+                else :
+                    print ": * BANG *"
+
+    if failures or unx_success:
+        print "Unexpected Failures:", failures
+        print "Unexpected Success: ", unx_success
+    else:
+        print "All tests normal"
+
+
+class ConfCSVDialect(csv.Dialect):
+    delimiter = ','
+    doublequote = True
+    lineterminator = '\n'
+    quotechar='"'
+    quoting = csv.QUOTE_MINIMAL
+    skipinitialspace = True
+    strict = True
+
+if __name__ == '__main__':
+
+    argp = argparse.ArgumentParser(description="FFmpeg h265 conformance tester")
+    argp.add_argument("tests", nargs='*')
+    argp.add_argument("--csvgen", action='store_true', help="Generate CSV file for dir")
+    argp.add_argument("--csv", default="pi-util/conf_h265.csv", help="CSV filename")
+    args = argp.parse_args()
+
+    if args.csvgen:
+        csv.writer(sys.stdout).writerows(scandir(conf_root))
+        exit(0)
+
+    with open(args.csv, 'rt') as csvfile:
+        csva = [a for a in csv.reader(csvfile, ConfCSVDialect())]
+
+
+    doconf(csva, args.tests)
+
diff -Naur ffmpeg-3.2.4/pi-util/qasm.py ffmpeg-3.2.4.patch/pi-util/qasm.py
--- ffmpeg-3.2.4/pi-util/qasm.py	1970-01-01 01:00:00.000000000 +0100
+++ ffmpeg-3.2.4.patch/pi-util/qasm.py	2017-05-28 20:42:45.767088769 +0200
@@ -0,0 +1,2502 @@
+#!/usr/bin/env python
+
+#    add.ifz.setf  -, r0, ra0 ; fmul  rb1, rany2, 0 ; thrend # comment
+#    add  r0, r0, 1                    # implicit mul nop
+#    nop                               # explicit add nop, implicit mul nop
+#    bkpt                              # implicit add/mul nop
+#    mov  r0, 0x1234                   # hex immediate
+#    mov  r0, 20 * 40                  # expressions...
+#    mov  r0, f(sqrt(2.0) * 3.0)       # f() converts float to bits
+#    mov  r0, a:label                  # put address of label in r0
+# :label
+#    bra.allnn  ra2, a:1f              # branch to label 1 (searching forward), using absolute address
+# :1
+#    brr.anyz  -, r:1b                 # branch to label 1 (searching backward), using relative address
+# :1                                   # multiple definitions of numeric labels (differentiated using f/b)
+# .set my_val, 3                       # introduce alias for 3
+# .set my_reg, r0                      # and for r0
+#    mov  my_reg, my_val               # then use them
+# .set my_reg2, my_reg + my_val        # r0 plus 3 is r3
+# .macro my_add, a, b, c               # a, b, c act as if .set on entry
+# .set my_val, 10
+#    add  a, b, c
+#    mov  r0, my_val                   # 10
+# .endm                                # forget all .sets since .macro (including arg .sets)
+#    mov  r0, my_val                   # 3
+#    my_add  my_reg2, my_reg, ra0 << 4 # << rotates left (>> rotates right)
+
+import math
+import optparse
+import os
+import random
+import re
+import struct
+import sys
+import time
+
+###############################################################################
+# constants
+###############################################################################
+
+# ops
+######
+
+# negatives are internal qasm ops
+
+AOP_MOV     = -3   # two operands
+AOP_BRA     = -2   # two operands
+AOP_BRR     = -1   # two operands
+AOP_NOP     = 0x00 # no operands
+AOP_FADD    = 0x01
+AOP_FSUB    = 0x02
+AOP_FMIN    = 0x03
+AOP_FMAX    = 0x04
+AOP_FMINABS = 0x05
+AOP_FMAXABS = 0x06
+AOP_FTOI    = 0x07 # two operands
+AOP_ITOF    = 0x08 # two operands
+AOP_ADD     = 0x0c
+AOP_SUB     = 0x0d
+AOP_SHR     = 0x0e
+AOP_ASR     = 0x0f
+AOP_ROR     = 0x10
+AOP_SHL     = 0x11
+AOP_MIN     = 0x12
+AOP_MAX     = 0x13
+AOP_AND     = 0x14
+AOP_OR      = 0x15
+AOP_XOR     = 0x16
+AOP_NOT     = 0x17 # two operands
+AOP_CLZ     = 0x18 # two operands
+AOP_V8ADDS  = 0x1e
+AOP_V8SUBS  = 0x1f
+
+MOP_MOV    = -1  # two operands
+MOP_NOP    = 0x0 # no operands
+MOP_FMUL   = 0x1
+MOP_MUL24  = 0x2
+MOP_V8MULD = 0x3
+MOP_V8MIN  = 0x4
+MOP_V8MAX  = 0x5
+MOP_V8ADDS = 0x6
+MOP_V8SUBS = 0x7
+
+# ldi modes
+############
+
+LDI_32          = 0
+LDI_EL_SIGNED   = 1
+LDI_EL_UNSIGNED = 3
+LDI_SEMA        = 4
+
+# conds
+########
+
+COND_NEVER  = 0
+COND_ALWAYS = 1
+COND_IFZ    = 2
+COND_IFNZ   = 3
+COND_IFN    = 4
+COND_IFNN   = 5
+COND_IFC    = 6
+COND_IFNC   = 7
+
+BCOND_ALLZ   = 0
+BCOND_ALLNZ  = 1
+BCOND_ANYZ   = 2
+BCOND_ANYNZ  = 3
+BCOND_ALLN   = 4
+BCOND_ALLNN  = 5
+BCOND_ANYN   = 6
+BCOND_ANYNN  = 7
+BCOND_ALLC   = 8
+BCOND_ALLNC  = 9
+BCOND_ANYC   = 10
+BCOND_ANYNC  = 11
+BCOND_ALWAYS = 15
+
+# packing/unpacking
+####################
+
+# regfile a pack modes
+PACK_A_NOP   = 0
+PACK_A_16A   = 1
+PACK_A_16B   = 2
+PACK_A_8888  = 3
+PACK_A_8A    = 4
+PACK_A_8B    = 5
+PACK_A_8C    = 6
+PACK_A_8D    = 7
+PACK_A_32S   = 8
+PACK_A_16AS  = 9
+PACK_A_16BS  = 10
+PACK_A_8888S = 11
+PACK_A_8AS   = 12
+PACK_A_8BS   = 13
+PACK_A_8CS   = 14
+PACK_A_8DS   = 15
+
+# mul unit pack modes
+PACK_MUL_NOP  = 0
+PACK_MUL_8888 = 3
+PACK_MUL_8A   = 4
+PACK_MUL_8B   = 5
+PACK_MUL_8C   = 6
+PACK_MUL_8D   = 7
+
+# regfile a unpack modes
+UNPACK_A_NOP = 0
+UNPACK_A_16A = 1
+UNPACK_A_16B = 2
+UNPACK_A_8R  = 3
+UNPACK_A_8A  = 4
+UNPACK_A_8B  = 5
+UNPACK_A_8C  = 6
+UNPACK_A_8D  = 7
+
+# r4 unpack modes
+UNPACK_R4_NOP = 0
+UNPACK_R4_16A = 1
+UNPACK_R4_16B = 2
+UNPACK_R4_8R  = 3
+UNPACK_R4_8A  = 4
+UNPACK_R4_8B  = 5
+UNPACK_R4_8C  = 6
+UNPACK_R4_8D  = 7
+
+PACK_TYPE_INT    = 0
+PACK_TYPE_FLOAT  = 1
+PACK_TYPE_EITHER = -1
+
+PACK_MODE_A      = 0 # regfile a
+PACK_MODE_M      = 1 # mul unit
+PACK_MODE_EITHER = -1
+
+UNPACK_LOC_A     = 0 # regfile a
+UNPACK_LOC_R4    = 1 # r4
+UNPACK_LOC_AB    = 2 # either regfile a or regfile b
+UNPACK_LOC_OTHER = 3 # somewhere else
+
+# args
+#######
+
+# loc_t, ie internal
+MUX_AC  = 0
+MUX_ANY = 1
+MUX_A   = 2
+MUX_B   = 3
+RW_EITHER = 0
+RW_READ   = 1
+RW_WRITE  = 2
+
+RADDR_NOP = 39
+
+# negatives are for internal use
+RMUX_SEMA  = -6
+RMUX_LABEL = -5
+RMUX_IMMV  = -4
+RMUX_IMM   = -3
+RMUX_AC    = -2
+RMUX_ANY   = -1
+RMUX_A0    = 0 # followed by A1, A2, A3, A4, A5
+RMUX_A     = 6
+RMUX_B     = 7
+
+WADDR_R0  = 32 # followed by R1, R2, R3
+WADDR_NOP = 39
+
+WMUX_ANY = 0
+WMUX_A   = 1
+WMUX_B   = 2
+
+# signals
+##########
+
+SIG_BKPT       = 0
+SIG_NORMAL     = 1
+SIG_THRSW      = 2
+SIG_THREND     = 3
+SIG_SBWAIT     = 4
+SIG_SBDONE     = 5
+SIG_INT        = 6 # on a0
+SIG_LTHRSW     = 6 # on b0
+SIG_LOADCV     = 7
+SIG_LOADC      = 8
+SIG_LDCEND     = 9
+SIG_LDTMU0     = 10
+SIG_LDTMU1     = 11
+SIG_ROTATE     = 12 # on a0
+SIG_LOADAM     = 12 # on b0
+SIG_SMALLIMMED = 13
+SIG_IMMED      = 14
+SIG_BRANCH     = 15
+
+# multi-line assembler constructs
+##################################
+
+CONSTRUCT_MACRO = 0x1
+CONSTRUCT_IF    = 0x2
+CONSTRUCT_ELSE  = 0x4
+CONSTRUCT_REP   = 0x8
+
+###############################################################################
+# helpers
+###############################################################################
+
+def asm_error(message, location = None):
+   if location is None:
+      location = current_location
+   if location == '':
+      sys.stderr.write('qasm ERROR: %s\n' % message)
+   else:
+      sys.stderr.write('qasm ERROR: %s: %s\n' % (location, message))
+   sys.exit(-1)
+
+def asm_warning(message, location = None):
+   if disable_warnings or (nwarn_level != 0):
+      return
+   if location is None:
+      location = current_location
+   if location == '':
+      sys.stderr.write('qasm WARNING: %s\n' % message)
+   else:
+      sys.stderr.write('qasm WARNING: %s: %s\n' % (location, message))
+   if warnings_are_errors:
+      asm_error('warnings are errors!', location)
+
+# smart_split('') = []
+# smart_split('a') = ['a']
+# smart_split('a(1, 2),[3, 4, 5],6') = ['a(1, 2)', '[3, 4, 5]', '6']
+def smart_split(s, delim = ',', count = 0):
+   if len(s) == 0:
+      return []
+   parts = []
+   depth = 0
+   i = 0
+   for j in xrange(len(s)):
+      if s[j] in '([{':
+         depth += 1
+      elif s[j] in ')]}':
+         depth -= 1
+      elif (s[j] == delim) and (depth == 0):
+         parts.append(s[i:j])
+         i = j + 1
+         if len(parts) == count:
+            break
+   if depth != 0:
+      asm_error('bracket nesting fail')
+   parts.append(s[i:])
+   return parts
+
+def is_int(x):
+   return isinstance(x, int) or isinstance(x, long)
+
+###############################################################################
+# "parsing" stuff
+###############################################################################
+
+re_macro = re.compile('\\.macro\\s+(?P<name>\\w+)(?P<params>(\\s*,\\s*\\w+)*)$')
+re_if = re.compile('\\.if((?P<set>n?set)\\s+(?P<name>\\w+)|\\s(?P<condition>.+))$')
+re_elif = re.compile('\\.elif((?P<set>n?set)\\s+(?P<name>\\w+)|\\s(?P<condition>.+))$')
+re_rep = re.compile('\\.rep\\s+(?P<name>\\w+)\\s*,(?P<count>.+)$')
+re_include = re.compile('\\.include\\s(?P<filename>.+)$')
+re_set = re.compile('\\.set\\s+(?P<name>\\w+)\\s*,(?P<val>.+)$')
+re_unset = re.compile('\\.unset\\s+(?P<name>\\w+)$')
+re_eval = re.compile('\\.eval\\s(?P<expr>.+)$')
+re_print_info_warn_error = re.compile('\\.(?P<print_info_warn_error>print|info|warn|error)\\s(?P<message>.+)$')
+re_assert = re.compile('\\.assert\\s(?P<condition>.+)$')
+re_data = re.compile('\\.d(?P<size>[124])\\s(?P<data>.+)$')
+re_macro_inst = re.compile('(?P<name>\\w+)(?P<args>\\s.+|)$')
+re_label = re.compile(':(?P<name>:?[a-zA-Z_]\\w*|\\d+)$')
+re_op = re.compile('(?P<op>\\w+)(\\.(?P<cond>\\w+))??(\\.(?P<sf>setf))?(?P<args>\\s.+|)$')
+re_label_ref_left = re.compile('\\b([ar]):')
+re_label_ref_right = re.compile('[a-zA-Z_]\\w*|\\d+[bf]$')
+re_pack = re.compile('\\.([0-9]\\w*[a-df-zA-DF-Z_])') # a bit weird because we don't want to pick up float literals...
+
+# ops
+######
+
+aops = {
+   'mov': (AOP_MOV, 2),
+   'bra': (AOP_BRA, 2),
+   'brr': (AOP_BRR, 2),
+   'nop': (AOP_NOP, 0),
+   'fadd': (AOP_FADD, 3),
+   'fsub': (AOP_FSUB, 3),
+   'fmin': (AOP_FMIN, 3),
+   'fmax': (AOP_FMAX, 3),
+   'fminabs': (AOP_FMINABS, 3),
+   'fmaxabs': (AOP_FMAXABS, 3),
+   'ftoi': (AOP_FTOI, 2),
+   'itof': (AOP_ITOF, 2),
+   'add': (AOP_ADD, 3),
+   'sub': (AOP_SUB, 3),
+   'shr': (AOP_SHR, 3),
+   'asr': (AOP_ASR, 3),
+   'ror': (AOP_ROR, 3),
+   'shl': (AOP_SHL, 3),
+   'min': (AOP_MIN, 3),
+   'max': (AOP_MAX, 3),
+   'and': (AOP_AND, 3),
+   'or': (AOP_OR, 3),
+   'xor': (AOP_XOR, 3),
+   'not': (AOP_NOT, 2),
+   'clz': (AOP_CLZ, 2),
+   'v8adds': (AOP_V8ADDS, 3),
+   'v8subs': (AOP_V8SUBS, 3)}
+
+def get_aop(aop):
+   if aop not in aops:
+      asm_error('invalid aop')
+   return aops[aop]
+
+mops = {
+   'mov': (MOP_MOV, 2),
+   'nop': (MOP_NOP, 0),
+   'fmul': (MOP_FMUL, 3),
+   'mul24': (MOP_MUL24, 3),
+   'v8muld': (MOP_V8MULD, 3),
+   'v8min': (MOP_V8MIN, 3),
+   'v8max': (MOP_V8MAX, 3),
+   'v8adds': (MOP_V8ADDS, 3),
+   'v8subs': (MOP_V8SUBS, 3)}
+
+def get_mop(mop):
+   if mop not in mops:
+      asm_error('invalid mop')
+   return mops[mop]
+
+# conds
+########
+
+conds = {
+   'ifz': COND_IFZ,
+   'ifnz': COND_IFNZ,
+   'ifn': COND_IFN,
+   'ifnn': COND_IFNN,
+   'ifc': COND_IFC,
+   'ifnc': COND_IFNC}
+
+def get_cond(cond):
+   if not cond:
+      return COND_ALWAYS
+   if cond not in conds:
+      asm_error('invalid cond')
+   return conds[cond]
+
+bconds = {
+   'allz': BCOND_ALLZ,
+   'allnz': BCOND_ALLNZ,
+   'anyz': BCOND_ANYZ,
+   'anynz': BCOND_ANYNZ,
+   'alln': BCOND_ALLN,
+   'allnn': BCOND_ALLNN,
+   'anyn': BCOND_ANYN,
+   'anynn': BCOND_ANYNN,
+   'allc': BCOND_ALLC,
+   'allnc': BCOND_ALLNC,
+   'anyc': BCOND_ANYC,
+   'anync': BCOND_ANYNC}
+
+def get_bcond(bcond):
+   if not bcond:
+      return BCOND_ALWAYS
+   if bcond not in bconds:
+      asm_error('invalid bcond')
+   return bconds[bcond]
+
+def get_setf(setf):
+   if not setf:
+      return False
+   return True
+
+# packing/unpacking
+####################
+
+packs = {
+   '16a':    (PACK_A_16A,    PACK_TYPE_INT,    PACK_MODE_A),
+   '16b':    (PACK_A_16B,    PACK_TYPE_INT,    PACK_MODE_A),
+   '16af':   (PACK_A_16A,    PACK_TYPE_FLOAT,  PACK_MODE_A),
+   '16bf':   (PACK_A_16B,    PACK_TYPE_FLOAT,  PACK_MODE_A),
+   '8abcd':  (PACK_A_8888,   PACK_TYPE_EITHER, PACK_MODE_A),
+   '8a':     (PACK_A_8A,     PACK_TYPE_EITHER, PACK_MODE_A),
+   '8b':     (PACK_A_8B,     PACK_TYPE_EITHER, PACK_MODE_A),
+   '8c':     (PACK_A_8C,     PACK_TYPE_EITHER, PACK_MODE_A),
+   '8d':     (PACK_A_8D,     PACK_TYPE_EITHER, PACK_MODE_A),
+   's':      (PACK_A_32S,    PACK_TYPE_EITHER, PACK_MODE_A),
+   '16as':   (PACK_A_16AS,   PACK_TYPE_EITHER, PACK_MODE_A),
+   '16bs':   (PACK_A_16BS,   PACK_TYPE_EITHER, PACK_MODE_A),
+   '8abcds': (PACK_A_8888S,  PACK_TYPE_EITHER, PACK_MODE_A),
+   '8as':    (PACK_A_8AS,    PACK_TYPE_EITHER, PACK_MODE_A),
+   '8bs':    (PACK_A_8BS,    PACK_TYPE_EITHER, PACK_MODE_A),
+   '8cs':    (PACK_A_8CS,    PACK_TYPE_EITHER, PACK_MODE_A),
+   '8ds':    (PACK_A_8DS,    PACK_TYPE_EITHER, PACK_MODE_A),
+   '8abcdc': (PACK_MUL_8888, PACK_TYPE_EITHER, PACK_MODE_M),
+   '8ac':    (PACK_MUL_8A,   PACK_TYPE_EITHER, PACK_MODE_M),
+   '8bc':    (PACK_MUL_8B,   PACK_TYPE_EITHER, PACK_MODE_M),
+   '8cc':    (PACK_MUL_8C,   PACK_TYPE_EITHER, PACK_MODE_M),
+   '8dc':    (PACK_MUL_8D,   PACK_TYPE_EITHER, PACK_MODE_M)}
+
+def get_pack(pack):
+   if not pack:
+      return (0, PACK_TYPE_EITHER, PACK_MODE_EITHER)
+   if pack not in packs:
+      asm_error('invalid pack')
+   return packs[pack]
+
+a_unpacks = {
+   '16a':  (UNPACK_A_16A, PACK_TYPE_INT),
+   '16b':  (UNPACK_A_16B, PACK_TYPE_INT),
+   '16af': (UNPACK_A_16A, PACK_TYPE_FLOAT),
+   '16bf': (UNPACK_A_16B, PACK_TYPE_FLOAT),
+   '8dr':  (UNPACK_A_8R,  PACK_TYPE_EITHER),
+   '8a':   (UNPACK_A_8A,  PACK_TYPE_INT),
+   '8b':   (UNPACK_A_8B,  PACK_TYPE_INT),
+   '8c':   (UNPACK_A_8C,  PACK_TYPE_INT),
+   '8d':   (UNPACK_A_8D,  PACK_TYPE_INT),
+   '8ac':  (UNPACK_A_8A,  PACK_TYPE_FLOAT),
+   '8bc':  (UNPACK_A_8B,  PACK_TYPE_FLOAT),
+   '8cc':  (UNPACK_A_8C,  PACK_TYPE_FLOAT),
+   '8dc':  (UNPACK_A_8D,  PACK_TYPE_FLOAT)}
+
+def get_a_unpack(unpack):
+   if not unpack:
+      return (UNPACK_A_NOP, PACK_TYPE_EITHER, UNPACK_LOC_A)
+   if unpack not in a_unpacks:
+      asm_error('invalid ra unpack')
+   return a_unpacks[unpack] + (UNPACK_LOC_A,)
+
+r4_unpacks = {
+   '16af': UNPACK_R4_16A,
+   '16bf': UNPACK_R4_16B,
+   '8dr':  UNPACK_R4_8R,
+   '8ac':  UNPACK_R4_8A,
+   '8bc':  UNPACK_R4_8B,
+   '8cc':  UNPACK_R4_8C,
+   '8dc':  UNPACK_R4_8D}
+
+def get_r4_unpack(unpack):
+   if not unpack:
+      return (UNPACK_R4_NOP, PACK_TYPE_EITHER, UNPACK_LOC_R4)
+   if unpack not in r4_unpacks:
+      asm_error('invalid r4 unpack')
+   return (r4_unpacks[unpack], PACK_TYPE_EITHER, UNPACK_LOC_R4)
+
+# args
+#######
+
+class loc_t:
+   def __init__(self, mux, i, rot, r5_rot, pack, rw):
+      self.mux = mux
+      self.i = i
+      self.rot = rot % 16
+      self.r5_rot = r5_rot % 16
+      self.pack = pack
+      self.rw = rw
+
+   def copy(self):
+      return loc_t(self.mux, self.i, self.rot, self.r5_rot, self.pack, self.rw)
+
+   def __add__(self, i):
+      if not is_int(i):
+         raise Exception('can only add integer to loc')
+      return loc_t(self.mux, self.i + i, self.rot, self.r5_rot, self.pack, self.rw)
+
+   def __sub__(self, i):
+      if not is_int(i):
+         raise Exception('can only subtract integer from loc')
+      return loc_t(self.mux, self.i - i, self.rot, self.r5_rot, self.pack, self.rw)
+
+   def __cmp__(self, other):
+      if is_int(other):
+         return cmp(self.i, other)
+      if not isinstance(other, loc_t):
+         raise Exception('can only compare loc to integer or other loc')
+      if self.mux != other.mux:
+         return cmp(self.mux, other.mux)
+      if self.i != other.i:
+         return cmp(self.i, other.i)
+      if self.rot != other.rot:
+         return cmp(self.rot, other.rot)
+      if self.r5_rot != other.r5_rot:
+         return cmp(self.r5_rot, other.r5_rot)
+      return cmp(self.pack, other.pack)
+
+   def is_r5(self):
+      return (self.mux == MUX_AC) and (self.i == 5)
+
+   def shift(self, rot, left):
+      if isinstance(rot, loc_t) and rot.is_r5():
+         if (rot.rot != 0) or (rot.r5_rot != 0) or rot.pack:
+            raise Exception('can\'t rotate by rotated/unpacked r5')
+         return loc_t(self.mux, self.i, self.rot, self.r5_rot + (-1 if left else 1), self.pack, self.rw)
+      if not is_int(rot):
+         raise Exception('can only rotate by integer or r5')
+      return loc_t(self.mux, self.i, self.rot + (-rot if left else rot), self.r5_rot, self.pack, self.rw)
+
+   def __lshift__(self, rot):
+      return self.shift(rot, True)
+
+   def __rshift__(self, rot):
+      return self.shift(rot, False)
+
+   def __getattr__(self, name):
+      # discard the first character if it is an underscore. this is a total hack
+      # to allow packs starting with a digit to work
+      if name[0] == '_':
+         name = name[1:]
+      if (name in packs) or (name in a_unpacks) or (name in r4_unpacks):
+         if self.pack:
+            raise Exception('can\'t specify two packs')
+         return loc_t(self.mux, self.i, self.rot, self.r5_rot, name, self.rw)
+      raise AttributeError()
+
+   def __str__(self):
+      if self.mux == MUX_AC:
+         return 'r%d' % self.i
+      if self.mux == MUX_ANY:
+         return 'rany%d' % self.i
+      if self.mux == MUX_A:
+         return 'ra%d' % self.i
+      if self.mux == MUX_B:
+         return 'rb%d' % self.i
+      assert 0
+
+class sema_t:
+   def __init__(self, acq, i):
+      if not is_int(i):
+         raise Exception('semaphore index must be integer')
+      self.acq = acq
+      self.i = i
+
+class label_t:
+   def __init__(self, rel, name, offset):
+      self.rel = rel
+      self.name = name
+      self.offset = offset
+
+   def __add__(self, offset):
+      return label_t(self.rel, self.name, self.offset + offset)
+
+   def __sub__(self, offset):
+      return label_t(self.rel, self.name, self.offset - offset)
+
+class label_maker_t:
+   def __init__(self, rel):
+      self.rel = rel
+
+   def __getattr__(self, name):
+      # we discard the first character. this is a total hack to allow numeric labels to work
+      if not re_label_ref_right.match(name[1:]):
+         raise Exception('invalid label reference')
+      return label_t(self.rel, name[1:], 0)
+
+def bits(x, n):
+   if (x >> n) != 0:
+      raise Exception('%d doesn\'t fit in %d bits' % (x, n))
+   return x
+
+def bitsw(x, n):
+   if x == (1 << n):
+      x = 0
+   return bits(x, n)
+
+def bitsws(x, n):
+   if x == (1 << (n - 1)):
+      x = 0
+   if -(1 << (n - 1)) <= x < 0:
+      x += 1 << n
+   return bits(x, n)
+
+def vpm_setup(n, stride, addr, v2 = False):
+   horiz, laned, size, y, x, p = addr
+   if size not in (0, 1, 2):
+      raise Exception('addr size should be 0, 1, or 2')
+   if horiz:
+      if x != 0:
+         raise Exception('horizontal accesses must have x of 0')
+   else:
+      if (y & 0xf) != 0:
+         raise Exception('vertical accesses must be 16 row aligned')
+   hls = (bits(horiz, 1) << 3) | (bits(laned, 1) << 2) | (2 - size)
+   if v2:
+      return ((1 << 29) | (bitsw(n, 5) << 24) | (bitsws(stride, 7) << 16) |
+         (hls << 12) | ((bits(y, 8) | bits(x, 4)) << size) | bits(p, size))
+   return ((bitsw(n, 4) << 20) | (bitsw(stride, 6) << 12) |
+      (hls << 8) | ((bits(y, 6) | bits(x, 4)) << size) | bits(p, size))
+
+def vdw_setup_0(n, m, addr):
+   horiz, size, y, x, p = addr
+   if size not in (0, 1, 2):
+      raise Exception('addr size should be 0, 1, or 2')
+   return ((2 << 30) | (bitsw(n, 7) << 23) | (bitsw(m, 7) << 16) |
+      (bits(horiz, 1) << 14) | (bits(y, 7) << 7) | (bits(x, 4) << 3) | (size << 1) | bits(p, size))
+
+def vdr_setup_0(n, m, addr, vpm_stride, stride):
+   horiz, size, y, x, p = addr
+   if size not in (0, 1, 2):
+      raise Exception('addr size should be 0, 1, or 2')
+   if (stride < 8) or (stride & (stride - 1)):
+      raise Exception('stride must be power of 2 >= 8, 8 meaning use extended stride')
+   log2_stride = 3
+   while (1 << log2_stride) != stride:
+      log2_stride += 1
+   return ((1 << 31) | (size << 29) | (bits(p, size) << 28) | (bits(log2_stride - 3, 4) << 24) |
+      (bitsw(m, 4) << 20) | (bitsw(n, 4) << 16) | (bitsw(vpm_stride, 4) << 12) |
+      (bits(1 - horiz, 1) << 11) | (bits(y, 7) << 4) | bits(x, 4))
+
+class allocator_t:
+   def __init__(self, *available):
+      self.available = list(available)
+      self.allocated = {}
+      self.reserved = []
+
+   def copy(self):
+      a = allocator_t()
+      a.available = self.available[:]
+      a.allocated = self.allocated.copy()
+      a.reserved = self.reserved[:]
+      return a
+
+   def forget(self):
+      self.__init__(self.available + self.allocated.values() + self.reserved)
+
+   def reserve(self, *rs):
+      for r in rs:
+         self.available.remove(r)
+         self.reserved.append(r)
+
+   def retire(self, name):
+      r = self.allocated.pop(name)
+      del r.__invert__
+      del r.retire
+      self.available.append(r)
+      return r
+
+   def __getattr__(self, name):
+      if name not in self.allocated:
+         r = self.available.pop()
+         r.retire = lambda: self.retire(name) # this is an ugly hack to get nicer retire syntax
+         r.__invert__ = r.retire
+         self.allocated[name] = r
+      return self.allocated[name]
+
+def pragma_allow_xor_0(x):
+   global allow_xor_0
+
+   if not isinstance(x, bool):
+      raise Exception('allow_xor_0 must be bool')
+   x, allow_xor_0 = allow_xor_0, x
+   return x
+
+def pragma_dont_warn_when_mul_rot_inp_r5(x):
+   global dont_warn_when_mul_rot_inp_r5
+
+   if not isinstance(x, bool):
+      raise Exception('dont_warn_when_mul_rot_inp_r5 must be bool')
+   x, dont_warn_when_mul_rot_inp_r5 = dont_warn_when_mul_rot_inp_r5, x
+   return x
+
+arg_defs = {
+   # special reg names (these alias the regular names, but also have appropriate read/write restrictions)
+   'w':             loc_t(MUX_A,   15, 0, 0, None, RW_EITHER),
+   'z':             loc_t(MUX_B,   15, 0, 0, None, RW_EITHER),
+   'unif':          loc_t(MUX_ANY, 32, 0, 0, None, RW_READ),
+   'vary':          loc_t(MUX_ANY, 35, 0, 0, None, RW_READ),
+   'tmurs':         loc_t(MUX_ANY, 36, 0, 0, None, RW_WRITE),
+   'r5quad':        loc_t(MUX_A,   37, 0, 0, None, RW_WRITE),
+   'r5rep':         loc_t(MUX_B,   37, 0, 0, None, RW_WRITE),
+   'elem_num':      loc_t(MUX_A,   38, 0, 0, None, RW_READ),
+   'qpu_num':       loc_t(MUX_B,   38, 0, 0, None, RW_READ),
+   'unif_addr':     loc_t(MUX_A,   40, 0, 0, None, RW_WRITE),
+   'unif_addr_rel': loc_t(MUX_B,   40, 0, 0, None, RW_WRITE),
+   'x_coord':       loc_t(MUX_A,   41, 0, 0, None, RW_EITHER),
+   'y_coord':       loc_t(MUX_B,   41, 0, 0, None, RW_EITHER),
+   'ms_mask':       loc_t(MUX_A,   42, 0, 0, None, RW_EITHER),
+   'rev_flag':      loc_t(MUX_B,   42, 0, 0, None, RW_EITHER),
+   'stencil':       loc_t(MUX_ANY, 43, 0, 0, None, RW_WRITE),
+   'tlbz':          loc_t(MUX_ANY, 44, 0, 0, None, RW_WRITE),
+   'tlbm':          loc_t(MUX_ANY, 45, 0, 0, None, RW_WRITE),
+   'tlbc':          loc_t(MUX_ANY, 46, 0, 0, None, RW_WRITE),
+   'vpm':           loc_t(MUX_ANY, 48, 0, 0, None, RW_EITHER),
+   'vr_busy':       loc_t(MUX_A,   49, 0, 0, None, RW_READ),
+   'vw_busy':       loc_t(MUX_B,   49, 0, 0, None, RW_READ),
+   'vr_setup':      loc_t(MUX_A,   49, 0, 0, None, RW_WRITE),
+   'vw_setup':      loc_t(MUX_B,   49, 0, 0, None, RW_WRITE),
+   'vr_wait':       loc_t(MUX_A,   50, 0, 0, None, RW_READ),
+   'vw_wait':       loc_t(MUX_B,   50, 0, 0, None, RW_READ),
+   'vr_addr':       loc_t(MUX_A,   50, 0, 0, None, RW_WRITE),
+   'vw_addr':       loc_t(MUX_B,   50, 0, 0, None, RW_WRITE),
+   'mutex':         loc_t(MUX_ANY, 51, 0, 0, None, RW_EITHER),
+   'recip':         loc_t(MUX_ANY, 52, 0, 0, None, RW_WRITE),
+   'recipsqrt':     loc_t(MUX_ANY, 53, 0, 0, None, RW_WRITE),
+   'rsqrt':         loc_t(MUX_ANY, 53, 0, 0, None, RW_WRITE),
+   'exp':           loc_t(MUX_ANY, 54, 0, 0, None, RW_WRITE),
+   'log':           loc_t(MUX_ANY, 55, 0, 0, None, RW_WRITE),
+   't0s':           loc_t(MUX_ANY, 56, 0, 0, None, RW_WRITE),
+   't0t':           loc_t(MUX_ANY, 57, 0, 0, None, RW_WRITE),
+   't0r':           loc_t(MUX_ANY, 58, 0, 0, None, RW_WRITE),
+   't0b':           loc_t(MUX_ANY, 59, 0, 0, None, RW_WRITE),
+   't1s':           loc_t(MUX_ANY, 60, 0, 0, None, RW_WRITE),
+   't1t':           loc_t(MUX_ANY, 61, 0, 0, None, RW_WRITE),
+   't1r':           loc_t(MUX_ANY, 62, 0, 0, None, RW_WRITE),
+   't1b':           loc_t(MUX_ANY, 63, 0, 0, None, RW_WRITE),
+
+   # semaphore acq/rel
+   'sacq': lambda i: sema_t(True, i),
+   'srel': lambda i: sema_t(False, i),
+
+   # label makers (before evaluating, the syntax x:label gets transformed to x_label_maker._label)
+   'r_label_maker': label_maker_t(True),
+   'a_label_maker': label_maker_t(False),
+
+   # handy functions
+   'f':     lambda x: struct.unpack('I', struct.pack('f', x))[0],
+   'sqrt':  math.sqrt,
+   'sin':   math.sin,
+   'cos':   math.cos,
+   'atan2': math.atan2,
+   'pi':    math.pi,
+   'rseed': random.seed,
+   'rand':  lambda: int(random.getrandbits(32)),
+   'bits':  bits,
+   'bitsw': bitsw,
+   'bitsws': bitsws,
+
+   # handy vpm/vdw/vdr stuff
+   'h32':  lambda y:       (1, 0, 0, y, 0, 0),
+   'h16l': lambda y, p:    (1, 1, 1, y, 0, p),
+   'h16p': lambda y, p:    (1, 0, 1, y, 0, p),
+   'h8l':  lambda y, p:    (1, 1, 2, y, 0, p),
+   'h8p':  lambda y, p:    (1, 0, 2, y, 0, p),
+   'v32':  lambda y, x:    (0, 0, 0, y, x, 0),
+   'v16l': lambda y, x, p: (0, 1, 1, y, x, p),
+   'v16p': lambda y, x, p: (0, 0, 1, y, x, p),
+   'v8l':  lambda y, x, p: (0, 1, 2, y, x, p),
+   'v8p':  lambda y, x, p: (0, 0, 2, y, x, p),
+   'dma_h32':  lambda y, x:    (1, 0, y, x, 0),
+   'dma_h16p': lambda y, x, p: (1, 1, y, x, p),
+   'dma_h8p':  lambda y, x, p: (1, 2, y, x, p),
+   'dma_v32':  lambda y, x:    (0, 0, y, x, 0),
+   'dma_v16p': lambda y, x, p: (0, 1, y, x, p),
+   'dma_v8p':  lambda y, x, p: (0, 2, y, x, p),
+   'vpm_setup': vpm_setup,
+   'vpm_setup_v2': lambda n, stride, addr: vpm_setup(n, stride, addr, True),
+   'vdw_setup_0': vdw_setup_0,
+   'vdw_setup_1': lambda stride: (3 << 30) | bits(stride, 13),
+   'vdr_setup_0': vdr_setup_0,
+   'vdr_setup_ext_stride': 8, # stride of 8 means use extended stride
+   'vdr_setup_1': lambda stride: (9 << 28) | bits(stride, 13),
+
+   # annotations
+   'mul_used': lambda *is_: ('mul_used', sum(1 << i for i in is_)),
+   'mul_unused': lambda *is_: ('mul_used', sum(1 << i for i in is_) ^ 0xffff),
+   'preserve_cond': ('preserve_cond', 1),
+
+   # somewhat experimental register allocator
+   'allocator_t': allocator_t,
+
+   # pragmas
+   'pragma_allow_xor_0': pragma_allow_xor_0,
+   'pragma_dont_warn_when_mul_rot_inp_r5': pragma_dont_warn_when_mul_rot_inp_r5}
+
+# accumulators and regs (regular names -- r0, ra0, etc)
+arg_defs.update(('r%d' % i, loc_t(MUX_AC, i, 0, 0, None, RW_EITHER)) for i in xrange(6))
+arg_defs.update(('rany%d' % i, loc_t(MUX_ANY, i, 0, 0, None, RW_EITHER)) for i in xrange(64))
+arg_defs.update(('ra%d' % i, loc_t(MUX_A, i, 0, 0, None, RW_EITHER)) for i in xrange(64))
+arg_defs.update(('rb%d' % i, loc_t(MUX_B, i, 0, 0, None, RW_EITHER)) for i in xrange(64))
+
+def arg_eval(arg, sets):
+   s = (arg.strip().split('.', 1) + [None])[:2]
+   if s[0] == '-':
+      return loc_t(MUX_ANY, WADDR_NOP, 0, 0, s[1], RW_WRITE)
+   arg = re_label_ref_left.sub('\\1_label_maker._', arg) # todo: we probably don't want to replace in strings...
+   arg = re_pack.sub('._\\1', arg)
+   try:
+      # todo: i would like to be able to pass both arg_defs and sets in here
+      # (with sets hiding arg_defs in the case of conflicts), but the obvious
+      # dict(arg_defs, **sets) won't permit things such as:
+      # .set f, lambda x: y
+      # .set y, 4
+      # (the y in the lambda will be looked up in the temporary dict we created
+      # when evaluating the f .set, which doesn't contain y)
+      #
+      # instead, sets is initially set to (a copy of) arg_defs. to simulate the
+      # hiding behaviour, on an unset, we restore any hidden arg_defs value.
+      # also, before dumping sets at the end, we strip out the arg_defs stuff
+      # (this isn't entirely correct as we want to dump sets that are hiding
+      # arg_defs)
+      return eval(arg, sets)
+   except Exception, e:
+      asm_error(e)
+   except:
+      asm_error('unknown error while evaluating argument')
+
+# doesn't check/fixup pack
+def check_and_fixup_loc(loc, read):
+   if (not read) and (loc.rw == RW_READ):
+      asm_error('writing to read-only hardware register')
+   if read and (loc.rw == RW_WRITE):
+      asm_error('reading from write-only hardware register')
+   if not read:
+      # conceptually, we are writing to a location rotated right by
+      # loc.rot/loc.r5_rot. but we are actually rotating the output right by
+      # -loc.rot/-loc.r5_rot then writing it to the unrotated location
+      loc.rot = -loc.rot % 16
+      loc.r5_rot = -loc.r5_rot % 16
+   if (loc.rot != 0) and (loc.r5_rot != 0):
+      asm_error('can\'t rotate by both r5 and immediate')
+   if (loc.r5_rot != 0) and (loc.r5_rot != 1):
+      asm_error('only supported rotation by r5 is once to the %s' % ('left', 'right')[read])
+   if (not mulw_rotate) and ((loc.rot != 0) or loc.r5_rot): # mulw_rotate source checking is done later
+      if not read:
+         asm_error('target doesn\'t support write rotation')
+      if loc.mux == MUX_ANY:
+         loc.mux = MUX_A # can't do rotated read from regfile b
+      if loc.mux != MUX_A:
+         asm_error('rotation on read only allowed from regfile a')
+      if loc.i >= 32:
+         asm_warning('rotation only works from physical regfile')
+   if loc.mux == MUX_AC:
+      if (loc.i < 0) or (loc.i >= 6):
+         asm_error('reg out of range')
+      if not read:
+         if loc.i == 4:
+            asm_error('not allowed to write to r4')
+         if loc.i == 5:
+
+            asm_error('not allowed to write to r5 -- please specify r5quad or r5rep')
+   elif (loc.mux == MUX_ANY) or (loc.mux == MUX_A) or (loc.mux == MUX_B):
+      if (loc.i < 0) or (loc.i >= 64):
+         asm_error('reg out of range')
+   else:
+      assert 0
+
+def get_dst(dst, sets):
+   if not dst:
+      return None, None, (0, PACK_TYPE_EITHER, PACK_MODE_EITHER), 0, 0
+   dst = arg_eval(dst, sets)
+   if not isinstance(dst, loc_t):
+      asm_error('invalid dst')
+   dst = dst.copy()
+   check_and_fixup_loc(dst, False)
+   pack = get_pack(dst.pack)
+   if dst.mux == MUX_AC:
+      if pack[2] == PACK_MODE_A:
+         asm_warning('ra packing only works when writing to physical regfile')
+         return WADDR_R0 + dst.i, WMUX_A, pack, dst.rot, dst.r5_rot
+      return WADDR_R0 + dst.i, WMUX_ANY, pack, dst.rot, dst.r5_rot
+   if (dst.mux == MUX_A) or ((dst.mux == MUX_ANY) and (pack[2] == PACK_MODE_A)): # can't pack to regfile b with this operation
+      if (pack[2] == PACK_MODE_A) and (dst.i >= 32):
+         asm_warning('ra packing only works when writing to physical regfile')
+      return dst.i, WMUX_A, pack, dst.rot, dst.r5_rot
+   if dst.mux == MUX_ANY:
+      return dst.i, WMUX_ANY, pack, dst.rot, dst.r5_rot
+   if dst.mux == MUX_B:
+      if pack[2] == PACK_MODE_A:
+         asm_error('this packing operation can only be used for regfile a')
+      return dst.i, WMUX_B, pack, dst.rot, dst.r5_rot
+   assert 0
+
+def get_src(src, sets):
+   if not src:
+      return None, None, (0, PACK_TYPE_EITHER, UNPACK_LOC_OTHER), None, None
+   src = arg_eval(src, sets)
+   if isinstance(src, sema_t):
+      if not have_sema:
+         asm_error('target does not support semaphores')
+      if (src.i < 0) or (src.i >= 16):
+         asm_error('semaphore number must be in [0, 16)')
+      return src.i | (src.acq << 4), RMUX_SEMA, (0, PACK_TYPE_EITHER, UNPACK_LOC_OTHER), 0, 0
+   if isinstance(src, label_t):
+      return (src.name, src.rel, src.offset), RMUX_LABEL, (0, PACK_TYPE_EITHER, UNPACK_LOC_OTHER), 0, 0
+   if isinstance(src, list):
+      if len(src) != 16:
+         asm_error('vector immediate must have length 16')
+      src = src[:]
+      for i in xrange(16):
+         if not is_int(src[i]):
+            asm_error('all elements of vector immediate must be integers')
+         src[i] &= (1 << 32) - 1
+      return src, RMUX_IMMV, (0, PACK_TYPE_EITHER, UNPACK_LOC_OTHER), 0, 0
+   if is_int(src):
+      return src & ((1 << 32) - 1), RMUX_IMM, (0, PACK_TYPE_EITHER, UNPACK_LOC_OTHER), 0, 0
+   if not isinstance(src, loc_t):
+      asm_error('invalid src')
+   src = src.copy()
+   check_and_fixup_loc(src, True)
+   if mulw_rotate:
+      srot, sr5rot = 0, 0
+      drot, dr5rot = src.rot, src.r5_rot
+   else:
+      srot, sr5rot = src.rot, src.r5_rot
+      drot, dr5rot = 0, 0
+   if src.mux == MUX_AC:
+      if src.i == 4:
+         return 4, RMUX_AC, get_r4_unpack(src.pack), drot, dr5rot
+      if src.pack:
+         asm_error('unpack only allowed for regfile a or r4')
+      return src.i, RMUX_AC, (0, PACK_TYPE_EITHER, UNPACK_LOC_OTHER), drot, dr5rot
+   if (src.mux == MUX_A) or ((src.mux == MUX_ANY) and src.pack): # can't unpack from regfile b
+      return (src.i, srot, sr5rot), RMUX_A, get_a_unpack(src.pack), drot, dr5rot
+   if src.mux == MUX_ANY:
+      return src.i, RMUX_ANY, (0, PACK_TYPE_EITHER, UNPACK_LOC_AB), drot, dr5rot
+   if src.mux == MUX_B:
+      if src.pack:
+         asm_error('unpack only allowed for regfile a or r4')
+      return src.i, RMUX_B, (0, PACK_TYPE_EITHER, UNPACK_LOC_OTHER), drot, dr5rot
+   assert 0
+
+# signals
+##########
+
+sigs = {
+   'bkpt': SIG_BKPT,
+   'thrsw': SIG_THRSW,
+   'thrend': SIG_THREND,
+   'sbwait': SIG_SBWAIT,
+   'sbdone': SIG_SBDONE,
+   'int': SIG_INT,
+   'loadcv': SIG_LOADCV,
+   'loadc': SIG_LOADC,
+   'ldcend': SIG_LDCEND,
+   'ldtmu0': SIG_LDTMU0,
+   'ldtmu1': SIG_LDTMU1}
+
+def get_sig(sig):
+   if sig not in sigs:
+      return SIG_NORMAL
+   return sigs[sig]
+
+# annotations
+##############
+
+def get_annots(annot, sets):
+   annots = arg_eval(annot, sets)
+   if isinstance(annots, list):
+      annots = annots[:]
+   else:
+      annots = [annots]
+   for i, annot in enumerate(annots):
+      if ((not isinstance(annot, tuple)) or (len(annot) != 2) or (not isinstance(annot[0], str)) or
+         (not is_int(annot[1]))):
+         asm_error('annotation must be (string, integer) pair, or a list of such pairs')
+      annots[i] = (annot[0], annot[1] & ((1 << 32) - 1))
+   return annots
+
+###############################################################################
+# core
+###############################################################################
+
+def calculate_pack_modes(rpacks, rfloats, couldrfloat, wpacks, wfloats):
+   needfloat = PACK_TYPE_EITHER
+   havefloata = False
+   havefloatr4 = False
+   unpacka = None
+   unpackr4 = None
+   forcebs = [False, False, False, False]
+   forcerafloat = False
+
+   pm = PACK_MODE_EITHER
+   for i in (0, 1, 2, 3):
+      if (rpacks[i][2] == UNPACK_LOC_OTHER) or (rpacks[i][2] == UNPACK_LOC_AB):
+         assert rpacks[i][0] == 0
+      else:
+         if rpacks[i][2] == UNPACK_LOC_A:
+            if unpacka is None:
+               unpacka = rpacks[i][0]
+            elif unpacka != rpacks[i][0]:
+               asm_error('conflicting unpack operations on regfile a')
+            havefloata = havefloata or rfloats[i]
+         elif rpacks[i][2] == UNPACK_LOC_R4:
+            if unpackr4 is None:
+               unpackr4 = rpacks[i][0]
+            elif unpackr4 != rpacks[i][0]:
+               asm_error('conflicting unpack operations on r4')
+            havefloatr4 = havefloatr4 or rfloats[i]
+         else:
+            assert 0
+
+         if rpacks[i][1] != PACK_TYPE_EITHER:
+            if (needfloat != PACK_TYPE_EITHER) and (needfloat != rpacks[i][1]):
+               asm_error('conflicting unpack float requirements')
+            needfloat = rpacks[i][1]
+   for i in (0, 1, 2, 3):
+      if rpacks[i][2] == UNPACK_LOC_AB:
+         if (unpacka is not None) and (unpacka != UNPACK_A_NOP):
+            forcebs[i] = True # non-nop unpack from regfile a. must use b
+
+   if unpacka:
+      if (needfloat == PACK_TYPE_FLOAT) and (not havefloata) and couldrfloat:
+         havefloata = True
+         forcerafloat = True
+      havefloat = havefloata
+   else:
+      havefloat = havefloatr4
+
+   if (needfloat == PACK_TYPE_FLOAT) and (not havefloat):
+      asm_error('float unpack operation used in integer alu operations')
+   if (needfloat == PACK_TYPE_INT) and havefloat:
+      asm_error('integer unpack operation used in float alu operation')
+
+   unpack = 0
+   if unpacka and unpackr4:
+      asm_error('cannot specify pack operation for both regfile a and r4')
+   if unpacka:
+      pm = PACK_MODE_A
+      unpack = unpacka
+   elif unpackr4:
+      pm = PACK_MODE_M
+      unpack = unpackr4
+
+   pack = 0
+   if wpacks[0][2] == PACK_MODE_M:
+      asm_error('mul-unit pack operation used on add result')
+   for i in (0, 1):
+      if wpacks[i][2] == PACK_MODE_A:
+         if (pm != PACK_MODE_EITHER) and (pm != PACK_MODE_A):
+            asm_error('conflicting pack modes')
+         pm = PACK_MODE_A
+         pack = wpacks[i][0]
+      elif wpacks[i][2] == PACK_MODE_M:
+         if (pm != PACK_MODE_EITHER) and (pm != PACK_MODE_M):
+            asm_error('conflicting pack modes')
+         pm = PACK_MODE_M
+         pack = wpacks[i][0]
+
+      if (wpacks[i][1] == PACK_TYPE_FLOAT) and (not wfloats[i]):
+         asm_error('float pack operation used with integer alu result')
+      if (wpacks[i][1] == PACK_TYPE_INT) and wfloats[i]:
+         asm_error('integer pack operation used with float alu result')
+
+   if pm == PACK_MODE_EITHER:
+      pm = PACK_MODE_A
+   return pm, pack, unpack, forcebs, forcerafloat
+
+# immediates that can be encoded with SIG_SMALLIMMED
+bimms = {}
+bimms.update((i, i) for i in xrange(16))
+bimms.update(((i - 32) + (1 << 32), i) for i in xrange(16, 32))
+bimms.update(((127 + (i - 32)) << 23, i) for i in xrange(32, 40))
+bimms.update(((127 + (i - 48)) << 23, i) for i in xrange(40, 48))
+
+def merge_rmux(raddr_a, raddr_b, immb, arot_r5, raddr, rmux):
+   if rmux == RMUX_SEMA:
+      asm_error('semaphore op can only be used with mov')
+   if rmux == RMUX_LABEL:
+      asm_error('label not allowed here')
+   if rmux == RMUX_IMMV:
+      asm_error('vector immediate can only be used with mov')
+   if rmux == RMUX_IMM:
+      if raddr not in bimms:
+         asm_error('can\'t encode immediate 0x%08x' % raddr)
+      raddr = bimms[raddr]
+      if not immb:
+         if raddr_b is not None:
+            asm_error('regfile b and immediates don\'t mix')
+         raddr_b = raddr
+         immb = True
+      elif raddr_b != raddr:
+         asm_error('can only encode one rotation/immediate')
+      return raddr_a, raddr_b, immb, arot_r5, RMUX_B
+   if rmux == RMUX_AC:
+      return raddr_a, raddr_b, immb, arot_r5, RMUX_A0 + raddr
+   if rmux == RMUX_ANY:
+      if (mulw_rotate or (((not immb) or (raddr_b < 48)) and (not arot_r5))) and (raddr_a == raddr):
+         return raddr_a, raddr_b, immb, arot_r5, RMUX_A
+      if (not immb) and (raddr_b == raddr):
+         return raddr_a, raddr_b, immb, arot_r5, RMUX_B
+      if raddr_a is None:
+         assert mulw_rotate or (((not immb) or (raddr_b < 48)) and (not arot_r5))
+         raddr_a = raddr
+         return raddr_a, raddr_b, immb, arot_r5, RMUX_A
+      if raddr_b is None:
+         assert not immb
+         raddr_b = raddr
+         return raddr_a, raddr_b, immb, arot_r5, RMUX_B
+      asm_error('no free read slots')
+   if rmux == RMUX_A:
+      if (not mulw_rotate) and (raddr_a is not None) and (
+         ((raddr[1] != 0) | ((raddr[2] != 0) << 1)) != ((immb and (raddr_b >= 48)) | (arot_r5 << 1))):
+         asm_error('conflicting rotations from regfile a')
+      if raddr_a is None:
+         raddr_a = raddr[0]
+      elif raddr_a != raddr[0]:
+         asm_error('can only read from one location in each regfile')
+      arot_r5 = raddr[2]
+      if raddr[1] == 0:
+         return raddr_a, raddr_b, immb, arot_r5, RMUX_A
+      raddr = 48 + raddr[1]
+      if not immb:
+         if raddr_b is not None:
+            asm_error('regfile b and rotation don\'t mix')
+         raddr_b = raddr
+         immb = True
+      elif raddr_b != raddr:
+         asm_error('can only encode one rotation/immediate')
+      return raddr_a, raddr_b, immb, arot_r5, RMUX_A
+   if rmux == RMUX_B:
+      if immb:
+         asm_error('regfile b and rotation/immediates don\'t mix')
+      if raddr_b is None:
+         raddr_b = raddr
+      elif raddr_b != raddr:
+         asm_error('can only read from one location in each regfile')
+      return raddr_a, raddr_b, immb, arot_r5, RMUX_B
+   assert 0
+
+# ok if:
+# - accumulator (r0-r3)
+# - uniform (ie all elements identical). this is true of unif, qpu_num, vr_busy,
+#   and vw_busy. it's also true of r5 if it was written by r5rep, but not if it
+#   was written by r5quad. so, by default, r5 isn't considered uniform. todo:
+#   what about vr_wait/vw_wait/mutex?
+def read_rot_ok(rmux, raddr_a, raddr_b):
+   return ((rmux < 4) or ((rmux == 5) and dont_warn_when_mul_rot_inp_r5) or
+      ((rmux == 6) and (raddr_a in (32, 49))) or # unif/vr_busy
+      ((rmux == 7) and (raddr_b in (32, 38, 49)))) # unif/qpu_num/vw_busy
+
+def asm_flush_prog_data():
+   global prog_data
+
+   while len(prog_data) & 7:
+      prog_data.append(0)
+   for i in xrange(0, len(prog_data), 8):
+      prog.append(((prog_data[i + 3] << 24) | (prog_data[i + 2] << 16) | (prog_data[i + 1] << 8) | (prog_data[i + 0] << 0),
+         (prog_data[i + 7] << 24) | (prog_data[i + 6] << 16) | (prog_data[i + 5] << 8) | (prog_data[i + 4] << 0), 'data', {}))
+   prog_data = []
+
+def asm_line(sets, location, line):
+   global current_location, construct, nwarn_level
+
+   prev_location = current_location
+   current_location = location
+
+   try:
+      if construct != None:
+         if re_macro.match(line):
+            construct_stack.append(CONSTRUCT_MACRO)
+         elif re_if.match(line):
+            construct_stack.append(CONSTRUCT_IF)
+         elif re_rep.match(line):
+            construct_stack.append(CONSTRUCT_REP)
+         else:
+            else_m = line == '.else'
+            elif_m = re_elif.match(line)
+            if elif_m:
+               end_construct = CONSTRUCT_IF
+            else:
+               end_construct = {
+                  '.endm':  CONSTRUCT_MACRO,
+                  '.else':  CONSTRUCT_IF,
+                  '.endif': CONSTRUCT_IF | CONSTRUCT_ELSE,
+                  '.endr':  CONSTRUCT_REP}.get(line)
+            if end_construct is not None:
+               end_construct &= construct_stack.pop()
+               if end_construct == 0:
+                  if elif_m:
+                     asm_error('unexpected .elif')
+                  asm_error('unexpected %s' % line)
+               if len(construct_stack) == 0:
+                  lines = construct
+                  construct = None
+                  if end_construct == CONSTRUCT_MACRO:
+                     return
+                  if (end_construct == CONSTRUCT_IF) or (end_construct == CONSTRUCT_ELSE):
+                     condition_if, condition_else = lines[0]
+                     lines = lines[1:]
+                     if condition_if:
+                        for location, line in lines:
+                           asm_line(sets, location, line)
+                     if else_m:
+                        construct = [(condition_else, False)]
+                        construct_stack.append(CONSTRUCT_ELSE)
+                     elif elif_m:
+                        if elif_m.group('set'):
+                           condition_if = condition_else and ((elif_m.group('set') == 'nset') ^ (elif_m.group('name') in sets))
+                        else:
+                           condition_if = condition_else and arg_eval(elif_m.group('condition'), sets)
+                        condition_else = condition_else and (not condition_if)
+                        construct = [(condition_if, condition_else)]
+                        construct_stack.append(CONSTRUCT_IF)
+                     return
+                  if end_construct == CONSTRUCT_REP:
+                     name, count = lines[0]
+                     lines = lines[1:]
+                     for i in xrange(count):
+                        sets[name] = i
+                        for location, line in lines:
+                           asm_line(sets, location, line)
+                     return
+                  assert 0
+               if else_m:
+                  construct_stack.append(CONSTRUCT_ELSE)
+               elif elif_m:
+                  construct_stack.append(CONSTRUCT_IF)
+         construct.append((current_location, line))
+         return
+
+      if line in ('.endm', '.else', '.endif', '.endr'):
+         asm_error('unexpected %s' % line)
+      if re_elif.match(line):
+         asm_error('unexpected .elif')
+
+      m = re_macro.match(line)
+      if m:
+         construct = []
+         construct_stack.append(CONSTRUCT_MACRO)
+         macros[m.group('name')] = ([param.strip() for param in m.group('params').split(',')[1:]], construct)
+         return
+
+      m = re_if.match(line)
+      if m:
+         if m.group('set'):
+            condition = (m.group('set') == 'nset') ^ (m.group('name') in sets)
+         else:
+            # not not forces condition to a bool (this matters if condition is
+            # something mutable like a list)
+            condition = not not arg_eval(m.group('condition'), sets)
+         construct = [(condition, not condition)]
+         construct_stack.append(CONSTRUCT_IF)
+         return
+
+      m = re_rep.match(line)
+      if m:
+         count = arg_eval(m.group('count'), sets)
+         if not is_int(count):
+            asm_error('.rep count must be integer')
+         construct = [(m.group('name'), count)]
+         construct_stack.append(CONSTRUCT_REP)
+         return
+
+      m = re_include.match(line)
+      if m:
+         filename = arg_eval(m.group('filename'), sets)
+         if not isinstance(filename, str):
+            asm_error('expected string')
+         asm_file(sets, '%s: %s' % (current_location, filename), filename)
+         return
+
+      m = re_set.match(line)
+      if m:
+         sets[m.group('name')] = arg_eval(m.group('val'), sets)
+         return
+
+      m = re_unset.match(line)
+      if m:
+         name = m.group('name')
+         if name not in sets:
+            asm_error('%s not set' % name)
+         if name in arg_defs: # todo: see arg_eval
+            sets[name] = arg_defs[name]
+         else:
+            del sets[name]
+         return
+
+      m = re_eval.match(line)
+      if m:
+         arg_eval(m.group('expr'), sets)
+         return
+
+      m = re_print_info_warn_error.match(line)
+      if m:
+         def print_fn(message):
+            print message
+         def info_fn(message):
+            sys.stderr.write('%s\n' % message)
+         {'print': print_fn, 'info': info_fn, 'warn': asm_warning, 'error': asm_error}[
+            m.group('print_info_warn_error')](arg_eval(m.group('message'), sets))
+         return
+
+      m = re_assert.match(line)
+      if m:
+         if not arg_eval(m.group('condition'), sets):
+            asm_error('assertion failure: \'%s\'' % m.group('condition'))
+         return
+
+      m = re_data.match(line)
+      if m:
+         size = int(m.group('size'))
+         for datum in smart_split(m.group('data')):
+            datum = arg_eval(datum, sets)
+            if not is_int(datum):
+               asm_error('datum must be integer')
+            prog_data.extend(((datum >> (i * 8)) & 0xff) for i in xrange(size))
+         return
+
+      m = re_macro_inst.match(line)
+      if m:
+         name = m.group('name')
+         if name in macros:
+            params, lines = macros[name]
+            args = smart_split(m.group('args'))
+            if len(args) > len(params):
+               asm_error('too many arguments to macro')
+            sets = sets.copy()
+            sets.update(zip(params, (arg_eval(arg, sets) for arg in args)))
+            for param in params[len(args):]:
+               if param in sets:
+                  if param in arg_defs: # todo: see arg_eval
+                     sets[param] = arg_defs[param]
+                  else:
+                     del sets[param]
+            for location, line in lines:
+               asm_line(sets, '%s: %s' % (current_location, location), line)
+            return
+
+      if line == '.pushnwarn':
+         nwarn_level += 1
+         return
+      if line == '.popnwarn':
+         if nwarn_level == 0:
+            asm_error('.popnwarn without .pushnwarn')
+         nwarn_level -= 1
+         return
+
+      # everything below assumes prog is up to date
+      asm_flush_prog_data()
+
+      m = re_label.match(line)
+      if m:
+         name = m.group('name')
+         if name[0].isdigit():
+            labels.setdefault(name, []).append(len(prog))
+         else:
+            if name[0] == ':':
+               undecorated_name = name[1:]
+            else:
+               undecorated_name = name
+            if (undecorated_name in labels) or ((':' + undecorated_name) in labels):
+               asm_error('named label defined twice')
+            labels[name] = len(prog)
+         return
+
+      annots = line.split('@')
+      ops = [op.strip() for op in annots[0].split(';')]
+      annots = sum((get_annots(annot, sets) for annot in annots[1:]), [])
+      sig = get_sig(ops[-1])
+      if sig != SIG_NORMAL:
+         ops = ops[:-1]
+      if len(ops) > 2:
+         asm_error('too many ops')
+      elif (len(ops) == 1) and (ops[0] == ''):
+         ops = []
+      ops = (ops + ['nop', 'nop'])[:2]
+      m = re_op.match(ops[0])
+      if not m:
+         asm_error('invalid syntax')
+      aop, aargs_n = get_aop(m.group('op'))
+      if (aop == AOP_BRA) or (aop == AOP_BRR):
+         acond = get_bcond(m.group('cond'))
+      else:
+         acond = get_cond(m.group('cond'))
+      asf = get_setf(m.group('sf'))
+      aargs = smart_split(m.group('args'))
+      if len(aargs) != aargs_n:
+         asm_error('wrong operand count')
+      ard, ara, arb = (aargs + [None, None, None])[:3]
+      m = re_op.match(ops[1])
+      if not m:
+         asm_error('invalid syntax')
+      mop, margs_n = get_mop(m.group('op'))
+      mcond = get_cond(m.group('cond'))
+      msf = get_setf(m.group('sf'))
+      margs = smart_split(m.group('args'))
+      if len(margs) != margs_n:
+         asm_error('wrong operand count')
+      mrd, mra, mrb = (margs + [None, None, None])[:3]
+      # eval srcs first so allocator can retire and reuse registers for dst
+      aaraddr, aarmux, aarpack, aadrot, aadrot_r5 = get_src(ara, sets)
+      abraddr, abrmux, abrpack, abdrot, abdrot_r5 = get_src(arb, sets)
+      maraddr, marmux, marpack, madrot, madrot_r5 = get_src(mra, sets)
+      mbraddr, mbrmux, mbrpack, mbdrot, mbdrot_r5 = get_src(mrb, sets)
+      awaddr, awmux, awpack, awrot, awrot_r5 = get_dst(ard, sets)
+      mwaddr, mwmux, mwpack, mwrot, mwrot_r5 = get_dst(mrd, sets)
+      if (((abrmux is not None) and ((aadrot != abdrot) or (aadrot_r5 != abdrot_r5))) or
+         ((mbrmux is not None) and ((madrot != mbdrot) or (madrot_r5 != mbdrot_r5)))):
+         asm_error('cannot have 2 arguments with different rotations')
+      if aarmux is not None:
+         awrot = (awrot + aadrot) % 16
+         awrot_r5 = (awrot_r5 + aadrot_r5) % 16
+      if (awrot != 0) or awrot_r5:
+         asm_error('rotate not allowed on add write')
+      if marmux is not None:
+         mwrot = (mwrot + madrot) % 16
+         mwrot_r5 = (mwrot_r5 + madrot_r5) % 16
+
+      afloatr = aop in (AOP_FADD, AOP_FSUB, AOP_FMIN, AOP_FMAX, AOP_FMINABS, AOP_FMAXABS, AOP_FTOI)
+      afloatw = aop in (AOP_FADD, AOP_FSUB, AOP_FMIN, AOP_FMAX, AOP_FMINABS, AOP_FMAXABS, AOP_ITOF)
+      pm, pack, unpack, forcebs, forcerafloat = calculate_pack_modes(
+         [aarpack, abrpack, marpack, mbrpack],
+         [afloatr, afloatr, mop == MOP_FMUL, mop == MOP_FMUL],
+         aop == AOP_FTOI,
+         [awpack, mwpack],
+         [afloatw, mop == MOP_FMUL])
+      if forcebs[0]:
+         aarmux = RMUX_B
+      if forcebs[1]:
+         abrmux = RMUX_B
+      if forcebs[2]:
+         marmux = RMUX_B
+      if forcebs[3]:
+         mbrmux = RMUX_B
+
+      # extend nops to 3 operands
+      if aop == AOP_NOP:
+         awaddr, awmux, aaraddr, aarmux, abraddr, abrmux = WADDR_NOP, WMUX_ANY, 0, RMUX_AC, 0, RMUX_AC
+      if mop == MOP_NOP:
+         mwaddr, mwmux, maraddr, marmux, mbraddr, mbrmux = WADDR_NOP, WMUX_ANY, 0, RMUX_AC, 0, RMUX_AC
+
+      # extend 2 operand alu ops to 3 operands (by duplicating the 2nd operand)
+      if (aop == AOP_FTOI) or (aop == AOP_ITOF) or (aop == AOP_NOT) or (aop == AOP_CLZ):
+         if forcerafloat:
+            assert aop == AOP_FTOI # can only forcerafloat if we have an unused float operand
+            # instead of duplicating the 2nd operand, take the ra operand from
+            # the mul op thus forcing the ra value to be considered a float for
+            # the purposes of unpacking
+            if marmux == RMUX_A:
+               abraddr, abrmux = maraddr, marmux
+            else:
+               assert mbrmux == RMUX_A
+               abraddr, abrmux = mbraddr, mbrmux
+         else:
+            abraddr, abrmux = aaraddr, aarmux
+      else:
+         assert not forcerafloat # can only forcerafloat if we have an unused operand
+
+      # handle write addrs
+      if (awmux == mwmux) and (awmux != WMUX_ANY):
+         asm_error('add/mul ops not allowed to write to same regfile')
+      ws = (awmux == WMUX_B) or (mwmux == WMUX_A)
+
+      # handle branch
+      if (aop == AOP_BRA) or (aop == AOP_BRR):
+         # check setf
+         if asf:
+            asm_error('setf not allowed on bra/brr')
+
+         # check pack/unpack
+         if (pack != 0) or (unpack != 0):
+            asm_error('pack/unpack not allowed with bra/brr')
+
+         # handle read address
+         if aarmux == RMUX_LABEL:
+            if (aop == AOP_BRA) and aaraddr[1]:
+               asm_warning('bra with rel label')
+            if (aop == AOP_BRR) and (not aaraddr[1]):
+               asm_warning('brr with abs label')
+            aaraddr, aarmux = (current_location,) + aaraddr, RMUX_IMM
+         if aarmux == RMUX_ANY:
+            aaraddr, aarmux = (aaraddr, 0, 0), RMUX_A
+         if (aarmux != RMUX_IMM) and (aarmux != RMUX_A):
+            asm_error('branch destination must be either label, immediate, or from regfile a')
+         if aarmux == RMUX_IMM:
+            imm = aaraddr
+            raddr = 0 # can't use RADDR_NOP
+         elif aarmux == RMUX_A:
+            if (aaraddr[1] != 0) or (aaraddr[2] != 0):
+               asm_error('rotation of read from regfile a not allowed with branch')
+            if aop == AOP_BRR:
+               asm_warning('brr with ra')
+            imm = 0
+            raddr = aaraddr[0]
+         else:
+            assert 0
+
+         # check mul op is nop
+         if mop != MOP_NOP:
+            asm_error('mul op not allowed with branch')
+
+         # check sig
+         if sig != SIG_NORMAL:
+            asm_error('no signal allowed with branch')
+
+         if raddr >= 32:
+            asm_error('can only branch to register locations in physical regfile')
+         if raddr & 1:
+            asm_warning('branch instruction will destroy flags (see hw-2780)')
+
+         # construct branch instruction
+         prog.append((imm,
+            (mwaddr << 0) | (awaddr << 6) | (ws << 12) | (raddr << 13) | ((aarmux == RMUX_A) << 18) | ((aop == AOP_BRR) << 19) | (acond << 20) | (SIG_BRANCH << 28),
+            line, annots))
+
+         return
+
+      # use COND_NEVER when possible (might save power / allow mul setf)
+      if not dict(annots).get('preserve_cond', 0):
+          if (awaddr == WADDR_NOP) and (not asf):
+             acond = COND_NEVER
+          if (mwaddr == WADDR_NOP) and (not msf):
+             mcond = COND_NEVER
+
+      # attempt to convert movs to ldi
+      if (# no mul setf
+         (not msf) and
+         # ops must either be nop or mov of sema/label/imm/immv
+         ((aop == AOP_NOP) or ((aop == AOP_MOV) and (aarmux in (RMUX_SEMA, RMUX_LABEL, RMUX_IMMV, RMUX_IMM)))) and
+         ((mop == MOP_NOP) or ((mop == MOP_MOV) and (marmux in (RMUX_SEMA, RMUX_LABEL, RMUX_IMMV, RMUX_IMM)))) and
+         # but we don't want 2 nops
+         ((aop != AOP_NOP) or (mop != MOP_NOP)) and
+         # if both ops are movs, srcs must be identical
+         ((aop != AOP_MOV) or (mop != MOP_MOV) or ((aarmux == marmux) and (aaraddr == maraddr))) and
+         # no signal
+         (sig == SIG_NORMAL)):
+         # make sure aarmux/aaraddr contains the value
+         if aop != AOP_MOV:
+            aarmux = marmux
+            aaraddr = maraddr
+
+         # convert immediate
+         if aarmux == RMUX_SEMA:
+            ldi_mode = LDI_SEMA
+         elif aarmux == RMUX_LABEL:
+            ldi_mode = LDI_32
+            aaraddr, aarmux = (current_location,) + aaraddr, RMUX_IMM
+         elif aarmux == RMUX_IMMV:
+            signed, unsigned = True, True
+            imm = 0
+            for i, elem in enumerate(aaraddr):
+               if elem not in (-2 + (1 << 32), -1 + (1 << 32), 0, 1):
+                  signed = False
+               if elem not in (0, 1, 2, 3):
+                  unsigned = False
+               imm |= ((elem & 0x1) << i) | ((elem & 0x2) << (15 + i))
+            if not (signed or unsigned):
+               asm_error('can\'t encode vector immediate')
+            if signed:
+               ldi_mode = LDI_EL_SIGNED
+            else:
+               ldi_mode = LDI_EL_UNSIGNED
+            aaraddr, aarmux = imm, RMUX_IMM
+         elif aarmux == RMUX_IMM:
+            ldi_mode = LDI_32
+         else:
+            assert 0
+
+         # construct ldi instruction
+         prog.append((aaraddr,
+            (mwaddr << 0) | (awaddr << 6) | (ws << 12) | (asf << 13) | (mcond << 14) | (acond << 17) | (pack << 20) | (pm << 24) | (ldi_mode << 25) | (SIG_IMMED << 28),
+            line, annots))
+
+         return
+
+      # convert movs to alu ops
+      if aop == AOP_MOV:
+         if allow_xor_0 and (aarmux == RMUX_IMM) and (aaraddr == 0):
+            aop = AOP_XOR
+            aaraddr, aarmux = 0, RMUX_AC
+            abraddr, abrmux = 0, RMUX_AC
+         else:
+            aop = AOP_OR
+            abraddr, abrmux = aaraddr, aarmux
+      if mop == MOP_MOV:
+         if allow_xor_0 and (marmux == RMUX_IMM) and (maraddr == 0):
+            mop = MOP_V8SUBS
+            maraddr, marmux = 0, RMUX_AC
+            mbraddr, mbrmux = 0, RMUX_AC
+         else:
+            mop = MOP_V8MIN
+            mbraddr, mbrmux = maraddr, marmux
+
+      # normal alu instruction...
+
+      # handle setf
+      if asf and (aop == AOP_NOP):
+         asm_error('nop.setf is not allowed in add pipe')
+      if msf and (mop == MOP_NOP):
+         asm_warning('nop.setf, really?')
+      if (aop == AOP_NOP) or (acond == COND_NEVER):
+         sf = msf
+      else:
+         if msf:
+            asm_error('setf only allowed on mul op if add op is nop or add condition is never')
+         sf = asf
+
+      # handle read addrs
+      raddr_a = None
+      raddr_b = None
+      immb = False
+      arot_r5 = False
+      muxes = [0, 0, 0, 0]
+      if mwrot != 0:
+         raddr_b = 48 + mwrot
+         immb = True
+      if mwrot_r5 and have_am:
+         raddr_b = 48
+         immb = True
+      for f in lambda rmux: rmux != RMUX_ANY, lambda rmux: rmux == RMUX_ANY: # do RMUX_ANY last
+         for i, raddr, rmux in (0, aaraddr, aarmux), (1, abraddr, abrmux), (2, maraddr, marmux), (3, mbraddr, mbrmux):
+            if f(rmux):
+               raddr_a, raddr_b, immb, arot_r5, muxes[i] = merge_rmux(raddr_a, raddr_b, immb, arot_r5, raddr, rmux)
+      add_a, add_b, mul_a, mul_b = muxes
+      if (not read_rot_ok(mul_a, raddr_a, raddr_b)) or (not read_rot_ok(mul_b, raddr_a, raddr_b)):
+         # some output elements might not be as expected
+         if mwrot_r5 or ((mwrot >= 4) and (mwrot <= 12)):
+            bad_elems = 0xffff
+         else:
+            bad_elems = ((1 << (mwrot & 0x3)) - 1) * 0x1111
+            if mwrot > 12:
+               bad_elems ^= 0xffff
+         bad_elems &= dict(annots).get('mul_used', 0xffff)
+         if not msf:
+            if mwaddr == WADDR_NOP:
+               # not writing anywhere and not setting flags. no elements used
+               bad_elems = 0
+            elif ((mwaddr in (36, 40, 43, 49, 50, 51)) or
+               ((not ws) and (mwaddr == 37))):
+               # writing to tmurs/r5rep/unif_addr/unif_addr_rel/stencil/
+               # vr_setup/vw_setup/vr_addr/vw_addr/mutex and not setting flags.
+               # only use element 0
+               bad_elems &= 0x0001
+            elif ((mwaddr == 41) or (ws and (mwaddr == 37)) or
+               ((not ws) and (mwaddr == 42))):
+               # writing to r5quad/x_coord/y_coord/rev_flag and not setting
+               # flags. only use elements 0, 4, 8, and 12
+               bad_elems &= 0x1111
+         if bad_elems:
+            asm_warning('mul inputs don\'t come from accumulators (r0-r3). output may not be as expected')
+      if raddr_a is None:
+         raddr_a = RADDR_NOP
+      if raddr_b is None:
+         raddr_b = RADDR_NOP
+      if immb:
+         if sig != SIG_NORMAL:
+            asm_error('rotation/immediates and signal don\'t mix')
+         sig = SIG_SMALLIMMED
+      if arot_r5 or (mwrot_r5 and (not have_am)):
+         if sig != SIG_NORMAL:
+            asm_error('rotation/immediates/signal don\'t mix')
+         sig = SIG_ROTATE
+
+      # construct instruction
+      prog.append(((mul_b << 0) | (mul_a << 3) | (add_b << 6) | (add_a << 9) | (raddr_b << 12) | (raddr_a << 18) | (aop << 24) | (mop << 29),
+         (mwaddr << 0) | (awaddr << 6) | (ws << 12) | (sf << 13) | (mcond << 14) | (acond << 17) | (pack << 20) | (pm << 24) | (unpack << 25) | (sig << 28),
+         line, annots))
+   finally:
+      current_location = prev_location
+
+def preprocess_passthrough(file):
+   line_number = 0
+   for line in file:
+      line_number += 1
+      yield line_number, line
+
+def asm_file(sets, location, filename, preprocess = None):
+   global current_dir, current_location
+
+   if filename is None:
+      location = '<stdin>'
+      file = sys.stdin
+
+      prev_dir = current_dir
+   else:
+      filename = os.path.normpath(os.path.join(current_dir, filename))
+
+      try:
+         file = open(filename)
+      except Exception, e:
+         asm_error(e)
+      except:
+         asm_error('unknown error while opening file %s' % filename)
+
+      prev_dir = current_dir
+      current_dir = os.path.dirname(filename)
+
+   prev_location = current_location
+   current_location = location
+
+   if preprocess is None:
+      preprocess = preprocess_passthrough
+
+   try:
+      for line_number, line in preprocess(file):
+         # strip off comments and whitespace
+         line = line.split('#')[0].strip()
+         if line == '':
+            continue
+
+         asm_line(sets, '%s: %d' % (current_location, line_number), line)
+   finally:
+      current_dir = prev_dir
+      current_location = prev_location
+
+def asm_end_prog():
+   # check we aren't in a multi-line construct (eg .macro or .rep)
+   if construct != None:
+      asm_error({
+         CONSTRUCT_MACRO: '.macro without .endm',
+         CONSTRUCT_IF:    '.if/.elif without .endif',
+         CONSTRUCT_ELSE:  '.else without .endif',
+         CONSTRUCT_REP:   '.rep without .endr'}[construct_stack[-1]])
+
+   # check no warnings level back to 0
+   if nwarn_level != 0:
+      asm_error('.pushnwarn without .popnwarn')
+
+   # flush queued up data
+   asm_flush_prog_data()
+
+   # fixup all the label references we can
+   for pc in xrange(len(prog)):
+      if isinstance(prog[pc][0], tuple):
+         location, label, rel, offset = prog[pc][0]
+         if label[0].isdigit():
+            label_pcs = labels.get(label[:-1], [])
+            if label[-1] == 'b':
+               label_pcs = filter(lambda label_pc: label_pc <= pc, label_pcs)[-1:]
+            else:
+               label_pcs = filter(lambda label_pc: label_pc > pc, label_pcs)[:1]
+            if label_pcs == []:
+               asm_error('search for label reached begin/end of file', location = location)
+            imm = label_pcs[0]
+         elif label in labels:
+            imm = labels[label]
+         elif (':' + label) in labels:
+            imm = labels[':' + label]
+         elif external_link:
+            continue # let the external linker deal with it
+         else:
+            asm_error('undefined label', location = location)
+         imm = (imm * 8) + offset
+         if rel:
+            imm -= (pc + 4) * 8 # relative to instruction after delay slots
+            imm &= (1 << 32) - 1
+         else:
+            if not external_link:
+               asm_error('can\'t get absolute address without using an external linker. this mode doesn\'t have an external linker', location = location)
+            imm = (location, label, rel, offset, imm)
+         prog[pc] = (imm,) + prog[pc][1:]
+
+def asm_init():
+   global current_dir, current_location, prog, prog_data, macros, labels, construct, construct_stack, nwarn_level
+
+   current_dir = os.getcwd()
+   current_location = ''
+   prog = []
+   prog_data = []
+   macros = {
+      'sacq': (['dst', 'i'], [('candyland', 'mov  dst, sacq(i)')]),
+      'srel': (['dst', 'i'], [('candyland', 'mov  dst, srel(i)')])}
+   labels = {}
+   construct = None
+   construct_stack = []
+   nwarn_level = 0
+
+def asm_reset_prog():
+   global prog, labels
+
+   prog = []
+   labels = {}
+
+###############################################################################
+# dumping
+###############################################################################
+
+def print_lines(lines):
+   for line in lines:
+      print line
+
+class dumper_t:
+   def external_link(self): return False
+   def begin(self): pass
+   def label(self, pc, name): pass
+   def line(self, pc, ls, ms, line, annots, first): pass
+   def end(self): pass
+   def sets(self, sets): pass
+   def direct(self, line): pass
+
+class clif_dumper_t(dumper_t):
+   def __init__(self):
+      self.annot_mode = 0
+
+   def external_link(self):
+      return True
+
+   def parse_annot_mode(self, line):
+      l = line.split(',')
+      self.annot_mode = int(l[0])
+      if self.annot_mode not in (0, 1, 2):
+         asm_error('bad annot mode')
+      if self.annot_mode == 2:
+         if len(l) != 2:
+            asm_error('expected buffer name')
+         self.annot_name = l[1].strip()
+         self.annot_offset = 0
+      elif len(l) != 1:
+         asm_error('unexpected comma')
+
+   def label(self, pc, name):
+      if (self.annot_mode != 1) and (name[0] == ':'):
+         if self.annot_mode == 2:
+            name = name + '_annotations'
+         print '@label %s' % name[1:]
+      else:
+         print '// :%s' % name
+
+   def line(self, pc, ls, ms, line, annots, first):
+      if self.annot_mode == 0:
+         if isinstance(ls, tuple):
+            if len(ls) == 5:
+               location, label, rel, offset, offset_from_prog = ls
+               assert not rel
+               ls = '[. - %d + %d]' % (pc * 8, offset_from_prog)
+            else:
+               location, label, rel, offset = ls
+               if rel:
+                  asm_error('relative external label references not allowed in this mode', location = location)
+               ls = '[%s + %d]' % (label, offset)
+         else:
+            ls = '0x%08x' % ls
+         print '%s 0x%08x // %s' % (ls, ms, line)
+      elif self.annot_mode == 1:
+         print '// %s' % line
+         for annot in annots:
+            print '0x%08x 0x%08x // %s' % ({
+               # todo: would rather not have these hard coded
+               'mul_used':              1,
+               'preserve_cond':         2,
+               'geomd_open':            3,
+               'geomd_i':               4,
+               'geomd_tris_clear':      5,
+               'geomd_verts':           6,
+               'geomd_tris_add':        7,
+               'geomd_tris_set_center': 8,
+               'geomd_region_clear':    9,
+               'geomd_region_set':      10,
+               'geomd_images_clear':    11,
+               'geomd_images_l':        12,
+               'geomd_images_b':        13,
+               'geomd_images_r':        14,
+               'geomd_images_t':        15,
+               'geomd_images_add_vpm':  16,
+               'trace_4c':              17,
+               'geomd_images_add_tex':  18,}[annot[0]], annot[1], annot[0])
+         if len(annots) != 0:
+            print '0x00000000 // end'
+      else:
+         assert self.annot_mode == 2
+         if len(annots) == 0:
+            print '0x00000000 // %s' % line
+         else:
+            print '[%s + %d] // %s' % (self.annot_name, self.annot_offset, line)
+            self.annot_offset += (len(annots) * 8) + 4
+
+   def direct(self, line):
+      print line
+
+class plain_dumper_t(dumper_t):
+   def line(self, pc, ls, ms, line, annots, first):
+      print '0x%08x, 0x%08x, // %s' % (ls, ms, line)
+
+class c_c_dumper_t(dumper_t):
+   def __init__(self, header_name, full_header_name, array_name):
+      self.header_name = header_name
+      self.array_name = array_name
+
+   def external_link(self):
+      return True
+
+   def begin(self):
+      self.external_labels = set()
+      self.lines = []
+
+      print '#include "%s.h"' % self.header_name
+      print ''
+      print '#ifdef _MSC_VER'
+      print '   #include <stdint.h>'
+      print '   /* cast through uintptr_t to avoid warnings */'
+      print '   #define POINTER_TO_UINT(X) ((unsigned int)(uintptr_t)(X))'
+      print '#else'
+      print '   #define POINTER_TO_UINT(X) ((unsigned int)(X))'
+      print '#endif'
+      print ''
+      print '#ifdef __cplusplus'
+      print 'extern "C" { /* the types are probably wrong... */'
+      print '#endif'
+
+   def label(self, pc, name):
+      self.lines.append('// :%s' % name)
+
+   def line(self, pc, ls, ms, line, annots, first):
+      if isinstance(ls, tuple):
+         if len(ls) == 5:
+            location, label, rel, offset, offset_from_prog = ls
+            assert not rel
+            ls = 'POINTER_TO_UINT(%s) + %d' % (self.array_name, offset_from_prog)
+         else:
+            location, label, rel, offset = ls
+            if rel:
+               asm_error('relative external label references not allowed in this mode', location = location)
+            if label not in self.external_labels:
+               self.external_labels.add(label)
+               print 'extern uint8_t %s[];' % label
+            ls = 'POINTER_TO_UINT(%s) + %d' % (label, offset)
+      else:
+         ls = '0x%08x' % ls
+      self.lines.append('/* [0x%08x] */ %s, 0x%08x, // %s' % (pc * 8, ls, ms, line))
+
+   def end(self):
+      print '#ifdef __cplusplus'
+      print '}'
+      print '#endif'
+      print ''
+      print '#ifdef _MSC_VER'
+      print '__declspec(align(8))'
+      print '#elif defined(__GNUC__)'
+      print '__attribute__((aligned(8)))'
+      print '#endif'
+      print 'unsigned int %s[] = {' % self.array_name
+      print_lines(self.lines)
+      print '};'
+      print '#ifdef __HIGHC__'
+      print '#pragma Align_to(8, %s)' % self.array_name
+      print '#endif'
+
+class c_h_dumper_t(dumper_t):
+   def __init__(self, header_name, full_header_name, array_name):
+      self.full_header_name = full_header_name
+      self.array_name = array_name
+
+   def external_link(self):
+      return True
+
+   def begin(self):
+      print '#ifndef %s_H' % self.full_header_name
+      print '#define %s_H' % self.full_header_name
+      print ''
+      print 'extern unsigned int %s[];' % self.array_name
+      print ''
+
+   def label(self, pc, name):
+      if name[0] == ':':
+         print '#define %s (%s + %d)' % (name[1:], self.array_name, pc * 2)
+
+   def end(self):
+      print ''
+      print '#endif'
+
+class ml_c_dumper_t(dumper_t):
+   def __init__(self, header_name, full_header_name, name, annots):
+      self.header_name = header_name
+      self.name = name
+      self.annots = annots
+
+   def external_link(self):
+      return True
+
+   def begin(self):
+      if self.annots:
+         self.annot_lines = []
+      self.lines = []
+      self.external_labels = set()
+      self.link_lines = []
+
+      print '#include "%s.h"' % self.header_name
+      print '#include <assert.h>'
+      if self.annots:
+         print '#ifdef SIMPENROSE'
+         print '#include <stddef.h>'
+         print '#include "v3d/verification/tools/2760sim/simpenrose.h"'
+      print ''
+
+   def label(self, pc, name):
+      self.lines.append('// :%s' % name)
+
+   def line(self, pc, ls, ms, line, annots, first):
+      if self.annots:
+         if len(annots) == 0:
+            self.annot_lines.append('NULL,')
+         else:
+            print 'static unsigned int const annotations_%d[] = {' % pc
+            for annot in annots:
+               print '   SIMPENROSE_SHADER_ANNOTATION_%s, 0x%08x,' % (annot[0].upper(), annot[1])
+            print '   SIMPENROSE_SHADER_ANNOTATION_END};'
+            print ''
+            self.annot_lines.append('annotations_%d,' % pc)
+      if isinstance(ls, tuple):
+         self.link_lines.append('   assert(p[%d] == 0xdeadbeef);' % (pc * 2))
+         if len(ls) == 5:
+            location, label, rel, offset, offset_from_prog = ls
+            assert not rel
+            self.link_lines.append('   p[%d] = base + %d;' % (pc * 2, offset_from_prog))
+         else:
+            location, label, rel, offset = ls
+            self.external_labels.add(label)
+            if rel:
+               self.link_lines.append('   p[%d] = (%s + %d) - (base + %d);' % (pc * 2, label, offset, (pc + 4) * 8))
+            else:
+               self.link_lines.append('   p[%d] = %s + %d;' % (pc * 2, label, offset))
+         ls = '0xdeadbeef'
+      else:
+         ls = '0x%08x' % ls
+      self.lines.append('/* [0x%08x] */ %s, 0x%08x, // %s' % (pc * 8, ls, ms, line))
+
+   def end(self):
+      if self.annots:
+         print 'unsigned int const *const %s_annotations_array[] = {' % self.name
+         print_lines(self.annot_lines)
+         print '};'
+         print '#endif'
+         print ''
+      print 'static unsigned int const array[] = {'
+      print_lines(self.lines)
+      print '};'
+      print ''
+      print 'void %s_link(void *p_in, unsigned int base' % self.name
+      for label in sorted(self.external_labels):
+         print '   , unsigned int %s' % label
+      print '   )'
+      print '{'
+      print '   unsigned int *p = (unsigned int *)p_in;'
+      print '   unsigned int i;'
+      print '   for (i = 0; i != (%s_SIZE / 4); ++i) {' % self.name.upper()
+      print '      p[i] = array[i];'
+      print '   }'
+      print_lines(self.link_lines)
+      print '}'
+
+class ml_h_dumper_t(dumper_t):
+   def __init__(self, header_name, full_header_name, name, annots):
+      self.full_header_name = full_header_name
+      self.name = name
+      self.annots = annots
+
+   def external_link(self):
+      return True
+
+   def begin(self):
+      self.external_labels = set()
+      self.lines_n = 0
+
+      print '#ifndef %s_H' % self.full_header_name
+      print '#define %s_H' % self.full_header_name
+      print ''
+      if self.annots:
+         print '#ifdef SIMPENROSE'
+         print '   extern unsigned int const *const %s_annotations_array[];' % self.name
+         print '#endif'
+         print ''
+
+   def label(self, pc, name):
+      if name[0] == ':':
+         print '#define %s_OFFSET %d' % (name[1:].upper(), pc * 8)
+         if self.annots:
+            print '#ifdef SIMPENROSE'
+            print '   #define %s_annotations (%s_annotations_array + %d)' % (name[1:], self.name, pc)
+            print '#endif'
+
+   def line(self, pc, ls, ms, line, annots, first):
+      if isinstance(ls, tuple) and (len(ls) != 5):
+         self.external_labels.add(ls[1])
+      self.lines_n += 1
+
+   def end(self):
+      print ''
+      print 'extern void %s_link(void *p, unsigned int base' % self.name
+      for label in sorted(self.external_labels):
+         print '   , unsigned int %s' % label
+      print '   );'
+      print ''
+      print '#define %s_SIZE %d' % (self.name.upper(), (self.lines_n * 8))
+      print ''
+      print '#endif'
+
+def print_lines_lc(lines):
+   for line in lines:
+      print '%s \\' % line
+
+def print_groups_lc(groups):
+   first = True
+   for group in groups:
+      if first:
+         print '{ \\'
+      else:
+         print ', { \\'
+      print_lines_lc(group)
+      print '} \\'
+      first = False
+
+class inline_c_dumper_t(dumper_t):
+   def __init__(self, annots):
+      self.annots = annots
+      self.iteration = False
+
+   def begin_iteration(self):
+      assert not self.iteration
+      self.iteration = True
+      self.iteration_lines = []
+      if self.annots:
+         self.iteration_annot_lines = []
+         self.annot_arrs = []
+
+   def end_iteration(self):
+      assert self.iteration
+      self.iteration = False
+      print '%d, \\' % self.iteration_n
+      if self.annots:
+         print '( \\'
+      print_groups_lc(self.iteration_lines)
+      if self.annots:
+         print '), ( \\'
+         print_groups_lc(self.iteration_annot_lines)
+         print '), ( \\'
+         for annot_arr in self.annot_arrs:
+            print_lines_lc(annot_arr)
+         print ') \\'
+
+   def begin(self):
+      self.n = 0
+      self.lines = []
+      if self.annots:
+         self.annot_lines = []
+         if not self.iteration:
+            self.annot_arrs = []
+
+   def label(self, pc, name):
+      self.lines.append('/* :%s */' % name)
+      if self.annots:
+         self.annot_lines.append('/* :%s */' % name)
+
+   def line(self, pc, ls, ms, line, annots, first):
+      self.n += 1
+      if first:
+         prefix = ''
+      else:
+         prefix = ', '
+      self.lines.append('%s0x%08x, 0x%08x /* %s */' % (prefix, ls, ms, line))
+      if self.annots:
+         if len(annots) == 0:
+            a = 'NULL'
+         else:
+            a = 'annotations_%d' % len(self.annot_arrs)
+            annot_arr = ['static unsigned int const annotations_%d[] = {' % len(self.annot_arrs)]
+            for annot in annots:
+               annot_arr.append('   SIMPENROSE_SHADER_ANNOTATION_%s, 0x%08x,' % (annot[0].upper(), annot[1]))
+            annot_arr.append('   SIMPENROSE_SHADER_ANNOTATION_END};')
+            self.annot_arrs.append(annot_arr)
+         self.annot_lines.append('%s%s /* %s */' % (prefix, a, line))
+
+   def end(self):
+      if self.iteration:
+         if len(self.iteration_lines) == 0:
+            self.iteration_n = self.n
+         elif self.iteration_n != self.n:
+            asm_error('number of instructions differs between iterations')
+         self.iteration_lines.append(self.lines)
+         if self.annots:
+            self.iteration_annot_lines.append(self.annot_lines)
+      else:
+         if self.annots:
+            print '( \\'
+         print_lines_lc(self.lines)
+         if self.annots:
+            print '), ( \\'
+            print_lines_lc(self.annot_lines)
+            print '), ( \\'
+            for annot_arr in self.annot_arrs:
+               print_lines_lc(annot_arr)
+            print ') \\'
+
+   def direct(self, line):
+      print line
+
+class asvc_dumper_t(dumper_t):
+   def external_link(self):
+      return True
+
+   def begin(self):
+      print '.align 8'
+
+   def label(self, pc, name):
+      if name[0] == ':':
+         print '%s::' % name[1:]
+      else:
+         print '%s:' % name
+
+   def line(self, pc, ls, ms, line, annots, first):
+      if isinstance(ls, tuple):
+         location, label, rel, offset = ls[:4]
+         if rel:
+            ls = '%s + %d - (. + 32)' % (label, offset)
+         else:
+            ls = '%s + %d' % (label, offset)
+      else:
+         ls = '0x%08x' % ls
+      print '.word %s, 0x%08x ; %s' % (ls, ms, line)
+
+def is_ra_or_rb(val):
+   return isinstance(val, loc_t) and ((val.mux == MUX_A) or (val.mux == MUX_B))
+
+class aliases_dumper_t(dumper_t):
+   def external_link(self):
+      return True
+
+   def begin(self):
+      print '#ifndef JUST_DQASM_ARGS'
+
+   def label(self, pc, name):
+      if not name[0].isdigit():
+         if name[0] == ':':
+            name = name[1:]
+         print '"bs%s", "bs%x",' % (name, pc * 8)
+         print '"bu%s", "bu%x",' % (name, pc * 8)
+
+   def end(self):
+      print '#endif'
+
+   # todo: handle things other than ra and rb? dqasm only allows ra and rb atm
+   def sets(self, sets):
+      dqasm_args = []
+      print '#ifndef JUST_DQASM_ARGS'
+      for name in sets:
+         if is_ra_or_rb(sets[name]):
+            dqasm_args.append('-r%s=%s' % (sets[name], name))
+            print '"%s", "%s",' % (name, sets[name])
+         elif isinstance(sets[name], list):
+            for i, val in enumerate(sets[name]):
+               if is_ra_or_rb(val):
+                  dqasm_args.append('-r%s=%s[%d]' % (val, name, i))
+                  print '"%s[%d]", "%s",' % (name, i, val)
+      print '#endif'
+      print '#define DQASM_ARGS "%s"' % ' '.join(dqasm_args)
+
+def dump(dumper):
+   if (len(prog) != 0) or (len(labels) != 0):
+      dumper.begin()
+
+      sorted_labels = []
+      for name in labels:
+         if name[0].isdigit():
+            for pc in labels[name]:
+               sorted_labels.append((pc, name))
+         else:
+            sorted_labels.append((labels[name], name))
+      sorted_labels.sort(reverse = True)
+
+      first = True
+      for pc in xrange(len(prog)):
+         ls, ms, line, annots = prog[pc]
+         while (len(sorted_labels) != 0) and (sorted_labels[-1][0] == pc):
+            dumper.label(*sorted_labels.pop())
+         dumper.line(pc, ls, ms, line, annots, first)
+         first = False
+      for sorted_label in sorted_labels:
+         assert sorted_label[0] == len(prog)
+         dumper.label(*sorted_label)
+
+      dumper.end()
+
+###############################################################################
+# preprocessing
+###############################################################################
+
+def preprocess_inline_c(dumper):
+   def preprocess(file):
+      ls = None
+      line_number = 0
+      for line in file:
+         line_number += 1
+         while True:
+            if ls is None:
+               l = line.split('%[', 1)
+               if len(l) == 1:
+                  dumper.direct(l[0].rstrip())
+                  break
+               dumper.direct('%s \\' % l[0].rstrip())
+               line = l[1]
+               ls = []
+            else:
+               l = line.split('%]', 1)
+               ls.append((line_number, l[0]))
+               if len(l) == 1:
+                  break
+               line = l[1]
+               l = ls[-1][1].split('%|', 1)
+               if len(l) == 1:
+                  for l_number, l in ls:
+                     yield l_number, l
+                  asm_end_prog()
+                  dump(dumper)
+                  asm_reset_prog()
+               else:
+                  ls[-1] = (ls[-1][0], l[0])
+                  if hasattr(dumper, 'begin_iteration'):
+                     dumper.begin_iteration()
+                  for repls in l[1].split('%,'):
+                     repls = [repl.strip() for repl in repls.split('%/')]
+                     for l_number, l in ls:
+                        for i, repl in enumerate(repls):
+                           l = l.replace('%' + str(i), repl)
+                        yield l_number, l
+                     asm_end_prog()
+                     dump(dumper)
+                     asm_reset_prog()
+                  if hasattr(dumper, 'end_iteration'):
+                     dumper.end_iteration()
+               ls = None
+   return preprocess
+
+def preprocess_clif(dumper):
+   def preprocess(file):
+      in_asm = False
+      line_number = 0
+      for line in file:
+         line_number += 1
+         if in_asm:
+            if line.strip() == '%]':
+               asm_end_prog()
+               dump(dumper)
+               asm_reset_prog()
+               in_asm = False
+            else:
+               yield line_number, line
+         else:
+            if line.strip() == '%[':
+               in_asm = True
+            elif (line[:1] == '%') and (line[:2] != '%@'):
+               yield line_number, line[1:]
+            else:
+               asm_end_prog()
+               dump(dumper)
+               asm_reset_prog()
+               if line[:2] == '%@':
+                  if hasattr(dumper, 'parse_annot_mode'):
+                     dumper.parse_annot_mode(line[2:])
+               else:
+                  dumper.direct(line.rstrip())
+   return preprocess
+
+###############################################################################
+# main
+###############################################################################
+
+def main():
+   global external_link, allow_xor_0, dont_warn_when_mul_rot_inp_r5
+   global warnings_are_errors, disable_warnings, have_sema, have_am, mulw_rotate
+
+   asm_init() # do this first so we can use asm_error without having to pass a location and so asm_warning will work
+
+   # parse command line
+   parser = optparse.OptionParser(usage = 'usage: %prog [options] <filename>')
+   parser.add_option('-m', '--mode', dest = 'mode',
+      help = '<mode> should be clif, plain, ' +
+      'c_c:<header_name>,<full_header_name>,<array_name>, ' +
+      'c_h:<header_name>,<full_header_name>,<array_name>, ' +
+      'ml_c:<header_name>,<full_header_name>,<name>[,annots], ' +
+      'ml_h:<header_name>,<full_header_name>,<name>[,annots], ' +
+      'inline_c[:annots], asvc, or aliases[:<preprocess_mode>]', metavar = '<mode>')
+   parser.add_option('-t', '--target', dest = 'target',
+      help = '<target> should be a0, b0, or hera', metavar = '<target>')
+   parser.add_option('-x', '--allow_xor_0', dest = 'allow_xor_0', action = 'store_true', default = False)
+   parser.add_option('-r', '--dont_warn_when_mul_rot_inp_r5', dest = 'dont_warn_when_mul_rot_inp_r5', action = 'store_true', default = False)
+   parser.add_option('-w', '--warnings_are_errors', dest = 'warnings_are_errors', action = 'store_true', default = False)
+   parser.add_option('-d', '--disable_warnings', dest = 'disable_warnings', action = 'store_true', default = False)
+   parser.add_option('-s', '--set', dest = 'sets', action = 'append', default = [], metavar = '<name>=<val>')
+   options, args = parser.parse_args()
+   if len(args) == 0:
+      filename = None
+   elif len(args) == 1:
+      filename = args[0]
+   else:
+      parser.print_help()
+      sys.exit(-1)
+
+   # handle mode
+   mode = options.mode or 'clif' # assume clif if no mode specified
+   if mode == 'clif':
+      dumper = clif_dumper_t()
+      preprocess = preprocess_clif(dumper)
+   elif mode == 'plain':
+      dumper = plain_dumper_t()
+      preprocess = None
+   elif (mode[:4] == 'c_c:') or (mode[:4] == 'c_h:'):
+      mode_options = mode[4:].split(',')
+      if len(mode_options) != 3:
+         asm_error('badly formatted mode on command line')
+      dumper = {'c_c': c_c_dumper_t, 'c_h': c_h_dumper_t}[mode[:3]](*mode_options)
+      preprocess = None
+   elif (mode[:5] == 'ml_c:') or (mode[:5] == 'ml_h:'):
+      mode_options = mode[5:].split(',')
+      if (len(mode_options) != 3) and ((len(mode_options) != 4) or (mode_options[3] != 'annots')):
+         asm_error('badly formatted mode on command line')
+      dumper = {'ml_c': ml_c_dumper_t, 'ml_h': ml_h_dumper_t
+         }[mode[:4]](*(mode_options[:3] + [len(mode_options) == 4]))
+      preprocess = None
+   elif mode == 'inline_c':
+      dumper = inline_c_dumper_t(False)
+      preprocess = preprocess_inline_c(dumper)
+   elif mode == 'inline_c:annots':
+      dumper = inline_c_dumper_t(True)
+      preprocess = preprocess_inline_c(dumper)
+   elif mode == 'asvc':
+      dumper = asvc_dumper_t()
+      preprocess = None
+   elif mode == 'aliases':
+      dumper = aliases_dumper_t()
+      preprocess = None
+   elif mode == 'aliases:inline_c':
+      dumper = aliases_dumper_t()
+      preprocess = preprocess_inline_c(dumper)
+   else:
+      asm_error('invalid mode')
+   external_link = dumper.external_link()
+
+   # handle target
+   target = options.target or 'b0' # assume b0 if no target specified
+   if target == 'a0':
+      have_sema = False
+      have_am = False
+      mulw_rotate = False
+      have_lthrsw = False
+   elif target == 'b0':
+      have_sema = True
+      have_am = True
+      mulw_rotate = True
+      have_lthrsw = True
+   elif target == 'hera':
+      have_sema = True
+      have_am = False
+      mulw_rotate = True
+      have_lthrsw = True
+   else:
+      asm_error('invalid target')
+   if have_am:
+      sigs['loadam'] = SIG_LOADAM
+      arg_defs['tlbam'] = loc_t(MUX_ANY, 47, 0, 0, None, RW_WRITE)
+   if have_lthrsw:
+      sigs['lthrsw'] = SIG_LTHRSW
+      del sigs['int']
+      arg_defs['interrupt'] = loc_t(MUX_ANY, 38, 0, 0, None, RW_WRITE)
+
+   # handle misc options
+   allow_xor_0 = options.allow_xor_0
+   dont_warn_when_mul_rot_inp_r5 = options.dont_warn_when_mul_rot_inp_r5
+   warnings_are_errors = options.warnings_are_errors
+   disable_warnings = options.disable_warnings
+
+   # make options visible to asm
+   arg_defs['mode'] = mode
+   arg_defs['target'] = target
+
+   # arg_defs all setup at this point
+   sets = arg_defs.copy() # todo: see arg_eval
+
+   # handle command line sets
+   re_options_set = re.compile('(?P<name>\\w+)=(?P<val>.+)$')
+   for options_set in options.sets:
+      m = re_options_set.match(options_set)
+      if not m:
+         asm_error('badly formatted set on command line')
+      sets[m.group('name')] = arg_eval(m.group('val'), sets)
+
+   # assemble input file and dump
+   asm_file(sets, filename, filename, preprocess)
+   asm_end_prog()
+   dump(dumper)
+   for name in arg_defs: # todo: see arg_eval
+      del sets[name]
+   dumper.sets(sets)
+
+if __name__ == '__main__':
+   main()
diff -Naur ffmpeg-3.2.4/pi-util/qem.sh ffmpeg-3.2.4.patch/pi-util/qem.sh
--- ffmpeg-3.2.4/pi-util/qem.sh	1970-01-01 01:00:00.000000000 +0100
+++ ffmpeg-3.2.4.patch/pi-util/qem.sh	2017-05-28 20:42:45.767088769 +0200
@@ -0,0 +1,9 @@
+TARGET_DIR=../src/eupton_vc4dev_2012a/software/vc4/DEV/applications/tutorials/user_shader_example_tex
+QASM=python\ pi-util/qasm.py
+SRC_FILE=libavcodec/rpi_shader.qasm
+DST_BASE=shader
+
+cp libavcodec/rpi_shader_cmd.h $TARGET_DIR
+$QASM -mc_c:$DST_BASE,$DST_BASE,$DST_BASE $SRC_FILE > $TARGET_DIR/$DST_BASE.c
+$QASM -mc_h:$DST_BASE,$DST_BASE,$DST_BASE $SRC_FILE > $TARGET_DIR/$DST_BASE.h
+
diff -Naur ffmpeg-3.2.4/pi-util/rebase_liblinks.py ffmpeg-3.2.4.patch/pi-util/rebase_liblinks.py
--- ffmpeg-3.2.4/pi-util/rebase_liblinks.py	1970-01-01 01:00:00.000000000 +0100
+++ ffmpeg-3.2.4.patch/pi-util/rebase_liblinks.py	2017-05-28 20:42:45.767088769 +0200
@@ -0,0 +1,37 @@
+#!/usr/bin/env python
+
+import os, sys
+from stat import *
+
+def walktree(top, callback, n, prefix):
+    '''recursively descend the directory tree rooted at top,
+       calling the callback function for each regular file'''
+
+    for f in os.listdir(top):
+        pathname = os.path.join(top, f)
+        mode = os.lstat(pathname).st_mode
+        if S_ISDIR(mode):
+            # It's a directory, recurse into it
+            walktree(pathname, callback, n+1, prefix)
+        elif S_ISLNK(mode):
+            # It's a file, call the callback function
+            callback(pathname, os.readlink(pathname), n, prefix)
+
+def visitfile(file, linkname, n, prefix):
+    if (linkname.startswith(prefix + 'lib/')):
+        newlink = "../" * n + linkname[len(prefix):]
+        print 'relinking', file, "->", newlink
+        os.remove(file)
+        os.symlink(newlink, file)
+
+if __name__ == '__main__':
+    argc = len(sys.argv)
+    if argc == 2:
+        walktree(sys.argv[1], visitfile, 0, "/")
+    elif argc == 3:
+        walktree(sys.argv[1], visitfile, 0, sys.argv[2])
+    else:
+        print "rebase_liblinks.py <local root> [<old sysroot>]"
+
+
+
diff -Naur ffmpeg-3.2.4/pi-util/syncroot.sh ffmpeg-3.2.4.patch/pi-util/syncroot.sh
--- ffmpeg-3.2.4/pi-util/syncroot.sh	1970-01-01 01:00:00.000000000 +0100
+++ ffmpeg-3.2.4.patch/pi-util/syncroot.sh	2017-05-28 20:42:45.767088769 +0200
@@ -0,0 +1,43 @@
+set -e
+
+if [ "$1" == "" ]; then
+  echo Usage: $0 \<src_dir\> [\<rootname\>]
+  echo src_dir is a source for rsync so may contain m/c name.
+  echo rootname will be set to \"raspian_jessie_pi1\" if missing
+  echo e.g.: pi-util/syncroot.sh my-pi: raspian_jessie_pi1
+  exit 1
+fi
+
+SYSROOT_NAME=$2
+if [ "$SYSROOT_NAME" == "" ]; then
+  SYSROOT_NAME=raspian_jessie_pi1
+fi
+
+DST_ROOT=`pwd`
+DST=$DST_ROOT/build/linux/$SYSROOT_NAME-sysroot
+SRC=$1
+
+echo Sync src:  $SRC
+echo Sync dest: $DST
+
+mkdir -p $DST/lib
+mkdir -p $DST/opt/vc/include
+mkdir -p $DST/usr/lib/pkgconfig
+mkdir -p $DST/usr/bin
+mkdir -p $DST/usr/share
+
+#### MUST NOT include /opt/vc/include/*GL*
+# Creates conflicts with GL includes inside Chrome
+
+rsync -rl $SRC/lib/arm-linux-gnueabihf $DST/lib
+rsync -rl $SRC/opt/vc/lib $DST/opt/vc
+rsync -l  $SRC/opt/vc/include/bcm_host.h $DST/opt/vc/include
+rsync -rl $SRC/opt/vc/include/interface $DST/opt/vc/include
+rsync -rl $SRC/opt/vc/include/vcinclude $DST/opt/vc/include
+rsync -rl $SRC/usr/lib/arm-linux-gnueabihf $DST/usr/lib
+rsync -rl $SRC/usr/lib/gcc $DST/usr/lib
+rsync -rl $SRC/usr/include $DST/usr
+
+pi-util/rebase_liblinks.py $DST
+
+
diff -Naur ffmpeg-3.2.4/pi-util/v3dusage.py ffmpeg-3.2.4.patch/pi-util/v3dusage.py
--- ffmpeg-3.2.4/pi-util/v3dusage.py	1970-01-01 01:00:00.000000000 +0100
+++ ffmpeg-3.2.4.patch/pi-util/v3dusage.py	2017-05-28 20:42:45.768088773 +0200
@@ -0,0 +1,128 @@
+#!/usr/bin/env python
+
+import sys
+import argparse
+import re
+
+def do_logparse(logname):
+
+    rmatch = re.compile(r'^([0-9]+\.[0-9]{3}): (done )?((vpu0)|(vpu1)|(qpu1)) ([A-Z_]+) cb:([0-9a-f]+) ')
+    rqcycle = re.compile(r'^([0-9]+\.[0-9]{3}): v3d: QPU Total clock cycles for all QPUs doing vertex/coordinate shading +([0-9]+)$')
+    rqtscycle = re.compile(r'^([0-9]+\.[0-9]{3}): v3d: QPU Total clock cycles for all QPUs stalled waiting for TMUs +([0-9]+)$')
+    rl2hits = re.compile(r'^([0-9]+\.[0-9]{3}): v3d: L2C Total Level 2 cache ([a-z]+) +([0-9]+)$')
+
+    ttotal = {'idle':0.0}
+    tstart = {}
+    qctotal = {}
+    qtstotal = {}
+    l2hits = {}
+    l2total = {}
+    time0 = None
+    idle_start = None
+    qpu_op_no = 0
+    op_count = 0
+
+    with open(logname, "rt") as infile:
+        for line in infile:
+            match = rmatch.match(line)
+            if match:
+#                print match.group(1), ":", match.group(2), ":", match.group(3), ":", match.group(7), ":"
+                time = float(match.group(1))
+                unit = match.group(3)
+                opstart = not match.group(2)
+                optype = match.group(7)
+                hascb = match.group(8) != "0"
+
+                if unit == 'qpu1':
+                    unit = unit + "." + str(qpu_op_no)
+                    if not opstart:
+                        if hascb or optype == 'EXECUTE_SYNC':
+                            qpu_op_no = 0
+                        else:
+                            qpu_op_no += 1
+
+                # Ignore sync type
+                if optype == 'EXECUTE_SYNC':
+                    continue
+
+                if not time0:
+                    time0 = time
+
+                if opstart:
+                    tstart[unit] = time;
+                elif unit in tstart:
+                    op_count += 1
+                    if not unit in ttotal:
+                        ttotal[unit] = 0.0
+                    ttotal[unit] += time - tstart[unit]
+                    del tstart[unit]
+
+                if not idle_start and not tstart:
+                    idle_start = time
+                elif idle_start and tstart:
+                    ttotal['idle'] += time - idle_start
+                    idle_start = None
+
+            match = rqcycle.match(line)
+            if match:
+                unit = "qpu1." + str(qpu_op_no)
+                if not unit in qctotal:
+                    qctotal[unit] = 0
+                qctotal[unit] += int(match.group(2))
+
+            match = rqtscycle.match(line)
+            if match:
+                unit = "qpu1." + str(qpu_op_no)
+                if not unit in qtstotal:
+                    qtstotal[unit] = 0
+                qtstotal[unit] += int(match.group(2))
+
+            match = rl2hits.match(line)
+            if match:
+                unit = "qpu1." + str(qpu_op_no)
+                if not unit in l2total:
+                    l2total[unit] = 0
+                    l2hits[unit] = 0
+                l2total[unit] += int(match.group(3))
+                if match.group(2) == "hits":
+                    l2hits[unit] += int(match.group(3))
+
+
+    if not time0:
+        print "No v3d profile records found"
+    else:
+        tlogged = time - time0
+
+        print "Logged time:", tlogged, "  Op count:", op_count
+        for unit in sorted(ttotal):
+            print b'%6s: %10.3f    %7.3f%%' % (unit, ttotal[unit], ttotal[unit] * 100.0 / tlogged)
+        print
+        for unit in sorted(qctotal):
+            if not unit in qtstotal:
+                qtstotal[unit] = 0;
+            print b'%6s: Qcycles: %10d, TMU stall: %10d (%7.3f%%)' % (unit, qctotal[unit], qtstotal[unit], (qtstotal[unit] * 100.0)/qctotal[unit])
+            if unit in l2total:
+                print b'        L2Total: %10d, hits:      %10d (%7.3f%%)' % (l2total[unit], l2hits[unit], (l2hits[unit] * 100.0)/l2total[unit])
+
+
+
+if __name__ == '__main__':
+    argp = argparse.ArgumentParser(
+        formatter_class=argparse.RawDescriptionHelpFormatter,
+        description="QPU/VPU perf summary from VC logging",
+        epilog = """
+Will also summarise TMU stalls if logging requests set in qpu noflush param
+in the profiled code.
+
+Example use:
+  vcgencmd set_logging level=0xc0
+  <command to profile>
+  sudo vcdbg log msg >& t.log
+  v3dusage.py t.log
+""")
+
+    argp.add_argument("logfile")
+    args = argp.parse_args()
+
+    do_logparse(args.logfile)
+
