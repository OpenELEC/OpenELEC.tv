diff -Naur a/adler32.c b/adler32.c
--- a/adler32.c	2011-09-11 01:15:50.000000000 +0200
+++ b/adler32.c	2015-08-16 05:57:02.000000000 +0200
@@ -1,5 +1,6 @@
 /* adler32.c -- compute the Adler-32 checksum of a data stream
  * Copyright (C) 1995-2011 Mark Adler
+ * Copyright (C) 2010-2011 Jan Seiffert
  * For conditions of distribution and use, see copyright notice in zlib.h
  */
 
@@ -21,6 +22,31 @@
 #define DO8(buf,i)  DO4(buf,i); DO4(buf,i+4);
 #define DO16(buf)   DO8(buf,0); DO8(buf,8);
 
+#if defined(__alpha__)
+/* even if GCC can generate a mul by inverse, the code is really
+ * ugly (find global const pool pointer, load constant, a mul, lots
+ * of shifts/add/sub), up to 14 instructions. The replacement code
+ * only needs >= 5 instructions
+ */
+#  define NO_DIVIDE
+#elif defined(__mips__)
+// TODO: i hate synthetized processors
+/*
+ * If we have a full "high-speed" Multiply/Divide Unit,
+ * the old multiply-by-reciproc should be the best way
+ * (since then we should get a 32x32 mul in 2 cycles?),
+ * but wait, we need 4 muls == 8 + 2 shift + 2 sub + 2 load
+ * imidiate + other.
+ * If we do not have the "full" MDU, a mul takes 32 cycles
+ * and a div 25 (?!?).
+ * GCC generates a classic div, prop. needs the right -mtune
+ * for a mul.
+ * Use our hand rolled reduce, 17 simple instructions for both
+ * operands.
+ */
+#  define NO_DIVIDE
+#endif
+
 /* use NO_DIVIDE if your processor does not do division in hardware --
    try it both ways to see which is faster */
 #ifdef NO_DIVIDE
@@ -56,50 +82,223 @@
         if (a >= BASE) a -= BASE; \
     } while (0)
 #else
+#  define CHOP(a) a %= BASE
 #  define MOD(a) a %= BASE
 #  define MOD28(a) a %= BASE
 #  define MOD63(a) a %= BASE
 #endif
 
+local int host_is_bigendian()
+{
+    local const union {
+        uInt d;
+        unsigned char endian[sizeof(uInt)];
+    } x = {1};
+    return x.endian[0] == 0;
+}
+
+#ifndef NO_ADLER32_VEC
+#  if defined(__arm__)
+#    include "arm/adler32.c"
+#  elif defined(__alpha__)
+#    include "alpha/adler32.c"
+#  elif defined(__bfin__)
+#    include "bfin/adler32.c"
+#  elif defined(__ia64__)
+#    include "ia64/adler32.c"
+#  elif defined(__mips__)
+#    include "mips/adler32.c"
+#  elif defined(__powerpc__) || defined(__powerpc64__)
+#    include "ppc/adler32.c"
+#  elif defined(__sparc) || defined(__sparc__)
+#    include "sparc/adler32.c"
+#  elif defined(__tile__)
+#    include "tile/adler32.c"
+#  elif defined(__i386__) || defined(__x86_64__)
+#    include "x86/adler32.c"
+#  endif
+#endif
+
+#ifndef MIN_WORK
+#  define MIN_WORK 16
+#endif
+
 /* ========================================================================= */
-uLong ZEXPORT adler32(adler, buf, len)
+local noinline uLong adler32_1(adler, buf, len)
+    uLong adler;
+    const Bytef *buf;
+    uInt len GCC_ATTR_UNUSED_PARAM;
+{
+    unsigned long sum2;
+
+    /* split Adler-32 into component sums */
+    sum2 = (adler >> 16) & 0xffff;
+    adler &= 0xffff;
+
+    adler += buf[0];
+    if (adler >= BASE)
+        adler -= BASE;
+    sum2 += adler;
+    if (sum2 >= BASE)
+        sum2 -= BASE;
+    return adler | (sum2 << 16);
+}
+
+/* ========================================================================= */
+local noinline uLong adler32_common(adler, buf, len)
     uLong adler;
     const Bytef *buf;
     uInt len;
 {
     unsigned long sum2;
-    unsigned n;
 
     /* split Adler-32 into component sums */
     sum2 = (adler >> 16) & 0xffff;
     adler &= 0xffff;
 
-    /* in case user likes doing a byte at a time, keep it fast */
-    if (len == 1) {
-        adler += buf[0];
-        if (adler >= BASE)
-            adler -= BASE;
+    while (len--) {
+        adler += *buf++;
         sum2 += adler;
-        if (sum2 >= BASE)
-            sum2 -= BASE;
-        return adler | (sum2 << 16);
     }
+    if (adler >= BASE)
+        adler -= BASE;
+    MOD28(sum2);             /* only added so many BASE's */
+    return adler | (sum2 << 16);
+}
 
-    /* initial Adler-32 value (deferred check for len == 1 speed) */
-    if (buf == Z_NULL)
-        return 1L;
+#ifndef HAVE_ADLER32_VEC
+#  if (defined(__LP64__) || ((SIZE_MAX-0) >> 31) >= 2) && !defined(NO_ADLER32_VEC)
 
-    /* in case short lengths are provided, keep it somewhat fast */
-    if (len < 16) {
-        while (len--) {
-            adler += *buf++;
-            sum2 += adler;
-        }
-        if (adler >= BASE)
-            adler -= BASE;
-        MOD28(sum2);            /* only added so many BASE's */
-        return adler | (sum2 << 16);
-    }
+/* On 64 Bit archs, we can do pseudo SIMD with a nice win.
+ * This is esp. important for old Alphas, they do not have byte
+ * access.
+ * This needs some register but x86_64 is fine (>= 9 for the mainloop
+ * req.). If your 64 Bit arch is more limited, throw it away...
+ */
+#    undef VNMAX
+#    define VNMAX (2*NMAX+((9*NMAX)/10))
+
+/* ========================================================================= */
+local noinline uLong adler32_vec(adler, buf, len)
+    uLong adler;
+    const Bytef *buf;
+    uInt len;
+{
+    unsigned int s1, s2;
+    unsigned int k;
+
+    /* split Adler-32 into component sums */
+    s1 = adler & 0xffff;
+    s2 = (adler >> 16) & 0xffff;
+
+    /* align input data */
+    k    = ALIGN_DIFF(buf, sizeof(size_t));
+    len -= k;
+    if (k) do {
+        s1 += *buf++;
+        s2 += s1;
+    } while(--k);
+
+    k = len > VNMAX ? VNMAX : len;
+    len -= k;
+    if (likely(k >= 2 * sizeof(size_t))) do
+    {
+        unsigned int vs1, vs2;
+        unsigned int vs1s;
+
+        /* add s1 to s2 for rounds to come */
+        s2 += s1 * ROUND_TO(k, sizeof(size_t));
+        vs1s = vs1 = vs2 = 0;
+        do {
+            size_t vs1l = 0, vs1h = 0, vs1l_s = 0, vs1h_s = 0;
+            unsigned int a, b, c, d, e, f, g, h;
+            unsigned int j;
+
+            j = k > 23 * sizeof(size_t) ? 23 : k/sizeof(size_t);
+            k -= j * sizeof(size_t);
+            /* add s1 to s1 round sum for rounds to come */
+            vs1s += j * vs1;
+            do {
+                size_t in8 = *(const size_t *)buf;
+                buf += sizeof(size_t);
+                /* add this s1 to s1 round sum */
+                vs1l_s += vs1l;
+                vs1h_s += vs1h;
+                /* add up input data to s1 */
+                vs1l +=  in8 & UINT64_C(0x00ff00ff00ff00ff);
+                vs1h += (in8 & UINT64_C(0xff00ff00ff00ff00)) >> 8;
+            } while(--j);
+
+            /* split s1 */
+            if(host_is_bigendian()) {
+                a = (vs1h >> 48) & 0x0000ffff;
+                b = (vs1l >> 48) & 0x0000ffff;
+                c = (vs1h >> 32) & 0x0000ffff;
+                d = (vs1l >> 32) & 0x0000ffff;
+                e = (vs1h >> 16) & 0x0000ffff;
+                f = (vs1l >> 16) & 0x0000ffff;
+                g = (vs1h      ) & 0x0000ffff;
+                h = (vs1l      ) & 0x0000ffff;
+            } else {
+                a = (vs1l      ) & 0x0000ffff;
+                b = (vs1h      ) & 0x0000ffff;
+                c = (vs1l >> 16) & 0x0000ffff;
+                d = (vs1h >> 16) & 0x0000ffff;
+                e = (vs1l >> 32) & 0x0000ffff;
+                f = (vs1h >> 32) & 0x0000ffff;
+                g = (vs1l >> 48) & 0x0000ffff;
+                h = (vs1h >> 48) & 0x0000ffff;
+            }
+
+            /* add s1 & s2 horiz. */
+            vs2 += 8*a + 7*b + 6*c + 5*d + 4*e + 3*f + 2*g + 1*h;
+            vs1 += a + b + c + d + e + f + g + h;
+
+            /* split and add up s1 round sum */
+            vs1l_s = ((vs1l_s      ) & UINT64_C(0x0000ffff0000ffff)) +
+                     ((vs1l_s >> 16) & UINT64_C(0x0000ffff0000ffff));
+            vs1h_s = ((vs1h_s      ) & UINT64_C(0x0000ffff0000ffff)) +
+                     ((vs1h_s >> 16) & UINT64_C(0x0000ffff0000ffff));
+            vs1l_s += vs1h_s;
+            vs1s += ((vs1l_s      ) & UINT64_C(0x00000000ffffffff)) +
+                    ((vs1l_s >> 32) & UINT64_C(0x00000000ffffffff));
+        } while (k >= sizeof(size_t));
+        CHOP(vs1s);
+        s2 += vs1s * 8 + vs2;
+        CHOP(s2);
+        s1 += vs1;
+        CHOP(s1);
+        len += k;
+        k = len > VNMAX ? VNMAX : len;
+        len -= k;
+    } while (k >= sizeof(size_t));
+
+    /* handle trailer */
+    if (k) do {
+        s1 += *buf++;
+        s2 += s1;
+    } while (--k);
+    MOD28(s1);
+    MOD28(s2);
+
+    /* return recombined sums */
+    return (s2 << 16) | s1;
+}
+
+#  else
+
+/* ========================================================================= */
+local noinline uLong adler32_vec(adler, buf, len)
+    uLong adler;
+    const Bytef *buf;
+    uInt len;
+{
+    unsigned long sum2;
+    unsigned n;
+
+    /* split Adler-32 into component sums */
+    sum2 = (adler >> 16) & 0xffff;
+    adler &= 0xffff;
 
     /* do length NMAX blocks -- requires just one modulo operation */
     while (len >= NMAX) {
@@ -131,6 +330,73 @@
     /* return recombined sums */
     return adler | (sum2 << 16);
 }
+#  endif
+#endif
+
+/* ========================================================================= */
+#if MIN_WORK - 16 > 0
+#  ifndef NO_ADLER32_GE16
+local noinline uLong adler32_ge16(adler, buf, len)
+    uLong adler;
+    const Bytef *buf;
+    uInt len;
+{
+    unsigned long sum2;
+    unsigned n;
+
+    /* split Adler-32 into component sums */
+    sum2 = (adler >> 16) & 0xffff;
+    adler &= 0xffff;
+    n = len / 16;
+    len %= 16;
+
+    do {
+        DO16(buf); /* 16 sums unrolled */
+        buf += 16;
+    } while (--n);
+
+    /* handle trailer */
+    while (len--) {
+        adler += *buf++;
+        sum2 += adler;
+    }
+
+    MOD28(adler);
+    MOD28(sum2);
+
+    /* return recombined sums */
+    return adler | (sum2 << 16);
+}
+#  endif
+#  define COMMON_WORK 16
+#else
+#  define COMMON_WORK MIN_WORK
+#endif
+
+/* ========================================================================= */
+uLong ZEXPORT adler32(adler, buf, len)
+    uLong adler;
+    const Bytef *buf;
+    uInt len;
+{
+    /* in case user likes doing a byte at a time, keep it fast */
+    if (len == 1)
+        return adler32_1(adler, buf, len); /* should create a fast tailcall */
+
+    /* initial Adler-32 value (deferred check for len == 1 speed) */
+    if (buf == Z_NULL)
+        return 1L;
+
+    /* in case short lengths are provided, keep it somewhat fast */
+    if (len < COMMON_WORK)
+        return adler32_common(adler, buf, len);
+#if MIN_WORK - 16 > 0
+    if (len < MIN_WORK)
+        return adler32_ge16(adler, buf, len);
+#endif
+
+    return adler32_vec(adler, buf, len);
+}
 
 /* ========================================================================= */
 local uLong adler32_combine_(adler1, adler2, len2)
diff -Naur a/alpha/adler32.c b/alpha/adler32.c
--- a/alpha/adler32.c	1970-01-01 01:00:00.000000000 +0100
+++ b/alpha/adler32.c	2015-08-16 05:57:02.000000000 +0200
@@ -0,0 +1,136 @@
+/*
+ * adler32.c -- compute the Adler-32 checksum of a data stream
+ *   alpha implementation
+ * Copyright (C) 1995-2007 Mark Adler
+ * Copyright (C) 2010-2011 Jan Seiffert
+ * For conditions of distribution and use, see copyright notice in zlib.h
+ */
+
+/* @(#) $Id$ */
+
+#if defined(__GNUC__) && defined(__alpha_max__)
+#  define HAVE_ADLER32_VEC
+#  define MIN_WORK 32
+#  define VNMAX (2*NMAX+((9*NMAX)/10))
+
+#  define SOUL (sizeof(unsigned long))
+
+#  if GCC_VERSION_GE(303)
+#    define unpkbw  __builtin_alpha_unpkbw
+#    define perr    __builtin_alpha_perr
+#  else
+/* ========================================================================= */
+local inline unsigned long unpkbw(unsigned long a)
+{
+    unsigned long r;
+    __asm__ (".arch ev6; unpkbw	%r1, %0" : "=r" (r) : "rJ" (a));
+    return r;
+}
+
+/* ========================================================================= */
+local inline unsigned long perr(unsigned long a, unsigned long b)
+{
+    unsigned long r;
+    __asm__ (".arch ev6; perr	%r1, %r2, %0" : "=r" (r) : "rJ" (a), "rJ" (b));
+    return r;
+}
+#  endif
+
+/* ========================================================================= */
+local noinline uLong adler32_vec(adler, buf, len)
+    uLong adler;
+    const Bytef *buf;
+    uInt len;
+{
+    unsigned int s1, s2;
+    unsigned k;
+
+    /* split Adler-32 into component sums */
+    s1 = adler & 0xffff;
+    s2 = (adler >> 16) & 0xffff;
+
+    /* align input */
+    k    = ALIGN_DIFF(buf, SOUL);
+    len -= k;
+    if (k) do {
+        s1 += *buf++;
+        s2 += s1;
+    } while (--k);
+
+    k = len < VNMAX ? len : VNMAX;
+    len -= k;
+    if (likely(k >= 2 * SOUL)) {
+        unsigned long vs1 = s1, vs2 = s2;
+
+        do {
+            unsigned long vs1_r = 0;
+            do {
+                unsigned long a, b, c, d, e, f, g, h;
+                unsigned long vs2l = 0, vs2h = 0;
+                unsigned j;
+
+                j = k > 257 * SOUL ? 257 : k/SOUL;
+                k -= j * SOUL;
+                do {
+                    /* get input data */
+                    unsigned long in = *(const unsigned long *)buf;
+                    /* add vs1 for this round */
+                    vs1_r += vs1;
+                    /* add horizontal */
+                    vs1 += perr(in, 0);
+                    /* extract */
+                    vs2l += unpkbw(in);
+                    vs2h += unpkbw(in >> 32);
+                    buf += SOUL;
+                } while (--j);
+                /* split vs2 */
+                if(host_is_bigendian()) {
+                    a = (vs2h >> 48) & 0x0000ffff;
+                    b = (vs2h >> 32) & 0x0000ffff;
+                    c = (vs2h >> 16) & 0x0000ffff;
+                    d = (vs2h      ) & 0x0000ffff;
+                    e = (vs2l >> 48) & 0x0000ffff;
+                    f = (vs2l >> 32) & 0x0000ffff;
+                    g = (vs2l >> 16) & 0x0000ffff;
+                    h = (vs2l      ) & 0x0000ffff;
+                } else {
+                    a = (vs2l      ) & 0x0000ffff;
+                    b = (vs2l >> 16) & 0x0000ffff;
+                    c = (vs2l >> 32) & 0x0000ffff;
+                    d = (vs2l >> 48) & 0x0000ffff;
+                    e = (vs2h      ) & 0x0000ffff;
+                    f = (vs2h >> 16) & 0x0000ffff;
+                    g = (vs2h >> 32) & 0x0000ffff;
+                    h = (vs2h >> 48) & 0x0000ffff;
+                }
+                /* mull&add vs2 horiz. */
+                vs2 += 8*a + 7*b + 6*c + 5*d + 4*e + 3*f + 2*g + 1*h;
+            } while (k >= SOUL);
+            /* chop vs1 round sum before multiplying by 8 */
+            CHOP(vs1_r);
+            /* add vs1 for this round (8 times) */
+            vs2 += vs1_r * 8;
+            /* CHOP both sums */
+            CHOP(vs2);
+            CHOP(vs1);
+            len += k;
+            k = len < VNMAX ? len : VNMAX;
+            len -= k;
+        } while (likely(k >= SOUL));
+        s1 = vs1;
+        s2 = vs2;
+    }
+
+    /* handle trailer */
+    if (unlikely(k)) do {
+        s1 += *buf++;
+        s2 += s1;
+    } while (--k);
+    /* at this point we should not have so big s1 & s2 */
+    MOD28(s1);
+    MOD28(s2);
+
+    /* return recombined sums */
+    return (s2 << 16) | s1;
+}
+#endif
diff -Naur a/alpha/slhash.c b/alpha/slhash.c
--- a/alpha/slhash.c	1970-01-01 01:00:00.000000000 +0100
+++ b/alpha/slhash.c	2015-08-16 05:57:02.000000000 +0200
@@ -0,0 +1,122 @@
+/* slhash.c -- slide the hash table during fill_window()
+ * Copyright (C) 1995-2010 Jean-loup Gailly and Mark Adler
+ * Copyright (C) 2011 Jan Seiffert
+ * For conditions of distribution and use, see copyright notice in zlib.h
+ */
+
+/* NOTE:
+ * We do not precheck the length or wsize for small values because
+ * we assume a minimum len of 256 (for MEM_LEVEL 1) and a minimum wsize
+ * of 256 for windowBits 8
+ */
+
+/* we use inline asm, so GCC it is */
+#ifdef __GNUC__
+#  define HAVE_SLHASH_VEC
+#  define SOUL (sizeof(unsigned long))
+
+#  if GCC_VERSION_GE(303)
+#    define cmpbge  __builtin_alpha_cmpbge
+#    define zapnot  __builtin_alpha_zapnot
+#  else
+/* ========================================================================= */
+local inline unsigned long cmpbge(unsigned long a, unsigned long b)
+{
+    unsigned long r;
+    asm ("cmpbge	%r1, %2, %0" : "=r" (r) : "rJ" (a), "rI" (b));
+    return r;
+}
+
+/* ========================================================================= */
+local inline unsigned long zapnot(unsigned long a, unsigned long mask)
+{
+    unsigned long r;
+    asm ("zapnot	%r1, %2, %0" : "=r" (r) : "rJ" (a), "rI" (mask));
+    return r;
+}
+#  endif
+
+/* ========================================================================= */
+/*
+ * We do not have saturation, but funky multi compare/mask
+ * instructions, since the first alpha, for character handling.
+ */
+local inline unsigned long psubus(unsigned long a, unsigned long b)
+{
+    unsigned long m = cmpbge(a, b);
+    m  = ((m & 0xaa) >> 1) & m;
+    m |= m << 1;
+    a  = zapnot(a, m);
+    return a - zapnot(b, m);
+}
+
+/* ========================================================================= */
+local inline void update_hoffset(Posf *p, uInt wsize, unsigned n)
+{
+    if (likely(wsize < (1<<16))) {
+        unsigned long vwsize;
+        unsigned int i, f;
+
+        vwsize = (wsize  << 16) | (wsize  & 0x0000ffff);
+        vwsize = (vwsize << 32) | (vwsize & 0xffffffff);
+        /* align */
+        f = (unsigned)ALIGN_DIFF(p, SOUL);
+        if (unlikely(f)) {
+            f /= sizeof(*p);
+            n -= f;
+            if (unlikely(f & 1)) {
+                register unsigned m = *p;
+                *p++ = (Pos)(m >= wsize ? m-wsize : NIL);
+                f--;
+            }
+            if (f >= 2) {
+                unsigned long x = psubus(*(unsigned int *)p, vwsize);
+                *(unsigned int *)p = (unsigned int)x;
+                p += 2;
+                f -= 2;
+            }
+        }
+
+        /* do it */
+        i  = n / (SOUL/sizeof(*p));
+        n %= SOUL/sizeof(*p);
+        if (unlikely(i & 1)) {
+            unsigned long x = psubus(*(unsigned long *)p, vwsize);
+            *(unsigned long *)p = x;
+            p += SOUL/sizeof(*p);
+            i--;
+        }
+        i /= 2;
+        do {
+            unsigned long x1, x2;
+            x1 = ((unsigned long *)p)[0];
+            x2 = ((unsigned long *)p)[1];
+            x1 = psubus(x1, vwsize);
+            x2 = psubus(x2, vwsize);
+            ((unsigned long *)p)[0] = x1;
+            ((unsigned long *)p)[1] = x2;
+            p += 2*(SOUL/sizeof(*p));
+        } while (--i);
+
+        /* handle trailer */
+        if (unlikely(n)) {
+            if (n >= 2) {
+                unsigned long x = psubus(*(unsigned int *)p, vwsize);
+                *(unsigned int *)p = (unsigned int)x;
+                p += 2;
+                n -= 2;
+            }
+            if (unlikely(n)) {
+                register unsigned m = *p;
+                *p++ = (Pos)(m >= wsize ? m-wsize : NIL);
+            }
+        }
+
+    } else {
+        do {
+            register unsigned m = *p;
+            *p++ = (Pos)(m >= wsize ? m-wsize : NIL);
+        } while (--n);
+    }
+}
+#endif
diff -Naur a/arm/adler32.c b/arm/adler32.c
--- a/arm/adler32.c	1970-01-01 01:00:00.000000000 +0100
+++ b/arm/adler32.c	2015-08-16 05:57:02.000000000 +0200
@@ -0,0 +1,614 @@
+/*
+ * adler32.c -- compute the Adler-32 checksum of a data stream
+ *   arm implementation
+ * Copyright (C) 1995-2007 Mark Adler
+ * Copyright (C) 2009-2011 Jan Seiffert
+ * For conditions of distribution and use, see copyright notice in zlib.h
+ */
+
+/* @(#) $Id$ */
+
+#if defined(__ARM_NEON__) && defined(__ARMEL__)
+/*
+ * Big endian NEON qwords are kind of broken.
+ * They are big endian within the dwords, but WRONG
+ * (really??) way round between lo and hi.
+ * Creating some kind of PDP11 middle endian.
+ *
+ * This is madness and unsupportable. For this reason
+ * GCC wants to disable qword endian specific patterns.
+ */
+#  include <arm_neon.h>
+
+#  define SOVUCQ sizeof(uint8x16_t)
+#  define SOVUC sizeof(uint8x8_t)
+/* since we do not have the 64bit psadbw sum, we could still go a little higher (we are at 0xc) */
+#  define VNMAX (8*NMAX)
+#  define HAVE_ADLER32_VEC
+#  define MIN_WORK 32
+
+/* ========================================================================= */
+local inline uint8x16_t neon_simple_alignq(uint8x16_t a, uint8x16_t b, unsigned amount)
+{
+    switch(amount % SOVUCQ)
+    {
+    case  0: return a;
+    case  1: return vextq_u8(a, b,  1);
+    case  2: return vextq_u8(a, b,  2);
+    case  3: return vextq_u8(a, b,  3);
+    case  4: return vextq_u8(a, b,  4);
+    case  5: return vextq_u8(a, b,  5);
+    case  6: return vextq_u8(a, b,  6);
+    case  7: return vextq_u8(a, b,  7);
+    case  8: return vextq_u8(a, b,  8);
+    case  9: return vextq_u8(a, b,  9);
+    case 10: return vextq_u8(a, b, 10);
+    case 11: return vextq_u8(a, b, 11);
+    case 12: return vextq_u8(a, b, 12);
+    case 13: return vextq_u8(a, b, 13);
+    case 14: return vextq_u8(a, b, 14);
+    case 15: return vextq_u8(a, b, 15);
+    }
+    return b;
+}
+
+/* ========================================================================= */
+local inline uint32x4_t vector_chop(uint32x4_t x)
+{
+    uint32x4_t y;
+
+    y = vshlq_n_u32(x, 16);
+    x = vshrq_n_u32(x, 16);
+    y = vshrq_n_u32(y, 16);
+    y = vsubq_u32(y, x);
+    x = vaddq_u32(y, vshlq_n_u32(x, 4));
+    return x;
+}
+
+/* ========================================================================= */
+local noinline uLong adler32_vec(adler, buf, len)
+    uLong adler;
+    const Bytef *buf;
+    uInt len;
+{
+    uint32x4_t v0_32 = (uint32x4_t){0,0,0,0};
+    uint8x16_t    v0 = (uint8x16_t)v0_32;
+    uint8x16_t vord, vord_a;
+    uint32x4_t vs1, vs2;
+    uint32x2_t v_tsum;
+    uint8x16_t in16;
+    uint32_t s1, s2;
+    unsigned k;
+
+    s1 = adler & 0xffff;
+    s2 = (adler >> 16) & 0xffff;
+
+    vord = (uint8x16_t){16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1};
+
+    if (likely(len >= 2*SOVUCQ)) {
+        unsigned f, n;
+
+        /*
+         * Add stuff to achieve alignment
+         */
+        /* align hard down */
+        f = (unsigned) ALIGN_DOWN_DIFF(buf, SOVUCQ);
+        n = SOVUCQ - f;
+        buf = (const unsigned char *)ALIGN_DOWN(buf, SOVUCQ);
+
+        /* add n times s1 to s2 for start round */
+        s2 += s1 * n;
+
+        /* set sums 0 */
+        vs1 = v0_32;
+        vs2 = v0_32;
+        /*
+         * the accumulation of s1 for every round grows very fast
+         * (quadratic?), even if we accumulate in 4 dwords, more
+         * rounds means nonlinear growth.
+         * We already split it out of s2, normaly it would be in
+         * s2 times 16... and even grow faster.
+         * Thanks to this split and vector reduction, we can stay
+         * longer in the loops. But we have to prepare for the worst
+         * (all 0xff), only do 6 times the work.
+         * (we could prop. stay a little longer since we have 4 sums,
+         * not 2 like on x86).
+         */
+        k = len < VNMAX ? (unsigned)len : VNMAX;
+        len -= k;
+        /* insert scalar start somewhere */
+        vs1 = vsetq_lane_u32(s1, vs1, 0);
+        vs2 = vsetq_lane_u32(s2, vs2, 0);
+
+        /* get input data */
+        in16 = *(const uint8x16_t *)buf;
+        /* mask out excess data */
+        in16 = neon_simple_alignq(in16, v0, f);
+        vord_a = neon_simple_alignq(vord, v0, f);
+        /* pairwise add bytes and long, pairwise add word long acc */
+        vs1 = vpadalq_u16(vs1, vpaddlq_u8(in16));
+        /* apply order, add words, pairwise add word long acc */
+        vs2 = vpadalq_u16(vs2,
+                vmlal_u8(
+                    vmull_u8(vget_low_u8(in16), vget_low_u8(vord_a)),
+                    vget_high_u8(in16), vget_high_u8(vord_a)
+                    )
+                );
+
+        buf += SOVUCQ;
+        k -= n;
+
+        if (likely(k >= SOVUCQ)) do {
+            uint32x4_t vs1_r = v0_32;
+            do {
+                uint16x8_t vs2_lo = (uint16x8_t)v0_32, vs2_hi = (uint16x8_t)v0_32;
+                unsigned j;
+
+                j  = (k/16) > 16 ? 16 : k/16;
+                k -= j * 16;
+                do {
+                    /* GCC does not create the most pretty inner loop,
+                     * with extra moves and stupid scheduling, but
+                     * i am not in the mood for inline ASM, keep it
+                     * compatible.
+                     */
+                    /* get input data */
+                    in16 = *(const uint8x16_t *)buf;
+                    buf += SOVUCQ;
+
+                    /* add vs1 for this round */
+                    vs1_r = vaddq_u32(vs1_r, vs1);
+
+                    /* pairwise add bytes and long, pairwise add word long acc */
+                    vs1 = vpadalq_u16(vs1, vpaddlq_u8(in16));
+                    /* apply order, word long and acc */
+                    vs2_lo = vmlal_u8(vs2_lo, vget_low_u8(in16), vget_low_u8(vord));
+                    vs2_hi = vmlal_u8(vs2_hi, vget_high_u8(in16), vget_high_u8(vord));
+                } while(--j);
+                /* pair wise add long and acc */
+                vs2 = vpadalq_u16(vs2, vs2_lo);
+                vs2 = vpadalq_u16(vs2, vs2_hi);
+            } while (k >= SOVUCQ);
+            /* chop vs1 round sum before multiplying by 16 */
+            vs1_r = vector_chop(vs1_r);
+            /* add vs1 for this round (16 times) */
+            /* they have shift right and accummulate, where is shift left and acc?? */
+            vs2 = vaddq_u32(vs2, vshlq_n_u32(vs1_r, 4));
+            /* chop both vectors to something within 16 bit */
+            vs2 = vector_chop(vs2);
+            vs1 = vector_chop(vs1);
+            len += k;
+            k = len < VNMAX ? (unsigned) len : VNMAX;
+            len -= k;
+        } while (likely(k >= SOVUCQ));
+
+        if (likely(k)) {
+            /*
+             * handle trailer
+             */
+            f = SOVUCQ - k;
+            /* add k times vs1 for this trailer */
+            vs2 = vmlaq_u32(vs2, vs1, vdupq_n_u32(k));
+
+            /* get input data */
+            in16 = *(const uint8x16_t *)buf;
+            /* masks out bad data */
+            in16 = neon_simple_alignq(v0, in16, k);
+
+            /* pairwise add bytes and long, pairwise add word long acc */
+            vs1 = vpadalq_u16(vs1, vpaddlq_u8(in16));
+            /* apply order, add words, pairwise add word long acc */
+            vs2 = vpadalq_u16(vs2,
+                    vmlal_u8(
+                        vmull_u8(vget_low_u8(in16), vget_low_u8(vord)),
+                        vget_high_u8(in16), vget_high_u8(vord)
+                        )
+                    );
+
+            buf += k;
+            k -= k;
+        }
+
+        /* add horizontal */
+        v_tsum = vpadd_u32(vget_high_u32(vs1), vget_low_u32(vs1));
+        v_tsum = vpadd_u32(v_tsum, v_tsum);
+        s1 = vget_lane_u32(v_tsum, 0);
+        v_tsum = vpadd_u32(vget_high_u32(vs2), vget_low_u32(vs2));
+        v_tsum = vpadd_u32(v_tsum, v_tsum);
+        s2 = vget_lane_u32(v_tsum, 0);
+    }
+
+    if (unlikely(len)) do {
+        s1 += *buf++;
+        s2 += s1;
+    } while (--len);
+    MOD28(s1);
+    MOD28(s2);
+
+    return (s2 << 16) | s1;
+}
+
+#elif defined(__IWMMXT__)
+#  ifndef __GNUC__
+/* GCC doesn't take it's own intrinsic header and ICEs if forced to */
+#    include <mmintrin.h>
+#  else
+typedef unsigned long long __m64;
+
+// TODO: older gcc may need U constrain instead of y?
+static inline __m64 _mm_setzero_si64(void)
+{
+    __m64 r;
+#  if 0
+    asm ("wzero %0" : "=y" (r));
+#  else
+    r = 0;
+#  endif
+    return r;
+}
+/* there is slli/srli and we want to use it, but it's iWMMXt-2 */
+static inline __m64 _mm_sll_pi32(__m64 a, __m64 c)
+{
+    asm ("wsllw %0, %1, %2" : "=y" (a) : "y" (a), "y" (c));
+    return a;
+}
+static inline __m64 _mm_srl_pi32(__m64 a, __m64 c)
+{
+    asm ("wsrlw %0, %1, %2" : "=y" (a) : "y" (a), "y" (c));
+    return a;
+}
+static inline __m64 _mm_sub_pi32(__m64 a, __m64 b)
+{
+    asm ("wsubw %0, %1, %2" : "=y" (a) : "y" (a), "y" (b));
+    return a;
+}
+static inline __m64 _mm_add_pi16(__m64 a, __m64 b)
+{
+    asm ("waddh %0, %1, %2" : "=y" (a) : "y" (a), "y" (b));
+    return a;
+}
+static inline __m64 _mm_add_pi32(__m64 a, __m64 b)
+{
+    asm ("waddw %0, %1, %2" : "=y" (a) : "y" (a), "y" (b));
+    return a;
+}
+static inline __m64 _mm_sada_pu8(__m64 acc, __m64 a, __m64 b)
+{
+    asm ("wsadb %0, %1, %2" : "=y" (acc) : "y" (a), "y" (b), "0" (acc));
+    return acc;
+}
+static inline __m64 _mm_madd_pu16(__m64 a, __m64 b)
+{
+    asm ("wmaddu %0, %1, %2" : "=y" (a) : "y" (a), "y" (b));
+    return a;
+}
+static inline __m64 _mm_mac_pu16(__m64 acc, __m64 a, __m64 b)
+{
+    asm ("wmacu %0, %1, %2" : "=y" (acc) : "y" (a), "y" (b), "0" (acc));
+    return acc;
+}
+static inline __m64 _mm_unpackel_pu8(__m64 a)
+{
+    asm ("wunpckelub %0, %1" : "=y" (a) : "y" (a));
+    return a;
+}
+static inline __m64 _mm_unpackeh_pu8(__m64 a)
+{
+    asm ("wunpckehub %0, %1" : "=y" (a) : "y" (a));
+    return a;
+}
+static inline __m64 _mm_shuffle_pi16(__m64 a, const int m)
+{
+    asm ("wshufh %0, %1, %2" : "=y" (a) : "y" (a), "i" (m));
+    return a;
+}
+static inline unsigned int _mm_extract_pu32(__m64 a, const int m)
+{
+    unsigned int r;
+    asm ("textrmuw %0, %1, %2" : "=r" (r) : "y" (a), "i" (m));
+    return r;
+}
+static inline __m64 _mm_insert_pi32(__m64 a, unsigned int b, const int m)
+{
+    asm ("tinsrw %0, %1, %2" : "=y" (a) : "r" (b), "i" (m), "0" (a));
+    return a;
+}
+static inline __m64 _mm_align_si64(__m64 a, __m64 b, int c)
+{
+    asm ("walignr%U3 %0, %1, %2" : "=y" (a) : "y" (a), "y" (b), "z" (c));
+    return a;
+}
+static inline __m64 _mm_set_pi16(short a, short b, short c, short d)
+{
+    __m64 r = (unsigned long long)d;
+    r |= ((unsigned long long)c) << 16;
+    r |= ((unsigned long long)b) << 32;
+    r |= ((unsigned long long)a) << 48;
+    return r;
+}
+#  endif
+
+// TODO: we could go over NMAX, since we have split the vs2 sum
+/* but we shuffle vs1_r only every 2056 byte, so we can not go full */
+#  define VNMAX (3*NMAX)
+#  define HAVE_ADLER32_VEC
+#  define MIN_WORK 32
+#  define SOV8 (sizeof(__m64))
+
+/* ========================================================================= */
+local inline __m64 vector_chop(__m64 x)
+{
+    static const __m64 four = 4;
+    static const __m64 sixten = 16;
+    __m64 y = _mm_sll_pi32(x, sixten);
+    x = _mm_srl_pi32(x, sixten);
+    y = _mm_srl_pi32(y, sixten);
+    y = _mm_sub_pi32(y, x);
+    x = _mm_add_pi32(y, _mm_sll_pi32(x, four));
+    return x;
+}
+
+/* ========================================================================= */
+local noinline uLong adler32_vec(adler, buf, len)
+    uLong adler;
+    const Bytef *buf;
+    uInt len;
+{
+    unsigned int s1, s2;
+    unsigned int k;
+
+    s1 = adler & 0xffff;
+    s2 = (adler >> 16) & 0xffff;
+
+    if (likely(len >= 4 * SOV8)) {
+        static const __m64 three = 3;
+        __m64 vs1, vs2;
+        __m64 vzero;
+        __m64 vorder_l, vorder_h;
+        unsigned int f, n;
+
+        vzero = _mm_setzero_si64();
+
+        /* align hard down */
+        f = (unsigned int) ALIGN_DOWN_DIFF(buf, SOV8);
+        buf = (const Bytef *)ALIGN_DOWN(buf, SOV8);
+        n = SOV8 - f;
+
+        /* add n times s1 to s2 for start round */
+        s2 += s1 * n;
+
+        k = len < VNMAX ? len : VNMAX;
+        len -= k;
+
+        /* insert scalar start */
+        vs1 = _mm_insert_pi32(vzero, s1, 0);
+        vs2 = _mm_insert_pi32(vzero, s2, 0);
+
+// TODO: byte order?
+        if (host_is_bigendian()) {
+            vorder_l = _mm_set_pi16(4, 3, 2, 1);
+            vorder_h = _mm_set_pi16(8, 7, 6, 5);
+        } else {
+            vorder_l = _mm_set_pi16(5, 6, 7, 8);
+            vorder_h = _mm_set_pi16(1, 2, 3, 4);
+        }
+
+        {
+            __m64 in = *(const __m64 *)buf;
+
+            /* mask excess info out */
+            if (host_is_bigendian()) {
+                in = _mm_align_si64(vzero, in, n);
+                in = _mm_align_si64(in, vzero, f);
+            } else {
+                in = _mm_align_si64(in, vzero, f);
+                in = _mm_align_si64(vzero, in, n);
+            }
+
+            /* add horizontal and acc */
+            vs1 = _mm_sada_pu8(vs1, in, vzero);
+
+            /* widen bytes to words, apply order and acc */
+            vs2 = _mm_mac_pu16(vs2, _mm_unpackel_pu8(in), vorder_l);
+            vs2 = _mm_mac_pu16(vs2, _mm_unpackeh_pu8(in), vorder_h);
+        }
+
+        buf += SOV8;
+        k -= n;
+
+        do {
+            __m64 vs1_r = vzero;
+
+            do {
+                __m64 vs2_l = vzero, vs2_h = vzero;
+                unsigned int j;
+
+                j  = k >= (257 * SOV8) ? 257 * SOV8 : k;
+                j /= SOV8;
+                k -= j * SOV8;
+                do {
+                    /* get input data */
+                    __m64 in = *(const __m64 *)buf;
+                    buf += SOV8;
+
+                    /* add vs1 for this round */
+                    vs1_r = _mm_add_pi32(vs1_r, vs1);
+
+                    /* add horizontal and acc */
+// TODO: how does wsad really work?
+                    /*
+                     * the Intel iwmmxt 1 & 2 manual says the wsad instruction
+                     * always zeros the upper word (32 in the arm context),
+                     * and then adds all sad into the lower word (again 32
+                     * bit). If the z version is choosen, the lower word is
+                     * also zeroed before, otherwise we get an acc.
+                     *
+                     * Visual studio only knows the sada intrinsic to reflect
+                     * that, but no description, no prototype.
+                     *
+                     * But there is no sada intrinsic in the Intel manual.
+                     * The Intel iwmmxt-1 manual only knows sad & sadz, two
+                     * operands, instead the acc is done with the lvalue
+                     * (which only really works with spec. compiler builtins).
+                     * GCC follows the intel manual (but does gcc manages to
+                     * use the lvalue?).
+                     * To make matters worse the description for the _mm_sad_pu8
+                     * intrinsic says it clears the upper _3_ fields, and only
+                     * acc in the lowest, so only working in 16 Bit.
+                     * So who is wrong?
+                     *
+                     * If this is different between 1 & 2 we are screwed, esp.
+                     * since i can not find a preprocessor define if 1 or 2.
+                     */
+                    vs1 = _mm_sada_pu8(vs1, in, vzero);
+
+                    /* widen bytes to words and acc */
+                    vs2_l = _mm_add_pi16(vs2_l, _mm_unpackel_pu8(in));
+                    vs2_h = _mm_add_pi16(vs2_h, _mm_unpackeh_pu8(in));
+                } while (--j);
+                /* shake and roll vs1_r, so both 32 bit sums get some input */
+                vs1_r = _mm_shuffle_pi16(vs1_r, 0x4e);
+                /* apply order and add to 32 bit */
+                vs2_l = _mm_madd_pu16(vs2_l, vorder_l);
+                vs2_h = _mm_madd_pu16(vs2_h, vorder_h);
+                /* acc */
+                vs2 = _mm_add_pi32(vs2, vs2_l);
+                vs2 = _mm_add_pi32(vs2, vs2_h);
+            } while (k >= SOV8);
+            /* chop vs1 round sum before multiplying by 8 */
+            vs1_r = vector_chop(vs1_r);
+            /* add vs1 for this round (8 times) */
+            vs2 = _mm_add_pi32(vs2, _mm_sll_pi32(vs1_r, three));
+            /* chop both sums to something within 16 bit */
+            vs2 = vector_chop(vs2);
+            vs1 = vector_chop(vs1);
+            len += k;
+            k = len < VNMAX ? len : VNMAX;
+            len -= k;
+        } while (likely(k >= SOV8));
+        len += k;
+        vs1 = _mm_add_pi32(vs1, _mm_shuffle_pi16(vs1, 0x4e));
+        vs2 = _mm_add_pi32(vs2, _mm_shuffle_pi16(vs2, 0x4e));
+        s1 = _mm_extract_pu32(vs1, 0);
+        s2 = _mm_extract_pu32(vs2, 0);
+    }
+
+    if (unlikely(len)) do {
+        s1 += *buf++;
+        s2 += s1;
+    } while (--len);
+    /* at this point we should have not so big s1 & s2 */
+    MOD28(s1);
+    MOD28(s2);
+
+    return (s2 << 16) | s1;
+}
+
+/* inline asm, so only on GCC (or compatible) && ARM v6 or better */
+#elif 0 && defined(__GNUC__) && ( \
+        defined(__thumb2__)  && ( \
+            !defined(__ARM_ARCH_7__) && !defined(__ARM_ARCH_7M__) \
+        ) || ( \
+        !defined(__thumb__) && ( \
+            defined(__ARM_ARCH_6__)   || defined(__ARM_ARCH_6J__)  || \
+            defined(__ARM_ARCH_6T2__) || defined(__ARM_ARCH_6ZK__) || \
+            defined(__ARM_ARCH_7A__)  || defined(__ARM_ARCH_7R__) \
+        )) \
+    )
+/* This code is disabled, since it is not faster, only for reference.
+ * We are at speedup: 0.952830
+ * Again counting instructions is futile, 5 instructions per 4 bytes
+ * against at least 3 per byte (loop overhead excluded) is no win.
+ * And split sums also does not save us.
+ */
+#  define SOU32 (sizeof(unsigned int))
+#  define HAVE_ADLER32_VEC
+#  define MIN_WORK 16
+// TODO: maybe 2*NMAX is possible, but that's very thin
+/* this way we are at 0xda */
+#  define VNMAX (NMAX+((NMAX*9)/10))
+
+/* ========================================================================= */
+local noinline uLong adler32_vec(adler, buf, len)
+    uLong adler;
+    const Bytef *buf;
+    uInt len;
+{
+    unsigned int s1, s2;
+    unsigned int k;
+
+    s1 = adler & 0xffff;
+    s2 = (adler >> 16) & 0xffff;
+
+    k    = ALIGN_DIFF(buf, SOU32);
+    len -= k;
+    if (k) do {
+        s1 += *buf++;
+        s2 += s1;
+    } while (--k);
+
+    if (likely(len >= 4 * SOU32)) {
+        unsigned int vs1 = s1, vs2 = s2;
+        unsigned int order_lo, order_hi;
+
+        if (host_is_bigendian()) {
+            order_lo = 0x00030001;
+            order_hi = 0x00040002;
+        } else {
+            order_lo = 0x00020004;
+            order_hi = 0x00010003;
+        }
+        k = len < VNMAX ? len : VNMAX;
+        len -= k;
+
+        do {
+            unsigned int vs1_r = 0;
+            do {
+                unsigned int j;
+                unsigned int vs2_lo = 0, vs2_hi = 0;
+
+                j  = (k/4) >= 128 ? 128 : (k/4);
+                k -= j * 4;
+                do {
+                    /* get input data */
+                    unsigned int in = *(const unsigned int *)buf;
+                    buf += SOU32;
+                    /* add vs1 for this round */
+                    vs1_r += vs1;
+                    /* add horizontal and acc */
+                    asm ("usada8 %0, %1, %2, %3" : "=r" (vs1) : "r" (in), "r" (0), "r" (vs1));
+                    /* widen bytes to words and acc */
+                    asm ("uxtab16 %0, %1, %2" : "=r" (vs2_lo) : "r" (vs2_lo), "r" (in));
+                    asm ("uxtab16 %0, %1, %2, ror #8" : "=r" (vs2_hi) : "r" (vs2_hi), "r" (in));
+                } while (--j);
+                /* aply order and acc */
+                asm ("smlad %0, %1, %2, %3" : "=r" (vs2) : "r" (vs2_lo) , "r" (order_lo), "r" (vs2));
+                asm ("smlad %0, %1, %2, %3" : "=r" (vs2) : "r" (vs2_hi) , "r" (order_hi), "r" (vs2));
+            } while (k >= SOU32);
+            /* chop vs1 round sum before multiplying by 4 */
+            CHOP(vs1_r);
+            /* add vs1 for this round (4 times) */
+            vs2 += vs1_r * 4;
+            /* chop both sums */
+            CHOP(vs2);
+            CHOP(vs1);
+            len += k;
+            k = len < VNMAX ? len : VNMAX;
+            len -= k;
+        } while (likely(k >= SOU32));
+        len += k;
+        s1 = vs1;
+        s2 = vs2;
+    }
+
+    if (unlikely(len)) do {
+        s1 += *buf++;
+        s2 += s1;
+    } while (--len);
+    /* at this point we should not have so big s1 & s2 */
+    MOD28(s1);
+    MOD28(s2);
+
+    return (s2 << 16) | s1;
+}
+#endif
diff -Naur a/arm/slhash.c b/arm/slhash.c
--- a/arm/slhash.c	1970-01-01 01:00:00.000000000 +0100
+++ b/arm/slhash.c	2015-08-16 05:57:02.000000000 +0200
@@ -0,0 +1,261 @@
+/* slhash.c -- slide the hash table during fill_window()
+ * Copyright (C) 1995-2010 Jean-loup Gailly and Mark Adler
+ * Copyright (C) 2011 Jan Seiffert
+ * For conditions of distribution and use, see copyright notice in zlib.h
+ */
+
+/* NOTE:
+ * We do not precheck the length or wsize for small values because
+ * we assume a minimum len of 256 (for MEM_LEVEL 1) and a minimum wsize
+ * of 256 for windowBits 8
+ */
+
+#if defined(__ARM_NEON__) && defined(__ARMEL__)
+/*
+ * Big endian NEON qwords are kind of broken.
+ * They are big endian within the dwords, but WRONG
+ * (really??) way round between lo and hi.
+ * Creating some kind of PDP11 middle endian.
+ *
+ * This is madness and unsupportable. For this reason
+ * GCC wants to disable qword endian specific patterns.
+ */
+#  include <arm_neon.h>
+
+#  define SOVUSQ sizeof(uint16x8_t)
+#  define SOVUS sizeof(uint16x4_t)
+#  define HAVE_SLHASH_VEC
+
+local void update_hoffset_l(Posf *p, uInt wsize, unsigned n)
+{
+    unsigned int i;
+    uint32x4_t vwsize;
+
+    vwsize = vdupq_n_u32(wsize);
+
+    if (ALIGN_DOWN_DIFF(p, SOVUSQ)) {
+        uint32x4_t in = vmovl_u16(*(uint16x4_t *)p);
+        in  = vqsubq_u32(in, vwsize);
+        *(uint16x4_t *)p = vmovn_u32(in);
+        p += SOVUS/sizeof(*p);
+        n -= SOVUS/sizeof(*p);
+    }
+    i  = n / (SOVUSQ/sizeof(*p));
+    n %= SOVUSQ/sizeof(*p);
+
+    do {
+        uint16x8_t in8 = *(uint16x8_t *)p;
+        uint32x4_t inl = vmovl_u16(vget_low_u16(in8));
+        uint32x4_t inh = vmovl_u16(vget_high_u16(in8));
+        inl = vqsubq_u32(inl, vwsize);
+        inh = vqsubq_u32(inh, vwsize);
+        in8 = vcombine_u16(vmovn_u32(inl), vmovn_u32(inh));
+        *(uint16x8_t *)p = in8;
+        p += SOVUSQ/sizeof(*p);
+    } while (--i);
+
+    if (n >= SOVUS/sizeof(*p)) {
+        uint32x4_t in = vmovl_u16(*(uint16x4_t *)p);
+        in  = vqsubq_u32(in, vwsize);
+        *(uint16x4_t *)p = vmovn_u32(in);
+        p += SOVUS/sizeof(*p);
+        n -= SOVUS/sizeof(*p);
+    }
+    if (unlikely(n)) do {
+        register unsigned m = *p;
+        *p++ = (Pos)(m >= wsize ? m-wsize : NIL);
+    } while (--n);
+}
+
+local void update_hoffset(Posf *p, uInt wsize, unsigned n)
+{
+    unsigned int i;
+    uint16x8_t vwsize;
+
+    i  = ALIGN_DIFF(p, SOVUS)/sizeof(Pos);
+    n -= i;
+    if (unlikely(i)) do {
+        register unsigned m = *p;
+        *p++ = (Pos)(m >= wsize ? m-wsize : NIL);
+    } while (--i);
+
+    if(unlikely(wsize > (1<<16)-1)) {
+        update_hoffset_l(p, wsize, n);
+        return;
+    }
+    vwsize = vdupq_n_u16(wsize);
+
+    if (ALIGN_DOWN_DIFF(p, SOVUSQ)) {
+        uint16x4_t in4 = *(uint16x4_t *)p;
+        in4 = vqsub_u16(in4, vget_low_u16(vwsize));
+        *(uint16x4_t *)p = in4;
+        p += SOVUS/sizeof(*p);
+        n -= SOVUS/sizeof(*p);
+    }
+    i  = n / (SOVUSQ/sizeof(*p));
+    n %= SOVUSQ/sizeof(*p);
+    do {
+        *(uint16x8_t *)p = vqsubq_u16(*(uint16x8_t *)p, vwsize);
+        p += SOVUSQ/sizeof(*p);
+    } while (--i);
+
+    if (n >= SOVUS/sizeof(*p)) {
+        uint16x4_t in4 = *(uint16x4_t *)p;
+        in4 = vqsub_u16(in4, vget_low_u16(vwsize));
+        *(uint16x4_t *)p = in4;
+        p += SOVUS/sizeof(*p);
+        n -= SOVUS/sizeof(*p);
+    }
+    if (unlikely(n)) do {
+        register unsigned m = *p;
+        *p++ = (Pos)(m >= wsize ? m-wsize : NIL);
+    } while (--n);
+}
+#elif defined(__IWMMXT__)
+#  define HAVE_SLHASH_VEC
+#  ifndef __GNUC__
+/* GCC doesn't take it's own intrinsic header and ICEs if forced to */
+#    include <mmintrin.h>
+#  else
+typedef unsigned long long __m64;
+
+local inline __m64 _mm_subs_pu16(__m64 a, __m64 b)
+{
+    __m64 ret;
+    asm ("wsubhus %0, %1, %2" : "=y" (ret) : "y" (a), "y" (b));
+    return ret;
+}
+#  endif
+#  define SOV4 (sizeof(__m64))
+
+/* ========================================================================= */
+local inline void update_hoffset(Posf *p, uInt wsize, unsigned n)
+{
+    if (likely(wsize < (1<<16))) {
+        unsigned int i, f, wst;
+        __m64 vwsize;
+
+        wst = (wsize  << 16) | (wsize  & 0x0000ffff);
+        vwsize = (__m64)(((unsigned long long)wst  << 32) | wst);
+        /* align */
+        f = (unsigned)ALIGN_DIFF(p, SOV4);
+        if (unlikely(f)) {
+            f /= sizeof(*p);
+            n -= f;
+            do {
+                register unsigned m = *p;
+                *p++ = (Pos)(m >= wsize ? m-wsize : NIL);
+            } while (--f);
+        }
+
+        /* do it */
+        i  = n / (SOV4/sizeof(*p));
+        n %= SOV4/sizeof(*p);
+        if (i & 1) {
+            __m64 x = _mm_subs_pu16(*(__m64 *)p, vwsize);
+            *(__m64 *)p = x;
+            p += SOV4/sizeof(*p);
+            i--;
+        }
+        i /= 2;
+        do {
+            __m64 x1, x2;
+            x1 = ((__m64 *)p)[0];
+            x2 = ((__m64 *)p)[1];
+            x1 = _mm_subs_pu16(x1, vwsize);
+            x2 = _mm_subs_pu16(x2, vwsize);
+            ((__m64 *)p)[0] = x1;
+            ((__m64 *)p)[1] = x2;
+            p += 2*(SOV4/sizeof(*p));
+        } while (--i);
+
+        /* handle trailer */
+        if (unlikely(n)) {
+            do {
+                register unsigned m = *p;
+                *p++ = (Pos)(m >= wsize ? m-wsize : NIL);
+            } while (--n);
+        }
+    } else {
+        do {
+            register unsigned m = *p;
+            *p++ = (Pos)(m >= wsize ? m-wsize : NIL);
+        } while (--n);
+    }
+}
+#elif defined(__GNUC__) && ( \
+        defined(__thumb2__)  && ( \
+            !defined(__ARM_ARCH_7__) && !defined(__ARM_ARCH_7M__) \
+        ) || ( \
+        !defined(__thumb__) && ( \
+            defined(__ARM_ARCH_6__)   || defined(__ARM_ARCH_6J__)  || \
+            defined(__ARM_ARCH_6T2__) || defined(__ARM_ARCH_6ZK__) || \
+            defined(__ARM_ARCH_7A__)  || defined(__ARM_ARCH_7R__) \
+        )) \
+    )
+#  define SOU32 (sizeof(unsigned int))
+#  define HAVE_SLHASH_VEC
+
+local noinline void update_hoffset_l(Posf *p, uInt wsize, unsigned n)
+{
+    do {
+        register unsigned m = *p;
+        *p++ = (Pos)(m >= wsize ? m-wsize : NIL);
+    } while (--n);
+}
+
+local void update_hoffset(Posf *p, uInt wsize, unsigned n)
+{
+    unsigned int i, vwsize;
+
+    if(unlikely(wsize > (1<<16)-1)) {
+        update_hoffset_l(p, wsize, n);
+        return;
+    }
+
+    vwsize = wsize | (wsize << 16);
+    if (ALIGN_DOWN_DIFF(p, SOU32)) {
+        register unsigned m = *p;
+        *p++ = (Pos)(m >= wsize ? m-wsize : NIL);
+        n--;
+    }
+
+    i  = n / (SOU32/sizeof(*p));
+    n %= SOU32/sizeof(*p);
+    if (i >= 4) {
+        unsigned int j = i / 4;
+        i %= 4;
+        asm (
+            "1:\n\t"
+            "subs %1, #1\n\t"
+            "ldmia %0, {r4-r7}\n\t"
+            "uqsub16 r4, r4, %2\n\t"
+            "uqsub16 r5, r5, %2\n\t"
+            "uqsub16 r6, r6, %2\n\t"
+            "uqsub16 r7, r7, %2\n\t"
+            "stmia %0!, {r4-r7}\n\t"
+            "bne 1b"
+            : /* 0 */ "=r" (p),
+              /* 1 */ "=r" (j)
+            : /* 2 */ "r" (vwsize),
+              /*  */ "0" (p),
+              /*  */ "1" (j)
+            : "r4", "r5", "r6", "r7"
+        );
+            unsigned int in = *(unsigned int *)p;
+            *(unsigned int *)p = in;
+    }
+    if (i) do {
+        unsigned int in = *(unsigned int *)p;
+        asm ("uqsub16 %0, %1, %2" : "=r" (in) : "r" (in), "r" (vwsize));
+        *(unsigned int *)p = in;
+        p += SOU32/sizeof(*p);
+    } while (--i);
+
+    if (unlikely(n)) {
+        register unsigned m = *p;
+        *p++ = (Pos)(m >= wsize ? m-wsize : NIL);
+    }
+}
+
+#endif
diff -Naur a/bfin/adler32.c b/bfin/adler32.c
--- a/bfin/adler32.c	1970-01-01 01:00:00.000000000 +0100
+++ b/bfin/adler32.c	2015-08-16 05:57:02.000000000 +0200
@@ -0,0 +1,195 @@
+/*
+ * adler32.c -- compute the Adler-32 checksum of a data stream
+ *   blackfin implementation
+ * Copyright (C) 1995-2007 Mark Adler
+ * Copyright (C) 2011 Jan Seiffert
+ * For conditions of distribution and use, see copyright notice in zlib.h
+ */
+
+/* @(#) $Id$ */
+
+/* we use inline asm, so GCC it is */
+#ifdef __GNUC__
+#  define HAVE_ADLER32_VEC
+#  define MIN_WORK 16
+#  define VNMAX (2*NMAX + ((9*NMAX)/10))
+
+/* ========================================================================= */
+local noinline uLong adler32_vec(adler, buf, len)
+    uLong adler;
+    const Bytef *buf;
+    uInt len;
+{
+    struct vord { unsigned long d[4]; };
+    static const struct vord vord_e = {{0x00080001, 0x00060001, 0x00040001, 0x00020001}};
+    static const struct vord vord_o = {{0x00070001, 0x00050001, 0x00030001, 0x00010001}};
+    unsigned long s1, s2, s1s;
+    unsigned long lt0, lt1;
+    unsigned long fo, fe, fex;
+    unsigned long lcount, pt;
+
+    /* split Adler-32 into component sums */
+    s1 = adler & 0xffff;
+    s2 = (adler >> 16) & 0xffff;
+
+    __asm__ __volatile__ (
+        /* setup circular adressing */
+        "I0 = %[lt1];\n\t"
+        "B0 = %[lt1];\n\t"
+        "%[pt] = %[len];\n\t"
+        "I2 = %[vord_e];\n\t"
+        "B2 = I2;\n\t"
+        "%[pt] += 4\n\t"
+        "L2 = %[sorder];\n\t"
+        "I3 = %[vord_o];\n\t"
+        "L0 = %[pt];\n\t"
+        "B3 = I3;\n\t"
+        "L3 = L2;\n\t"
+        /* fetch start data */
+        "DISALGNEXCPT || %[lt0] = [I0 ++ %[m_4]];\n\t"
+        "%[fe] = [I2 ++ %[m_4]];\n\t"
+        "%[fo] = [I3 ++ %[m_4]];\n"
+        /****************************/
+        "1:\n\t"
+        /* put sums into ACC */
+        "A1 = %0;\n\t"
+        "A0 = %1;\n\t"
+        /* prepare len and hw loop counter */
+        "%[lcount] = %[vnmax] (Z);\n\t"
+        "%[pt] = %[len];\n\t"
+        "%[pt] += -1;\n\t"
+        "CC = %[lcount] <= %[pt] (IU);\n\t"
+        "IF !CC %[lcount] = %[pt];\n"
+        "%[lcount] = %[lcount] >> 2;\n\t"
+        "%[lcount] = %[lcount] >> 1;\n\t"
+        "%[pt] = %[lcount] << 2;\n\t"
+        "%[pt] = %[pt] << 1;\n\t"
+        "%[len] -= %[pt];\n\t"
+        "%[pt] = 8;\n\t"
+        "%2 = 0;\n\t" /* s1s = 0 */
+        "LOOP inner_add LC1 = %[lcount];\n\t"
+        /*=========================*/
+        "LOOP_BEGIN inner_add;\n\t"
+        "%2 = %2 + %0;\n\t"
+        "DISALGNEXCPT || %[lt1] = [I0 ++ %[m_4]];\n\t"
+        "(%0, %1) = BYTEUNPACK r1:0;\n\t"
+        "A1 += %h1 * %h[fe], A0 += %h1 * %d[fe] (FU) || %[fex] = [I2 ++ %[m_4]] || %[lt0] = [I3 ++ %[m_4]];\n\t"
+        "A1 += %d1 * %h[fo], A0 += %d1 * %d[fo] (FU);\n\t"
+        "A1 += %h0 * %h[fex], A0 += %h0 * %d[fex] (FU) || %[fe] = [I2 ++ %[m_4]] || %[fo] = [I3 ++ %[m_4]];\n\t"
+        "A1 += %d0 * %h[lt0], A0 += %d0 * %d[lt0] (FU);\n\t"
+        "DISALGNEXCPT || %[lt0] = [I0++%[m_4]];\n\t"
+        "(%0, %1) = BYTEUNPACK r1:0 (R);\n\t"
+        "A1 += %h1 * %h[fe], A0 += %h1 * %d[fe] (FU) || %[fex] = [I2 ++ %[m_4]] || %[lt1] = [I3 ++ %[m_4]];\n\t"
+        "A1 += %d1 * %h[fo], A0 += %d1 * %d[fo] (FU);\n\t"
+        "A1 += %h0 * %h[fex], A0 += %h0 * %d[fex] (FU) || %[fe] = [I2 ++ %[m_4]] || %[fo] = [I3 ++ %[m_4]];\n\t"
+        "%0 = (A1 += %d0 * %h[lt1]), %1 = (A0 += %d0 * %d[lt1]) (FU);\n\t"
+        "LOOP_END inner_add;\n\t"
+        /*=========================*/
+        /* chop s1s */
+        "%[lt1] = %h2 (Z);\n\t"
+        "%2 >>= 16;\n\t"
+        "%[lt1] = %[lt1] - %2;\n\t"
+        "%2 <<= 4;\n\t"
+        "%2 = %2 + %[lt1];\n\t"
+        /* add s1s * 8 to s2 */
+        "%2 <<= 3;\n\t"
+        "%1 = %1 + %2;\n\t"
+        /* chop s2 */
+        "%2 = %h1 (Z);\n\t"
+        "%1 >>= 16;\n\t"
+        "%2 = %2 - %1;\n\t"
+        "%1 <<= 4;\n\t"
+        "%1 = %1 + %2;\n\t"
+        /* chop s1 */
+        "%2 = %h0 (Z);\n\t"
+        "%0 >>= 16;\n\t"
+        "%2 = %2 - %0;\n\t"
+        "%0 <<= 4;\n\t"
+        "%0 = %0 + %2;\n\t"
+        /* more then 8 byte left? */
+        "CC = %[len] <= %[pt] (IU);\n\t"
+        "IF !CC JUMP 1b;\n\t"
+        /****************************/
+        /* complete piece left? */
+        "CC = %[len] <= 3 (IU);\n\t"
+        "IF CC JUMP 3f;\n\t"
+        /* handle complete piece */
+        "%0 <<= 2;\n\t"
+        "%1 = %1 + %0;\n\t"
+        "A0 = %1;\n\t"
+        "%[len] += -4;\n\t"
+        "I2 += %[m_4];\n\t"
+        "I3 += %[m_4];\n\t"
+        "DISALGNEXCPT || %[lt1] = [I0 ++ %[m_4]];\n\t"
+        "(%0, %1) = BYTEUNPACK r1:0 || %[fe] = [I2 ++ %[m_4]] || %[fo] = [I3 ++ %[m_4]];\n\t"
+        "A1 += %h1 * %h[fe], A0 += %h1 * %d[fe] (FU) || %[fex] = [I2 ++ %[m_4]] || %[lt0] = [I3 ++ %[m_4]];\n\t"
+        "A1 += %d1 * %h[fo], A0 += %d1 * %d[fo] (FU);\n\t"
+        "A1 += %h0 * %h[fex], A0 += %h0 * %d[fex] (FU) || %[fe] = [I2 ++ %[m_4]] || %[fo] = [I3 ++ %[m_4]];\n\t"
+        "%0 = (A1 += %d0 * %h[lt0]), %1 = (A0 += %d0 * %d[lt0]) (FU);\n\t"
+        /* check remaining len */
+        "3:\n\t"
+        "CC = %[len] == 0;\n\t"
+        "IF CC JUMP 4f;\n\t"
+        /* handle trailer */
+        "%[pt] = I0;\n\t"
+        "%[pt] += -4;\n\t"
+        "LOOP trailer_add LC1 = %[len];\n\t"
+        /*=========================*/
+        "LOOP_BEGIN trailer_add;\n\t"
+        "%[lt0] = B [%[pt] ++] (Z);\n\t"
+        "%0 = %0 + %[lt0];\n\t"
+        "%1 = %1 + %0;\n\t"
+        "LOOP_END trailer_add;\n\t"
+        "%[len] = 0;\n\t"
+        "4:\n\t"
+        /* chop s1 */
+        /* chop s2 */
+        "%2 = %h1 (Z);\n\t"
+        "%8 = %h0 (Z);\n\t"
+        "%1 >>= 16;\n\t"
+        "%0 >>= 16;\n\t"
+        "%2 = %2 - %1;\n\t"
+        "%8 = %8 - %0;\n\t"
+        "%1 <<= 4;\n\t"
+        "%0 <<= 4;\n\t"
+        "%1 = %1 + %2;\n\t"
+        "%2 = %[base] (Z);\n\t"
+        "%0 = %0 + %8;\n\t"
+        "%9 = %1 - %2;\n\t"
+        "%8 = %0 - %2;\n\t"
+        "CC = %2 <= %1 (IU);\n\t"
+        "IF CC %1 = %9;\n\t"
+        "CC = %2 <= %0 (IU);\n\t"
+        "IF CC %0 = %8\n\t"
+        /* disable circular addressing again */
+        "L0 = %[len];\n\t"
+        "L2 = %[len];\n\t"
+        "L3 = %[len];\n\t"
+        : /* %0  */ "=q7" (s1),
+          /* %1  */ "=q6" (s2),
+          /* %2  */ "=d" (s1s),
+          /* %3  */ [lt0] "=q0" (lt0),
+          /* %4  */ [lt1] "=q1" (lt1),
+          /* %5  */ [len] "=a" (len),
+          /* %6  */ [lcount] "=a" (lcount),
+          /* %7  */ [pt] "=a" (pt),
+          /* %8  */ [fo] "=d" (fo),
+          /* %9  */ [fe] "=d" (fe),
+          /* %10 */ [fex] "=d" (fex)
+        : /*     */ [vnmax] "Kuh" (VNMAX),
+          /*     */ [base] "Kuh" (BASE),
+          /*     */ [m_4] "f" (4),
+          /*     */ [vord_o] "x" (&vord_o),
+          /*     */ [vord_e] "x" (&vord_e),
+          /*     */ [sorder] "i" (sizeof(vord_o)),
+          /*     */ "4" (buf),
+          /*     */ "0" (s1),
+          /*     */ "1" (s2),
+          /*     */ "5" (len)
+        : "CC", "LC1", "LB1", "LT1", "A0", "A1", "I0", "B0", "L0", "I2", "B2", "L2", "I3", "B3", "L3"
+    );
+
+    /* return recombined sums */
+    return (s2 << 16) | s1;
+}
+#endif
diff -Naur a/bfin/slhash.c b/bfin/slhash.c
--- a/bfin/slhash.c	1970-01-01 01:00:00.000000000 +0100
+++ b/bfin/slhash.c	2015-08-16 05:57:02.000000000 +0200
@@ -0,0 +1,171 @@
+/* slhash.c -- slide the hash table during fill_window()
+ * Copyright (C) 1995-2010 Jean-loup Gailly and Mark Adler
+ * Copyright (C) 2011 Jan Seiffert
+ * For conditions of distribution and use, see copyright notice in zlib.h
+ */
+
+/* NOTE:
+ * We do not precheck the length or wsize for small values because
+ * we assume a minimum len of 256 (for MEM_LEVEL 1) and a minimum wsize
+ * of 256 for windowBits 8
+ */
+
+/* we use inline asm, so GCC it is */
+#ifdef __GNUC__
+#  define HAVE_SLHASH_VEC
+#  define SO32 (sizeof(unsigned int))
+
+/* ========================================================================= */
+local inline void update_hoffset(Posf *p, uInt wsize, unsigned n)
+{
+    /*
+     * Cycles mesured with the cycle counter, both hashes, 32k:
+     * orig:   1983162
+     * new:     721636
+     * So this code is 2.7 times faster. If only the memory
+     * subsystem would be faster, then there would be more
+     * speedup possible.
+     * But i don't see it...
+     */
+    if (likely(wsize <= (1<<15))) {
+        unsigned int vwsize, i, t, u, v;
+        Posf *p1;
+
+        /*
+         * We have saturation, but not unsigned...
+         * So we do dirty tricks.
+         * We subtract 32k from the unsigned value, then add the
+         * negated wsize with singed saturation, then we add 32k
+         * again, unsigend.
+         */
+        wsize = (unsigned short)((~wsize)+1);
+        vwsize = wsize | (wsize << 16);
+
+        asm (
+            "CC = BITTST(%0, 1);\n\t"
+#  ifdef __ELF__
+            "IF CC JUMP 3f;\n\t"
+            ".subsection 2\n"
+            "3:\t"
+#  else
+            "IF !CC JUMP 4f (BP);\n\t"
+#  endif
+            "%0 = W[%[i]] (Z);\n\t"
+            "%h0 = %h0 - %h[off];\n\t"
+            "%h0 = %h0 + %h[vwsize] (S);\n\t"
+            "%h0 = %h0 + %h[off];\n\t"
+            "W[%[i]++] = %0;\n\t"
+            "%[n] += -1;\n\t"
+#  ifdef __ELF__
+            "JUMP 4f\n\t"
+            ".previous\n"
+#  endif
+            "4:\n\t"
+            "%0 = %[n];\n\t"
+            "%[p1] = %[i];\n\t"
+            "CC = %[n] == 0;\n\t"
+            "%0 >>= 1;\n\t"
+            "%[i] += 4;\n\t"
+            "%0 += -2;\n\t"
+            "%[p0] = %[i];\n\t"
+            "%0 = ROT %0 by -1;\n\t"
+            "%1 = [%[p1]];\n\t"
+            "%[i] = %0;\n\t"
+            "%0 = [%[p0]];\n\t"
+            "%1 = %1 -|- %[off];\n\t"
+            "%1 = %1 +|+ %[vwsize] (S);\n\t"
+            "%1 = %1 +|+ %[off];\n\t"
+            "LSETUP (1f, 2f) LC1 = %[i];\n"
+            /*=========================*/
+            "1:\t"
+            "%0 = %0 -|- %[off] || [%[p1]++%[m8]] = %1;\n\t"
+            "%0 = %0 +|+ %[vwsize] (S) || %1 = [%[p1]];\n\t"
+            "%0 = %0 +|+ %[off];\n\t"
+            "%1 = %1 -|- %[off] || [%[p0]++%[m8]] = %0;\n\t"
+            "%1 = %1 +|+ %[vwsize] (S) || %0 = [%[p0]];\n\t"
+            "2:\t"
+            "%1 = %1 +|+ %[off];\n\t"
+            /*=========================*/
+            "%0 = %0 -|- %[off] || [%[p1]] = %1;\n\t"
+            "%0 = %0 +|+ %[vwsize] (S);\n\t"
+            "%0 = %0 +|+ %[off];\n\t"
+            "[%[p0]++] = %0;\n\t"
+#  ifdef __ELF__
+            "IF CC JUMP 5f;\n\t"
+            ".subsection 2\n"
+            "5:\t"
+#  else
+            "IF !CC JUMP 6f (BP);\n\t"
+#  endif
+            "%0 = [%[p0]];\n\t"
+            "%0 = %0 -|- %[off];\n\t"
+            "%0 = %0 +|+ %[vwsize] (S);\n\t"
+            "%0 = %0 +|+ %[off];\n"
+            "[%[p0]++] = %0;\n"
+#  ifdef __ELF__
+            "JUMP 6f\n\t"
+            ".previous\n"
+#  endif
+            "6:\n\t"
+            "%0 = %[n];\n\t"
+            "CC = BITTST(%0, 0);\n\t"
+#  ifdef __ELF__
+            "IF CC JUMP 7f;\n\t"
+            ".subsection 2\n"
+            "7:\t"
+#  else
+            "IF !CC JUMP 8f (BP);\n\t"
+#  endif
+            "%[n] = %[p0];\n\t"
+            "%0 = W[%[n]] (Z);\n\t"
+            "%h0 = %h0 - %h[off];\n\t"
+            "%h0 = %h0 + %h[vwsize] (S);\n\t"
+            "%h0 = %h0 + %h[off];\n\t"
+            "W[%[n]++] = %0;\n\t"
+            "%[p0] = %[n]\n\t"
+#  ifdef __ELF__
+            "JUMP 8f\n\t"
+            ".previous\n"
+#  endif
+            "8:"
+            : /*  0 */ "=&d" (t),
+              /*  1 */ "=&d" (u),
+              /*  2 */ [p0] "=b" (p),
+              /*  3 */ [p1] "=b" (p1),
+              /*  4 */ [n] "=a" (v),
+              /*  5 */ [i] "=a" (i),
+              /*  6 */ "=m" (*p)
+            : /*  7 */ [off] "d" (0x80008000),
+              /*  8 */ [vwsize] "d" (vwsize),
+              /*  9 */ [m8] "f" (2*SO32),
+              /*  */ "0" (p),
+              /*  */ "4" (n),
+              /*  */ "5" (p),
+              /*  */ "m" (*p)
+            : "cc"
+        );
+    } else {
+        register unsigned m;
+        /* GCC generates ugly code, so do it by hand */
+        asm (
+            "LSETUP (1f, 2f) LC1 = %5;\n"
+            "1:\t"
+            "%1 = W [%0] (Z);\n\t"
+            "%1 = %1 - %3;\n\t"
+            "CC = AC0_COPY;\n\t"
+            "IF !CC %1 = %4;\n"
+            "2:\t"
+            "W [%0++] = %1;\n\t"
+            : /* 0 */ "=a" (p),
+              /* 1 */ "=&d" (m),
+              /* 2 */ "=m" (*p)
+            : /* 3 */ "d" (wsize),
+              /* 3 */ "d" (0),
+              /* 5 */ "a" (n),
+              /*  */ "0" (p),
+              /*  */ "m" (*p)
+            : "cc"
+        );
+    }
+}
+#endif
diff -Naur a/deflate.c b/deflate.c
--- a/deflate.c	2013-04-29 00:57:10.000000000 +0200
+++ b/deflate.c	2015-08-16 05:57:59.000000000 +0200
@@ -1390,8 +1390,7 @@
 local void fill_window(s)
     deflate_state *s;
 {
-    register unsigned n, m;
-    register Posf *p;
+    register unsigned n;
     unsigned more;    /* Amount of free space at the end of the window. */
     uInt wsize = s->w_size;
 
@@ -1429,24 +1428,7 @@
                later. (Using level 0 permanently is not an optimal usage of
                zlib, so we don't care about this pathological case.)
              */
-            n = s->hash_size;
-            p = &s->head[n];
-            do {
-                m = *--p;
-                *p = (Pos)(m >= wsize ? m-wsize : NIL);
-            } while (--n);
-
-            n = wsize;
-#ifndef FASTEST
-            p = &s->prev[n];
-            do {
-                m = *--p;
-                *p = (Pos)(m >= wsize ? m-wsize : NIL);
-                /* If n is not on any hash chain, prev[n] is garbage but
-                 * its value will never be used.
-                 */
-            } while (--n);
-#endif
+            _sh_slide(s->head, s->prev, wsize, s->hash_size);
             more += wsize;
         }
         if (s->strm->avail_in == 0) break;
diff -Naur a/deflate.h b/deflate.h
--- a/deflate.h	2012-06-02 19:16:43.000000000 +0200
+++ b/deflate.h	2015-08-16 05:57:02.000000000 +0200
@@ -301,6 +301,8 @@
 void ZLIB_INTERNAL _tr_align OF((deflate_state *s));
 void ZLIB_INTERNAL _tr_stored_block OF((deflate_state *s, charf *buf,
                         ulg stored_len, int last));
+        /* in slhash.c */
+void ZLIB_INTERNAL _sh_slide OF((Posf *p, Posf *q, uInt wsize, unsigned n));
 
 #define d_code(dist) \
    ((dist) < 256 ? _dist_code[dist] : _dist_code[256+((dist)>>7)])
diff -Naur a/.gitignore b/.gitignore
--- a/.gitignore	1970-01-01 01:00:00.000000000 +0100
+++ b/.gitignore	2015-08-16 05:57:59.000000000 +0200
@@ -0,0 +1,25 @@
+*.diff
+*.patch
+*.orig
+*.rej
+
+*~
+*.a
+*.lo
+*.o
+*.dylib
+
+*.gcda
+*.gcno
+*.gcov
+
+/example
+/example64
+/examplesh
+/libz.so*
+/minigzip
+/minigzip64
+/minigzipsh
+/zlib.pc
+
+.DS_Store
diff -Naur a/ia64/adler32.c b/ia64/adler32.c
--- a/ia64/adler32.c	1970-01-01 01:00:00.000000000 +0100
+++ b/ia64/adler32.c	2015-08-16 05:57:02.000000000 +0200
@@ -0,0 +1,650 @@
+/*
+ * adler32.c -- compute the Adler-32 checksum of a data stream
+ *   ia64 implementation
+ * Copyright (C) 1995-2007 Mark Adler
+ * Copyright (C) 2009-2011 Jan Seiffert
+ * For conditions of distribution and use, see copyright notice in zlib.h
+ */
+
+/* @(#) $Id$ */
+
+/* inline asm ahead, GCC only */
+#include <limits.h>
+#ifdef __GNUC__
+#  define HAVE_ADLER32_VEC
+#  define MIN_WORK 64
+
+#  define VNMAX (7*NMAX)
+#  define SOULL (sizeof(unsigned long long))
+
+local noinline uLong adler32_vec(adler, buf, len)
+    uLong adler;
+    const Bytef *buf;
+    uInt len;
+{
+    union scale_order { unsigned short x[4][4]; unsigned long long d[4];};
+    local const union scale_order ord_le = {{{8,7,6,5},{4,3,2,1},{16,15,14,13},{12,11,10,9}}};
+// TODO: Big Endian is untested
+    local const union scale_order ord_be = {{{1,2,3,4},{5,6,7,8},{ 9,10,11,12},{13,14,15,16}}};
+    unsigned int s1, s2;
+
+    s1 = adler & 0xffff;
+    s2 = (adler >> 16) & 0xffff;
+
+    if (likely(len >= 4*SOULL)) {
+        const union scale_order *scale_order;
+        unsigned long long vs1, vs2;
+        unsigned long long in8;
+        unsigned int f, n, k;
+
+        if (!host_is_bigendian())
+            scale_order = &ord_le;
+        else
+            scale_order = &ord_be;
+
+        /* align hard down */
+        f = (unsigned int) ALIGN_DOWN_DIFF(buf, SOULL);
+        buf = (const Bytef *)ALIGN_DOWN(buf, SOULL);
+        n = SOULL - f;
+
+        /* add n times s1 to s2 for start round */
+        s2 += s1 * n;
+
+        k = len < VNMAX ? len : VNMAX;
+        len -= k;
+
+        /* insert scalar start somewhere */
+        vs1 = s1;
+        vs2 = s2;
+        {
+            unsigned long long a, b, c, x, y;
+
+            /* get input data */
+            in8 = *(const unsigned long long *)buf;
+            if (f) {
+                if (!host_is_bigendian()) {
+                    in8 >>= f * CHAR_BIT;
+                    in8 <<= f * CHAR_BIT;
+                } else {
+                    in8 <<= f * CHAR_BIT;
+                    in8 >>= f * CHAR_BIT;
+                }
+            }
+
+            asm (
+                /* add 8 byte horizontal and add to old qword */
+                "psad1	%3=r0, %7\n\t"
+                "unpack1.l	%5=r0, %7;;\n\t"
+
+                /* apply order, widen to 32 bit */
+                "add	%0=%0, %3\n\t"
+                "unpack1.h	%6=r0, %7\n\t"
+                "pmpy2.r	%2=%5, %9;;\n\t"
+
+                "add	%1=%1, %2\n\t"
+                "pmpy2.r	%4=%6, %8\n\t"
+                "pmpy2.l	%3=%5, %9;;\n\t"
+
+                "add	%2=%3, %4\n\t"
+                "pmpy2.l	%3=%6, %8;;\n\t"
+                "add	%1=%1, %3\n\t"
+                : /* %0 */ "=&r" (vs1),
+                  /* %1 */ "=&r" (vs2),
+                  /* %2 */ "=&r" (a),
+                  /* %3 */ "=&r" (b),
+                  /* %4 */ "=&r" (c),
+                  /* %5 */ "=&r" (x),
+                  /* %6 */ "=&r" (y)
+                : /* %7 */ "r" (in8),
+                  /* %8 */ "r" (scale_order->d[1]),
+                  /* %9 */ "r" (scale_order->d[0]),
+                  /*  */ "0" (vs1),
+                  /*  */ "1" (vs2)
+            );
+            vs2 += a;
+        }
+
+        buf += SOULL;
+        k -= n;
+
+        if (likely(k >= 2*SOULL)) {
+            register const union scale_order *l_ord asm ("r15");
+            register const Bytef *l_buf asm ("r16");
+            register unsigned int l_k asm ("r17");
+            register unsigned int l_len asm ("r18");
+            register unsigned long long l_vs1 asm ("r19");
+            register unsigned long long l_vs2 asm ("r20");
+            register unsigned long long l_frame asm ("r21");
+            void *br_reg;
+
+            asm (
+                ".equ inner_loop_count, 31*64\n\t" /* 127*16/64 == 31.75 */
+                ".equ buf_ptr_1, %0\n\t"
+                ".equ k, %1\n\t"
+                ".equ len, %2\n\t"
+                ".equ order_ptr, %3\n\t"
+                ".equ vs1, %4\n\t"
+                ".equ vs2, %5\n\t"
+                ".equ vs1_l, vs1\n\t"
+
+                ".equ old_frame_state, r34\n\t"
+                ".equ buf_ptr_2, r35\n\t"
+                ".equ buf_ptr_3, r36\n\t"
+                ".equ buf_ptr_4, r37\n\t"
+                ".equ loop_rem, r38\n\t"
+                ".equ order_hihi, r39\n\t"
+                ".equ order_hilo, r40\n\t"
+                ".equ order_lohi, r41\n\t"
+                ".equ order_lolo, r42\n\t"
+
+                ".equ input8_1, r43\n\t"
+                ".equ input8_2, r44\n\t"
+                ".equ input8_3, r45\n\t"
+                ".equ input8_4, r46\n\t"
+                ".equ input8_5, r47\n\t"
+                ".equ input8_6, r48\n\t"
+                ".equ input8_7, r49\n\t"
+                ".equ input8_8, r50\n\t"
+
+                ".equ sad_1, r51\n\t"
+                ".equ sad_2, r52\n\t"
+                ".equ sad_3, r53\n\t"
+                ".equ sad_4, r54\n\t"
+                ".equ sad_5, r55\n\t"
+                ".equ sad_6, r56\n\t"
+                ".equ sad_7, r57\n\t"
+                ".equ sad_8, r58\n\t"
+
+                ".equ vs1_rsum_l, r59\n\t"
+                ".equ vs1_rsum_h, r60\n\t"
+                ".equ vs1_h, r61\n\t"
+                ".equ vs2_wsum_ll, r62\n\t"
+                ".equ vs2_wsum_lh, r63\n\t"
+                ".equ vs2_wsum_hl, r64\n\t"
+                ".equ vs2_wsum_hh, r65\n\t"
+
+                ".equ unpck_01, r66\n\t"
+                ".equ unpck_02, r67\n\t"
+                ".equ unpck_03, r68\n\t"
+                ".equ unpck_04, r69\n\t"
+                ".equ unpck_05, r70\n\t"
+                ".equ unpck_06, r71\n\t"
+                ".equ unpck_07, r72\n\t"
+                ".equ unpck_08, r73\n\t"
+                ".equ unpck_09, r74\n\t"
+                ".equ unpck_10, r75\n\t"
+                ".equ unpck_11, r76\n\t"
+                ".equ unpck_12, r77\n\t"
+                ".equ unpck_13, r78\n\t"
+                ".equ unpck_14, r79\n\t"
+                ".equ unpck_15, r80\n\t"
+                ".equ unpck_16, r81\n\t"
+
+                "mov	%6 = ar.pfs\n\t"
+                /* force creation of a new register frame */
+                "br.call.sptk.many %7 = 8f\n\t"
+
+                "nop.m 0;;\n\t"
+                "mov	ar.pfs = %6\n\t"
+
+                ".subsection 2\n"
+                "8:\n\t"
+                "alloc	old_frame_state = ar.pfs, 3, 47, 0, 0;;\n\t"
+                "ld8	order_hilo = [order_ptr], 8\n\t"
+                "nop.i 0;;\n\t"
+
+                "ld8	order_hihi = [order_ptr], 8;;\n\t"
+                "ld8	order_lolo = [order_ptr], 8\n\t"
+                "nop.i 0;;\n\t"
+
+                "ld8	order_lohi = [order_ptr], 8\n\t"
+                "mov	vs1_h = r0;;\n\t"
+                "nop.i 0\n\t"
+
+                "nop.m 0\n\t"
+                "mov	vs1_rsum_l = r0\n\t"
+                "mov	vs1_rsum_h = r0\n\t"
+
+                "4:\n\t"
+                "add	buf_ptr_2 = 8, buf_ptr_1\n\t"
+                "add	buf_ptr_3 = 16, buf_ptr_1\n\t"
+                "add	buf_ptr_4 = 24, buf_ptr_1\n\t"
+
+                "ld8	input8_1 = [buf_ptr_1], 32\n\t"
+                "mov	unpck_13 = inner_loop_count\n\t"
+                "mov	vs2_wsum_ll = r0;;\n\t"
+
+                "ld8	input8_2 = [buf_ptr_2], 32\n\t"
+                "cmp4.ltu	p7,p6 = k, unpck_13\n\t"
+                "mov	vs2_wsum_lh = r0\n\t"
+
+                "mov	vs2_wsum_hl = r0;;\n\t"
+                "(p6) mov	unpck_15 = inner_loop_count\n\t"
+                "(p7) mov	unpck_15 = k;;\n\t"
+
+                "mov	unpck_16 = buf_ptr_1\n\t"
+                "and	unpck_14 = -16, unpck_15\n\t"
+                "and	loop_rem = 63, unpck_15;;\n\t"
+
+                "mov	vs2_wsum_hh = r0\n\t"
+                "shr.u	unpck_13 = unpck_15, 6\n\t"
+                "add	unpck_16 = unpck_15, unpck_16;;\n\t"
+
+                "cmp4.eq	p7,p6 = 0, unpck_13\n\t"
+                "sub	k = k, unpck_14\n\t"
+                "add	unpck_16 = -33, unpck_16;;\n\t"
+
+                /* prefetch end of buffer for this iterration,
+                 * to make sure OS sees possible page faults so
+                 * OS can swap page in, because speculative
+                 * loads will not.
+                 */
+                "lfetch.fault	[unpck_16]\n\t"
+                "nop.i	1\n\t"
+                "(p7) br.cond.dpnt.few 5f\n\t"
+
+                "ld8	input8_3 = [buf_ptr_3], 32\n\t"
+                "mov	unpck_01 = r0\n\t"
+                "add	unpck_13 = -1, unpck_13\n\t"
+
+                "ld8	input8_4 = [buf_ptr_4], 32\n\t"
+                "mov	unpck_09 = r0\n\t"
+                "mov	unpck_02 = r0\n\t"
+
+                "mov	unpck_10 = r0\n\t"
+                "mov	unpck_03 = r0\n\t"
+                "mov	unpck_11 = r0\n\t"
+
+                "mov	unpck_04 = r0\n\t"
+                "mov	unpck_12 = r0;;\n\t"
+                "mov.i	ar.lc = unpck_13\n"
+
+                "1:\n\t"
+                "ld8	input8_5 = [buf_ptr_1], 32\n\t"
+                "padd2	unpck_01 = unpck_01, unpck_09\n\t"
+                "psad1	sad_1 = r0, input8_1\n\t"
+
+                "ld8	input8_6 = [buf_ptr_2], 32\n\t"
+                "padd2	unpck_02 = unpck_02, unpck_10\n\t"
+                "psad1	sad_2 = r0, input8_2\n\t"
+
+                "ld8	input8_7 = [buf_ptr_3], 32\n\t"
+                "padd2	unpck_03 = unpck_03, unpck_11\n\t"
+                "psad1	sad_3 = r0, input8_3\n\t"
+
+                "ld8	input8_8 = [buf_ptr_4], 32\n\t"
+                "padd2	unpck_04 = unpck_04, unpck_12\n\t"
+                "psad1	sad_4 = r0, input8_4\n\t"
+
+                "padd4	vs1_rsum_l = vs1_rsum_l, vs1_l\n\t"
+                "padd4	vs1_rsum_h = vs1_rsum_h, vs1_h;;\n\t"
+                "psad1	sad_5 = r0, input8_5\n\t"
+
+                "padd2	vs2_wsum_ll = vs2_wsum_ll, unpck_01\n\t"
+                "padd2	vs2_wsum_lh = vs2_wsum_lh, unpck_02\n\t"
+                "unpack1.l	unpck_05 = r0, input8_3\n\t"
+
+                "padd4	vs1_l = vs1_l, sad_1\n\t"
+                "psad1	sad_6 = r0, input8_6\n\t"
+                "unpack1.h	unpck_06 = r0, input8_3\n\t"
+
+                "padd4	vs1_h = vs1_h, sad_2\n\t"
+                "unpack1.l	unpck_01 = r0, input8_1\n\t"
+                "unpack1.h	unpck_02 = r0, input8_1\n\t"
+
+                "padd2	vs2_wsum_hl = vs2_wsum_hl, unpck_03\n\t"
+                "unpack1.l	unpck_07 = r0, input8_4\n\t"
+                "unpack1.h	unpck_08 = r0, input8_4\n\t"
+
+                "padd2	vs2_wsum_hh = vs2_wsum_hh, unpck_04\n\t"
+                "unpack1.l	unpck_09 = r0, input8_5\n\t"
+                "unpack1.h	unpck_10 = r0, input8_5;;\n\t"
+
+                "padd4	vs1_rsum_l = vs1_rsum_l, vs1_l\n\t"
+                "unpack1.l	unpck_03 = r0, input8_2\n\t"
+                "unpack1.h	unpck_04 = r0, input8_2\n\t"
+
+                "padd4	vs1_rsum_h = vs1_rsum_h, vs1_h\n\t"
+                "unpack1.l	unpck_11 = r0, input8_6\n\t"
+                "unpack1.h	unpck_12 = r0, input8_6\n\t"
+
+                "padd2	unpck_01 = unpck_01, unpck_05\n\t"
+                "unpack1.l	unpck_13 = r0, input8_7;;\n\t"
+                "mux2	vs1_rsum_h = vs1_rsum_h, 0x4e\n\t"
+
+                "padd2	unpck_02 = unpck_02, unpck_06\n\t"
+                "mux2	vs1_rsum_l = vs1_rsum_l, 0x4e\n\t"
+                "unpack1.h	unpck_14 = r0, input8_7\n\t"
+
+                "padd4	vs1_l = vs1_l, sad_3\n\t"
+                "unpack1.h	unpck_16 = r0, input8_8\n\t"
+                "unpack1.l	unpck_15 = r0, input8_8\n\t"
+
+                "padd2	unpck_03 = unpck_03, unpck_07\n\t"
+                "padd4	vs1_h = vs1_h, sad_4\n\t"
+                "psad1	sad_7 = r0, input8_7\n\t"
+
+                "ld8.s	input8_1 = [buf_ptr_1], 32\n\t"
+                "padd2	unpck_04 = unpck_04, unpck_08\n\t"
+                "psad1	sad_8 = r0, input8_8\n\t"
+
+                "ld8.s	input8_2 = [buf_ptr_2], 32\n\t"
+                "padd2	unpck_09 = unpck_09, unpck_13;;\n\t"
+                "padd2	unpck_10 = unpck_10, unpck_14\n\t"
+
+                "ld8.s	input8_3 = [buf_ptr_3], 32\n\t"
+                "padd4	vs1_rsum_l = vs1_rsum_l, vs1_l\n\t"
+                "padd4	vs1_rsum_h = vs1_rsum_h, vs1_h\n\t"
+
+                "ld8.s	input8_4 = [buf_ptr_4], 32\n\t"
+                "padd4	vs1_l = vs1_l, sad_5\n\t"
+                "padd4	vs1_h = vs1_h, sad_6;;\n\t"
+
+                "padd4	vs1_rsum_l = vs1_rsum_l, vs1_l\n\t"
+                "padd4	vs1_rsum_h = vs1_rsum_h, vs1_h\n\t"
+                "padd4	vs1_l = vs1_l, sad_7\n\t"
+
+                "padd4	vs1_h = vs1_h, sad_8\n\t"
+                "padd2	unpck_11 = unpck_11, unpck_15\n\t"
+                "padd2	unpck_12 = unpck_12, unpck_16\n\t"
+
+                "nop.m 0\n\t"
+                "nop.i 0\n\t"
+                "br.cloop.sptk.many 1b\n\t"
+
+                "padd2	unpck_01 = unpck_01, unpck_09\n\t"
+                "padd2	unpck_02 = unpck_02, unpck_10;;\n\t"
+                "padd2	unpck_03 = unpck_03, unpck_11\n\t"
+
+                "padd2	unpck_04 = unpck_04, unpck_12\n\t"
+                "padd2	vs2_wsum_ll = vs2_wsum_ll, unpck_01\n\t"
+                "padd2	vs2_wsum_lh = vs2_wsum_lh, unpck_02;;\n\t"
+
+                "padd2	vs2_wsum_hl = vs2_wsum_hl, unpck_03\n\t"
+                "padd2	vs2_wsum_hh = vs2_wsum_hh, unpck_04\n\t"
+                "nop.i 0\n\t"
+
+                "5:\n\t"
+                "cmp4.gtu	p7,p6 = 16, loop_rem\n\t"
+                "add	buf_ptr_1 = -16, buf_ptr_1\n\t"
+                "add	buf_ptr_2 = -16, buf_ptr_2;;\n\t"
+
+                "nop.m 0\n\t"
+                "nop.i 0\n\t"
+                "(p7) br.cond.dptk.few 2f\n"
+
+                "add	loop_rem = -16, loop_rem;;\n\t"
+                "nop.m 0\n\t"
+                "shr.u	unpck_15 = loop_rem, 4;;\n\t"
+
+                "nop.m 0\n\t"
+                "mov.i ar.lc = unpck_15\n\t"
+                "nop.i 0\n\t"
+
+                "3:\n\t"
+                "padd4	vs1_rsum_l = vs1_rsum_l, vs1_l\n\t"
+                "psad1	sad_1 = r0, input8_1;;\n\t"
+                "psad1	sad_2 = r0, input8_2\n\t"
+
+                "padd4	vs1_rsum_h = vs1_rsum_h, vs1_h\n\t"
+                "unpack1.l	unpck_01 = r0, input8_1;;\n\t"
+                "mux2	vs1_rsum_l = vs1_rsum_l, 0x4e\n\t"
+
+                "padd4 vs1_l = vs1_l, sad_1\n\t"
+                "unpack1.h	unpck_02 = r0, input8_1\n\t"
+                "mux2	vs1_rsum_h = vs1_rsum_h, 0x4e\n\t"
+
+                "padd4 vs1_h = vs1_h, sad_2\n\t"
+                "unpack1.l	unpck_03 = r0, input8_2\n\t"
+                "unpack1.h	unpck_04 = r0, input8_2\n\t"
+
+                "ld8.s input8_1 = [buf_ptr_1], 16\n\t"
+                "padd2	vs2_wsum_ll = vs2_wsum_ll, unpck_01;;\n\t"
+                "padd2	vs2_wsum_lh = vs2_wsum_lh, unpck_02\n\t"
+
+                "ld8.s input8_2 = [buf_ptr_2], 16\n\t"
+                "padd2	vs2_wsum_hl = vs2_wsum_hl, unpck_03\n\t"
+                "padd2	vs2_wsum_hh = vs2_wsum_hh, unpck_04\n\t"
+
+                "nop.m 0\n\t"
+                "nop.i 0\n\t"
+                "br.cloop.sptk.few 3b\n"
+
+                "2:\n\t"
+                "add	buf_ptr_1 = -16, buf_ptr_1\n\t"
+                "pmpy2.r	unpck_01 = vs2_wsum_ll, order_lolo\n\t"
+                "pmpy2.l	unpck_02 = vs2_wsum_ll, order_lolo;;\n\t"
+
+                "add	buf_ptr_2 = -16, buf_ptr_2\n\t"
+                "pmpy2.r	unpck_03 = vs2_wsum_lh, order_lohi\n\t"
+                "pmpy2.l	unpck_04 = vs2_wsum_lh, order_lohi\n\t"
+
+                "padd4	unpck_01 = unpck_01, unpck_02\n\t"
+                "pmpy2.r	unpck_05 = vs2_wsum_hl, order_hilo\n\t"
+                "pmpy2.l	unpck_06 = vs2_wsum_hl, order_hilo;;\n\t"
+
+                "padd4	unpck_03 = unpck_03, unpck_04\n\t"
+                "pmpy2.r	unpck_07 = vs2_wsum_hh, order_hihi\n\t"
+                "pmpy2.l	unpck_08 = vs2_wsum_hh, order_hihi\n\t"
+
+                "padd4	unpck_05 = unpck_05, unpck_06;;\n\t"
+                "padd4	unpck_07 = unpck_07, unpck_08\n\t"
+                "padd4	unpck_01 = unpck_01, unpck_03;;\n\t"
+
+                "padd4	unpck_05 = unpck_05, unpck_07\n\t"
+                "padd4	vs2 = vs2, unpck_01;;\n\t"
+                "padd4	vs2 = vs2, unpck_05\n\t"
+
+                "cmp4.ltu	p7,p6 = 16, k\n\t"
+                "nop.m 0\n\t"
+                "nop.i 0\n\t;;"
+
+                "nop.m 0\n\t"
+                "nop.i 0\n\t"
+                "(p7) br.cond.dptk.few 4b\n\t"
+
+                "padd4	vs1_l = vs1_l, vs1_h\n\t"
+                "mix2.r	unpck_01 = r0, vs1_rsum_l\n\t"
+                "mix2.l	unpck_02 = r0, vs1_rsum_l;;\n\t"
+
+                "mov	vs1_h = r0\n\t"
+                "mix2.r	unpck_03 = r0, vs1_rsum_h\n\t"
+                "mix2.l	unpck_04 = r0, vs1_rsum_h;;\n\t"
+
+                "nop.m 0\n\t"
+                "mix2.r	unpck_05 = r0, vs1_l\n\t"
+                "mix2.l	unpck_06 = r0, vs1_l\n\t"
+
+                "psub4	unpck_01 = unpck_01, unpck_02\n\t"
+                "psub4	unpck_03 = unpck_03, unpck_04\n\t"
+                "shl	unpck_02 = unpck_02, 4;;\n\t"
+
+                "padd4	vs1_rsum_l = unpck_01, unpck_02\n\t"
+                "psub4	unpck_05 = unpck_05, unpck_06\n\t"
+                "shl	unpck_04 = unpck_04, 4;;\n\t"
+
+                "padd4	vs1_rsum_h = unpck_03, unpck_04\n\t"
+                "shl	unpck_06 = unpck_06, 4;;\n\t"
+
+                "padd4	vs1_l = unpck_05, unpck_06\n\t"
+                "padd4	vs1_rsum_l = vs1_rsum_l, vs1_rsum_h;;\n\t"
+                "shl	vs1_rsum_l = vs1_rsum_l, 4;;\n\t"
+
+                "padd4	vs2 = vs2, vs1_rsum_l\n\t"
+                "add	len = len, k;;\n\t"
+                "mix2.r	unpck_01 = r0, vs2\n\t"
+
+                "mov	vs1_rsum_l = r0\n\t"
+                "mov	unpck_15 = %8\n\t"
+                "mix2.l	unpck_02 = r0, vs2;;\n\t"
+
+                "psub4	unpck_01 = unpck_01, unpck_02\n\t"
+                "shl	unpck_02 = unpck_02, 4;;\n\t"
+                "padd4	vs2 = unpck_01, unpck_02\n\t"
+
+                "cmp4.ltu	p7,p6 = len, unpck_15;;\n\t"
+                "(p6) mov k = %8\n\t"
+                "(p7) mov k = len;;\n\t"
+
+                "sub	len = len, k\n\t"
+                "cmp4.ltu	p7,p6 = 16, k\n\t"
+                "mov	vs1_rsum_h = r0;;\n\t"
+
+                "nop.m 0\n\t"
+                "nop.i 0\n\t"
+                "(p7) br.cond.dptk 4b\n\t"
+
+                "nop.m 0\n\t"
+                "mov	ar.pfs = old_frame_state;;\n\t"
+                "nop.i 0\n\t"
+
+                "nop.m 0\n\t"
+                "nop.i 0\n\t"
+                "br.ret.sptk.many %7\n\t"
+
+                "nop.m 22\n"
+                "nop.i 23\n"
+                "nop.i 24\n"
+                /*
+                 * ??? .previous seems to be buggy. It
+                 * removes the last bundle from this
+                 * section and inserts it into the previous...
+                 */
+                ".previous\n\t"
+                : /* %0 */ "=r" (l_buf),
+                  /* %1 */ "=r" (l_k),
+                  /* %2 */ "=r" (l_len),
+                  /* %3 */ "=r" (l_ord),
+                  /* %4 */ "=r" (l_vs1),
+                  /* %5 */ "=r" (l_vs2),
+                  /* %6 */ "=r" (l_frame),
+                  /* %7 */ "=b" (br_reg)
+                : /* %8 */ "i" (VNMAX),
+                  /*  */ "0" (buf),
+                  /*  */ "1" (k),
+                  /*  */ "2" (len),
+                  /*  */ "3" (scale_order),
+                  /*  */ "4" (vs1),
+                  /*  */ "5" (vs2)
+                : "p6", "p7", "ar.lc"
+            );
+            buf = l_buf;
+            k = l_k;
+            len = l_len;
+            vs1 = l_vs1;
+            vs2 = l_vs2;
+        }
+
+        /* complete piece left? */
+        if (k >= SOULL) {
+            unsigned long long a, b, c, x, y;
+
+            in8 = *(const unsigned long long *)buf;
+            asm (
+                "shladd	%1=%0,3,%1\n\t"
+                "psad1	%3=r0, %7\n\t"
+                "unpack1.l	%5=r0, %7;;\n\t"
+
+                "add	%0=%0, %3\n\t"
+                "unpack1.h	%6=r0, %7\n\t"
+                "pmpy2.r	%2=%5, %9;;\n\t"
+
+                "add	%1=%1, %2\n\t"
+                "pmpy2.r	%4=%6, %8\n\t"
+                "pmpy2.l	%3=%5, %9;;\n\t"
+
+                "add	%2=%3, %4\n\t"
+                "pmpy2.l	%3=%6, %8;;\n\t"
+                "add	%1=%1, %3\n\t"
+                : /* %0 */ "=&r" (vs1),
+                  /* %1 */ "=&r" (vs2),
+                  /* %2 */ "=&r" (a),
+                  /* %3 */ "=&r" (b),
+                  /* %4 */ "=&r" (c),
+                  /* %5 */ "=&r" (x),
+                  /* %6 */ "=&r" (y)
+                : /* %7 */ "r" (in8),
+                  /* %8 */ "r" (scale_order->d[1]),
+                  /* %9 */ "r" (scale_order->d[0]),
+                  /*  */ "0" (vs1),
+                  /*  */ "1" (vs2)
+            );
+            vs2 += a;
+            buf += SOULL;
+            k   -= SOULL;
+        }
+
+        if (likely(k)) {
+            unsigned long long a, b, c, x, y;
+            /* get input data */
+            in8 = *(const unsigned long long *)buf;
+            unsigned int n = SOULL - k;
+
+            /* swizzle data in place */
+            if (!host_is_bigendian())
+                in8 <<= n * CHAR_BIT;
+            else
+                in8 >>= n * CHAR_BIT;
+
+            /* add k times vs1 for this trailer */
+            vs2 += vs1 * k;
+
+            asm (
+                /* add 8 byte horizontal and add to old qword */
+                "psad1	%3=r0, %7\n\t"
+                "unpack1.l	%5=r0, %7;;\n\t"
+
+                /* apply order, widen to 32 bit */
+                "add	%0=%0, %3\n\t"
+                "unpack1.h	%6=r0, %7\n\t"
+                "pmpy2.r	%2=%5, %9;;\n\t"
+
+                "add	%1=%1, %2\n\t"
+                "pmpy2.r	%4=%6, %8\n\t"
+                "pmpy2.l	%3=%5, %9;;\n\t"
+
+                "add	%2=%3, %4\n\t"
+                "pmpy2.l	%3=%6, %8;;\n\t"
+                "add	%1=%1, %3\n\t"
+                : /* %0 */ "=&r" (vs1),
+                  /* %1 */ "=&r" (vs2),
+                  /* %2 */ "=&r" (a),
+                  /* %3 */ "=&r" (b),
+                  /* %4 */ "=&r" (c),
+                  /* %5 */ "=&r" (x),
+                  /* %6 */ "=&r" (y)
+                : /* %7 */ "r" (in8),
+                  /* %8 */ "r" (scale_order->d[1]),
+                  /* %9 */ "r" (scale_order->d[0]),
+                  /*  */ "0" (vs1),
+                  /*  */ "1" (vs2)
+            );
+            vs2 += a;
+
+            buf += k;
+            k -= k;
+        }
+
+        /* vs1 is one giant 64 bit sum, but we only calc to 32 bit */
+        s1 = vs1;
+        /* add both vs2 sums */
+        asm (
+            "unpack4.h	%0=r0, %1\n\t"
+            "unpack4.l	%1=r0, %1\n\t"
+            : /* %0 */ "=r" (s2),
+              /* %1 */ "=r" (vs2)
+            : /* %2 */ "1" (vs2)
+        );
+        s2 += vs2;
+        /* modulo again in scalar code */
+    }
+
+    if (unlikely(len)) do {
+        s1 += *buf++;
+        s2 += s1;
+    } while (--len);
+    MOD28(s1);
+    MOD28(s2);
+    return (s2 << 16) | s1;
+}
+#endif
diff -Naur a/ia64/slhash.c b/ia64/slhash.c
--- a/ia64/slhash.c	1970-01-01 01:00:00.000000000 +0100
+++ b/ia64/slhash.c	2015-08-16 05:57:02.000000000 +0200
@@ -0,0 +1,188 @@
+/* slhash.c -- slide the hash table during fill_window()
+ * Copyright (C) 1995-2010 Jean-loup Gailly and Mark Adler
+ * Copyright (C) 2011 Jan Seiffert
+ * For conditions of distribution and use, see copyright notice in zlib.h
+ */
+
+/* NOTE:
+ * We do not precheck the length or wsize for small values because
+ * we assume a minimum len of 256 (for MEM_LEVEL 1) and a minimum wsize
+ * of 256 for windowBits 8
+ */
+
+/* we use a bunch of inline asm, so GCC it is */
+#ifdef __GNUC__
+#  define HAVE_SLHASH_VEC
+#  define SOULL (sizeof(unsigned long long))
+
+#include <stdio.h>
+
+/* ========================================================================= */
+local inline void update_hoffset(Posf *p, uInt wsize, unsigned n)
+{
+    if (likely(wsize < (1<<16))) {
+        unsigned int i, j, f;
+        unsigned long long vwsize;
+        unsigned long long m1, m2;
+
+        vwsize = (wsize << 16) | (wsize  & 0x0000ffff);
+        vwsize = (vwsize << 32) | vwsize;
+
+        /* align */
+        f = (unsigned)ALIGN_DIFF(p, SOULL);
+        if (unlikely(f)) {
+            f /= sizeof(*p);
+            n -= f;
+            if (f & 1) {
+                register unsigned m = *p;
+                *p++ = (Pos)(m >= wsize ? m-wsize : NIL);
+                f--;
+            }
+            if (f >= 2) {
+                unsigned int m = *(unsigned int *)p;
+                asm ("psub2.uuu %0 = %1, %2" : "=r" (m) : "r" (m), "r" (vwsize));
+                *(unsigned int *)p = m;
+                p += 2;
+            }
+        }
+
+        /* do it */
+        i  = n / (SOULL/sizeof(*p));
+        n %= SOULL/sizeof(*p);
+        if (unlikely(i & 1)) {
+                m1 = *(unsigned long long *)p;
+                asm ("psub2.uuu %0 = %1, %2" : "=r" (m1) : "r" (m1), "r" (vwsize));
+                *(unsigned long long *)p = m1;
+                p += SOULL/sizeof(*p);
+                i--;
+        }
+        i /= 2;
+        if (likely(j = i / 4)) {
+            unsigned long long m3, m4, m5, m6, m7, m8;
+            Posf *p1, *p2, *p3, *p4, *p5, *p6, *p7;
+            i -= j * 4;
+            asm (
+                "mov.i ar.lc = %17\n"
+                "1:\n\t"
+                "ld8 %0 = [%8]\n\t"
+                "ld8 %1 = [%9]\n\t"
+                "nop.i 0x1\n\t"
+
+                "ld8 %2 = [%10]\n\t"
+                "ld8 %3 = [%11]\n\t"
+                "nop.i 0x2\n\t"
+
+                "ld8 %4 = [%12]\n\t"
+                "ld8 %5 = [%13]\n\t"
+                "nop.i 0x3\n\t"
+
+                "ld8 %6 = [%14]\n\t"
+                "ld8 %7 = [%15]\n\t"
+                "nop.i 0x4;;\n\t"
+
+                "psub2.uuu %0 = %0, %16\n\t"
+                "psub2.uuu %1 = %1, %16\n\t"
+                "psub2.uuu %2 = %2, %16\n\t"
+
+                "psub2.uuu %3 = %3, %16\n\t"
+                "psub2.uuu %4 = %4, %16\n\t"
+                "psub2.uuu %5 = %5, %16\n\t"
+
+                "psub2.uuu %6 = %6, %16\n\t"
+                "psub2.uuu %7 = %7, %16\n\t"
+                "nop.i 0x5\n\t;;"
+
+                "st8 [%8] = %0, 64\n\t"
+                "st8 [%9] = %1, 64\n\t"
+                "nop.i 0x6\n\t"
+
+                "st8 [%10] = %2, 64\n\t"
+                "st8 [%11] = %3, 64\n\t"
+                "nop.i 0x7\n\t"
+
+                "st8 [%12] = %4, 64\n\t"
+                "st8 [%13] = %5, 64\n\t"
+                "nop.i 0x8\n\t"
+
+                "st8 [%14] = %6, 64\n\t"
+                "st8 [%15] = %7, 64\n\t"
+                "br.cloop.sptk.few 1b;;"
+                : /*  0 */ "=&r" (m1),
+                  /*  1 */ "=&r" (m2),
+                  /*  2 */ "=&r" (m3),
+                  /*  3 */ "=&r" (m4),
+                  /*  4 */ "=&r" (m5),
+                  /*  5 */ "=&r" (m6),
+                  /*  6 */ "=&r" (m7),
+                  /*  7 */ "=&r" (m8),
+                  /*  8 */ "=r" (p),
+                  /*  9 */ "=r" (p1),
+                  /* 10 */ "=r" (p2),
+                  /* 11 */ "=r" (p3),
+                  /* 12 */ "=r" (p4),
+                  /* 13 */ "=r" (p5),
+                  /* 14 */ "=r" (p6),
+                  /* 15 */ "=r" (p7)
+                : /* 16 */ "r" (vwsize),
+                  /* 17 */ "r" (j-1),
+                  /*   */  "8" (p),
+                  /*   */  "9" (p+4),
+                  /*   */ "10" (p+8),
+                  /*   */ "11" (p+12),
+                  /*   */ "12" (p+16),
+                  /*   */ "13" (p+20),
+                  /*   */ "14" (p+24),
+                  /*   */ "15" (p+28)
+                : "ar.lc"
+            );
+        }
+        if (unlikely(i)) {
+            Posf *p1;
+            asm (
+                "mov.i ar.lc = %5\n"
+                "1:\n\t"
+                "ld8 %0 = [%2]\n\t"
+                "ld8 %1 = [%3]\n\t"
+                "nop.i 0x1;;\n\t"
+
+                "psub2.uuu %0 = %0, %4\n\t"
+                "psub2.uuu %1 = %1, %4\n\t"
+                "nop.i 0x2;;\n\t"
+
+                "st8 [%2] = %0, 16\n\t"
+                "st8 [%3] = %1, 16\n\t"
+                "br.cloop.sptk.few 1b;;"
+                : /* 0 */ "=&r" (m1),
+                  /* 1 */ "=&r" (m2),
+                  /* 2 */ "=r" (p),
+                  /* 3 */ "=r" (p1)
+                : /* 4 */ "r" (vwsize),
+                  /* 5 */ "r" (i-1),
+                  /*  */ "2" (p),
+                  /*  */ "3" (p+8)
+                : "ar.lc"
+            );
+        }
+
+        /* handle trailer */
+        if (unlikely(n)) {
+            if (n >= 2) {
+                unsigned int m = *(unsigned int *)p;
+                asm ("psub2.uuu %0 = %1, %2" : "=r" (m) : "r" (m), "r" (vwsize));
+                *(unsigned int *)p = m;
+                p += 2;
+                n -= 2;
+            }
+            if (n & 1) {
+                register unsigned m = *p;
+                *p++ = (Pos)(m >= wsize ? m-wsize : NIL);
+            }
+        }
+    } else {
+        do {
+            register unsigned m = *p;
+            *p++ = (Pos)(m >= wsize ? m-wsize : NIL);
+        } while (--n);
+    }
+}
+#endif
diff -Naur a/INDEX b/INDEX
--- a/INDEX	2012-03-11 18:16:04.000000000 +0100
+++ b/INDEX	2015-08-16 05:57:02.000000000 +0200
@@ -20,16 +20,25 @@
 zlib.pc.cmakein zlib.pc template for cmake
 zlib2ansi       perl script to convert source files for C++ compilation
 
+alpha/          files for the alpha platform
 amiga/          makefiles for Amiga SAS C
+arm/            files for the arm platform
 as400/          makefiles for AS/400
+bfin/           files for the bfin platform
 doc/            documentation for formats and algorithms
+mips/           files for the mips platform
+ia64/           files for the ia64 platform
 msdos/          makefiles for MSDOS
 nintendods/     makefile for Nintendo DS
 old/            makefiles for various architectures and zlib documentation
                 files that have not yet been updated for zlib 1.2.x
+parisc/         files for the parisc platform
+ppc/            files for the ppc platform
 qnx/            makefiles for QNX
+sparc/          files for the sparc platform
 watcom/         makefiles for OpenWatcom
 win32/          makefiles for Windows
+x86/            files for the x86 platform
 
                 zlib public header files (required for library use):
 zconf.h
@@ -55,6 +64,7 @@
 inflate.h
 inftrees.c
 inftrees.h
+slhash.c
 trees.c
 trees.h
 uncompr.c
diff -Naur a/Makefile.in b/Makefile.in
--- a/Makefile.in	2013-04-29 00:57:11.000000000 +0200
+++ b/Makefile.in	2015-08-16 05:57:59.000000000 +0200
@@ -54,11 +54,11 @@
 man3dir = ${mandir}/man3
 pkgconfigdir = ${libdir}/pkgconfig
 
-OBJZ = adler32.o crc32.o deflate.o infback.o inffast.o inflate.o inftrees.o trees.o zutil.o
+OBJZ = adler32.o crc32.o deflate.o slhash.o infback.o inffast.o inflate.o inftrees.o trees.o zutil.o
 OBJG = compress.o uncompr.o gzclose.o gzlib.o gzread.o gzwrite.o
 OBJC = $(OBJZ) $(OBJG)
 
-PIC_OBJZ = adler32.lo crc32.lo deflate.lo infback.lo inffast.lo inflate.lo inftrees.lo trees.lo zutil.lo
+PIC_OBJZ = adler32.lo crc32.lo deflate.lo slhash.lo infback.lo inffast.lo inflate.lo inftrees.lo trees.lo zutil.lo
 PIC_OBJG = compress.lo uncompr.lo gzclose.lo gzlib.lo gzread.lo gzwrite.lo
 PIC_OBJC = $(PIC_OBJZ) $(PIC_OBJG)
 
@@ -267,21 +267,25 @@
 
 # DO NOT DELETE THIS LINE -- make depend depends on it.
 
-adler32.o zutil.o: zutil.h zlib.h zconf.h
+adler32.o: adler32.c ppc/adler32.c arm/adler32.c x86/adler32.c alpha/adler32.c mips/adler32.c bfin/adler32.c ia64/adler32.c sparc/adler32.c zutil.h zlib.h zconf.h
+zutil.o: zutil.h zlib.h zconf.h
 gzclose.o gzlib.o gzread.o gzwrite.o: zlib.h zconf.h gzguts.h
 compress.o example.o minigzip.o uncompr.o: zlib.h zconf.h
 crc32.o: zutil.h zlib.h zconf.h crc32.h
 deflate.o: deflate.h zutil.h zlib.h zconf.h
+slhash.o: slhash.c x86/slhash.c arm/slhash.c ppc/slhash.c bfin/slhash.c alpha/slhash.c mips/slhash.c parisc/slhash.c ia64/slhash.c deflate.h zutil.h zlib.h zconf.h
 infback.o inflate.o: zutil.h zlib.h zconf.h inftrees.h inflate.h inffast.h inffixed.h
 inffast.o: zutil.h zlib.h zconf.h inftrees.h inflate.h inffast.h
 inftrees.o: zutil.h zlib.h zconf.h inftrees.h
 trees.o: deflate.h zutil.h zlib.h zconf.h trees.h
 
-adler32.lo zutil.lo: zutil.h zlib.h zconf.h
+adler32.lo: adler32.c ppc/adler32.c arm/adler32.c x86/adler32.c alpha/adler32.c mips/adler32.c bfin/adler32.c ia64/adler32.c sparc/adler32.c zutil.h zlib.h zconf.h
+zutil.lo: zutil.h zlib.h zconf.h
 gzclose.lo gzlib.lo gzread.lo gzwrite.lo: zlib.h zconf.h gzguts.h
 compress.lo example.lo minigzip.lo uncompr.lo: zlib.h zconf.h
 crc32.lo: zutil.h zlib.h zconf.h crc32.h
 deflate.lo: deflate.h zutil.h zlib.h zconf.h
+slhash.lo: slhash.c x86/slhash.c arm/slhash.c ppc/slhash.c bfin/slhash.c alpha/slhash.c mips/slhash.c parisc/slhash.c ia64/slhash.c deflate.h zutil.h zlib.h zconf.h
 infback.lo inflate.lo: zutil.h zlib.h zconf.h inftrees.h inflate.h inffast.h inffixed.h
 inffast.lo: zutil.h zlib.h zconf.h inftrees.h inflate.h inffast.h
 inftrees.lo: zutil.h zlib.h zconf.h inftrees.h
diff -Naur a/mips/adler32.c b/mips/adler32.c
--- a/mips/adler32.c	1970-01-01 01:00:00.000000000 +0100
+++ b/mips/adler32.c	2015-08-16 05:57:02.000000000 +0200
@@ -0,0 +1,521 @@
+/*
+ * adler32.c -- compute the Adler-32 checksum of a data stream
+ *   mips implementation
+ * Copyright (C) 1995-2004 Mark Adler
+ * Copyright (C) 2009-2011 Jan Seiffert
+ * For conditions of distribution and use, see copyright notice in zlib.h
+ */
+
+/* @(#) $Id$ */
+
+/* we use a bunch of inline asm and GCC vector internals, so GCC it is */
+#ifdef __GNUC__
+#  if _MIPS_SZPTR < 64
+#    define SZPRFX
+#  else
+#    define SZPRFX "d"
+#  endif
+#  include <limits.h>
+#  ifdef __mips_loongson_vector_rev
+#    define HAVE_ADLER32_VEC
+#    define MIN_WORK 64
+
+/*
+ * The loongson vector stuff is basically a variant of MMX,
+ * looks like ST had some licenses for that from their x86 days
+ */
+#    include <loongson.h>
+#    define SOV8 (sizeof(uint8x8_t))
+#    define VNMAX (4*NMAX)
+
+/* GCCs loongson port looks like a quick hack.
+ * It can output some simple vector instruction sequences,
+ * but creates horrible stuff when you really use it.
+ * So more coding by hand...
+ */
+
+/* ========================================================================= */
+local inline uint32x2_t vector_chop(uint32x2_t x)
+{
+    uint32x2_t y;
+
+    y = psllw_u(x, 16);
+    y = psrlw_u(y, 16);
+    x = psrlw_u(x, 16);
+    y = psubw_u(y, x);
+    x = psllw_u(x, 4);
+    x = paddw_u(x, y);
+    return x;
+}
+
+/* ========================================================================= */
+local noinline uLong adler32_vec(adler, buf, len)
+    uLong adler;
+    const Bytef *buf;
+    uInt len;
+{
+    unsigned int s1, s2;
+
+    /* split Adler-32 into component sums */
+    s1 = adler & 0xffff;
+    s2 = (adler >> 16) & 0xffff;
+
+    if(likely(len >= 2*SOV8))
+    {
+        /* Loongsons and their ST MMX foo are little endian */
+        static const int16x4_t vord_lo = {8,7,6,5};
+        static const int16x4_t vord_hi = {4,3,2,1};
+        uint32x2_t vs2, vs1;
+        int16x4_t in_lo, in_hi;
+        uint8x8_t v0 = {0};
+        uint8x8_t in8;
+        unsigned f, n;
+        unsigned k;
+
+        /*
+         * Add stuff to achieve alignment
+         */
+        /* align hard down */
+        f = (unsigned) ALIGN_DOWN_DIFF(buf, SOV8);
+        n = SOV8 - f;
+        buf = (const unsigned char *)ALIGN_DOWN(buf, SOV8);
+
+        /* add n times s1 to s2 for start round */
+        s2 += s1 * n;
+
+        k = len < VNMAX ? (unsigned)len : VNMAX;
+        len -= k;
+
+        /* insert scalar start somewhere */
+        vs1 = (uint32x2_t)(unsigned long long)s1;
+        vs2 = (uint32x2_t)(unsigned long long)s2;
+
+        /* get input data */
+        /* add all byte horizontal and add to old qword */
+        /* apply order, add 4 byte horizontal and add to old dword */
+        asm (
+                "ldc1	%0, %9\n\t"
+                "dsrl	%0, %0, %5\n\t"
+                "dsll	%0, %0, %5\n\t"
+                "biadd	%4, %0\n\t"
+                "punpcklbh	%3, %0, %6\n\t"
+                "paddw	%1, %1, %4\n\t"
+                "punpckhbh	%4, %0, %6\n\t"
+                "pmaddhw	%3, %3, %7\n\t"
+                "pmaddhw	%4, %4, %8\n\t"
+                "paddw	%2, %2, %3\n\t"
+                "paddw	%2, %2, %4\n\t"
+                : /* %0  */ "=&f" (in8),
+                  /* %1  */ "=f" (vs1),
+                  /* %2  */ "=f" (vs2),
+                  /* %3  */ "=&f" (in_lo),
+                  /* %4  */ "=&f" (in_hi)
+                : /* %5  */ "f" (f * CHAR_BIT),
+                  /* %6  */ "f" (v0),
+                  /* %7  */ "f" (vord_lo),
+                  /* %8  */ "f" (vord_hi),
+                  /* %9  */ "m" (*buf),
+                  /* %10 */ "1" (vs1),
+                  /* %11 */ "2" (vs2)
+        );
+        buf += SOV8;
+        k -= n;
+
+        if (likely(k >= SOV8)) do {
+            uint32x2_t vs1_r;
+            uint16x4_t vs2_lo, vs2_hi;
+            int t, j;
+
+            /*
+             * GCC generates horible loop code, so write
+             * the core loop by hand...
+             */
+            __asm__ __volatile__ (
+                    ".set noreorder\n\t"
+                    "b	5f\n\t"
+                    "xor	%3, %3, %3\n"
+                    "2:\n\t"
+                    "xor	%6, %6, %6\n\t"
+                    "sll	%10, %11, 3\n\t"
+                    "xor	%7, %7, %7\n\t"
+                    "subu	%9, %9, %10\n"
+                    "1:\n\t"
+                    "ldc1	%0, (%8)\n\t"
+                    SZPRFX"addiu	%8, %8, 8\n\t"
+                    "addiu	%11, %11, -1\n\t"
+                    "paddw	%3, %1, %3\n\t"
+                    "biadd	%5, %0\n\t"
+                    "punpcklbh	%4, %0, %12\n\t"
+                    "paddw	%1, %5, %1\n\t"
+                    "punpckhbh	%5, %0, %12\n\t"
+                    "paddh	%6, %6, %4\n\t"
+                    "bnez	%11, 1b\n\t"
+                    "paddh	%7, %7, %5\n\t"
+                    /* loop bottom  */
+                    "pshufh	%1, %1, %15\n\t"
+                    "sltiu	%10, %9, 8\n\t"
+                    "pmaddhw	%4, %6, %13\n\t"
+                    "pmaddhw	%5, %7, %14\n\t"
+                    "paddw	%2, %2, %4\n\t"
+                    "bnez	%10, 4f\n\t"
+                    "paddw	%2, %2, %5\n"
+                    "5:\n\t"
+                    "sltiu	%10, %9, 1032\n\t"
+                    "beqz	%10, 2b\n\t"
+                    "li	%11, 128\n\t"
+                    "b	2b\n\t"
+                    "srl	%11, %9, 3\n"
+                    "4:\n\t"
+                    ".set reorder\n\t"
+                    : /* %0  */ "=&f" (in8),
+                      /* %1  */ "=f" (vs1),
+                      /* %2  */ "=f" (vs2),
+                      /* %3  */ "=&f" (vs1_r),
+                      /* %4  */ "=&f" (in_lo),
+                      /* %5  */ "=&f" (in_hi),
+                      /* %6  */ "=&f" (vs2_lo),
+                      /* %7  */ "=&f" (vs2_hi),
+                      /* %8  */ "=d" (buf),
+                      /* %9  */ "=r" (k),
+                      /* %10 */ "=r" (t),
+                      /* %11 */ "=r" (j)
+                    : /* %12 */ "f" (v0),
+                      /* %13 */ "f" (vord_lo),
+                      /* %14 */ "f" (vord_hi),
+                      /* %15 */ "f" (0x4e),
+                      /* %15 */ "1" (vs1),
+                      /* %16 */ "2" (vs2),
+                      /* %17 */ "8" (buf),
+                      /* %18 */ "9" (k)
+            );
+            /* And the rest of the generated code also looks awful.
+             * Looks like GCC is missing instruction patterns for:
+             * - 64 bit shifts in loongson copro regs
+             * - logic in loongson copro regs
+             * and to make things much worse, GCC seems to be missing
+             * a loongson copro register <-> copro register move
+             * pattern (for example using an or instruction), instead
+             * GCC always moves over the GPR.
+             *
+             * But still, let the compiler handle this, we get some
+             * extra moves between copro regs and GPR, but save us
+             * a lot of work.
+             * And maybe some day some one will fix this...
+             */
+
+            /* chop vs1 round sum before multiplying by 8 */
+            vs1_r = vector_chop(vs1_r);
+            /* add all vs1 for 8 times */
+            vs2 = paddw_u(psllw_u(vs1_r, 3), vs2);
+            /* chop the vectors to something in the range of BASE */
+            vs2 = vector_chop(vs2);
+            vs1 = vector_chop(vs1);
+            len += k;
+            k = len < VNMAX ? (unsigned)len : VNMAX;
+            len -= k;
+        } while (likely(k >= SOV8));
+
+        if (likely(k)) {
+            uint32x2_t vk;
+            /* handle trailer */
+            f = SOV8 - k;
+
+            vk = (uint32x2_t)(unsigned long long)k;
+
+            /* get input data */
+            /* add all byte horizontal and add to old qword */
+            /* add k times vs1 for this trailer */
+            /* apply order, add 4 byte horizontal and add to old dword */
+            __asm__ (
+                    "ldc1	%0, %11\n\t"
+                    "pmuluw	%3, %1, %6\n\t"
+                    "pshufh	%1, %1, %10\n\t"
+                    "paddw	%2, %2, %3\n\t"
+                    "pmuluw	%3, %1, %6\n\t"
+                    "dsll	%0, %0, %5\n\t"
+                    "biadd	%4, %0\n\t"
+                    "paddw	%2, %2, %3\n\t"
+                    "punpcklbh	%3, %0, %7\n\t"
+                    "paddw	%1, %1, %4\n\t"
+                    "punpckhbh	%4, %0, %7\n\t"
+                    "pmaddhw	%3, %3, %8\n\t"
+                    "pmaddhw	%4, %4, %9\n\t"
+                    "paddw	%2, %2, %3\n\t"
+                    "paddw	%2, %2, %4\n\t"
+                    : /* %0  */ "=&f" (in8),
+                      /* %1  */ "=f" (vs1),
+                      /* %2  */ "=f" (vs2),
+                      /* %3  */ "=&f" (in_lo),
+                      /* %4  */ "=&f" (in_hi)
+                    : /* %5  */ "f" (f * CHAR_BIT),
+                      /* %6  */ "f" (vk),
+                      /* %7  */ "f" (v0),
+                      /* %8  */ "f" (vord_lo),
+                      /* %9  */ "f" (vord_hi),
+                      /* %10 */ "f" (0x4e),
+                      /* %11 */ "m" (*buf),
+                      /* %12 */ "1" (vs1),
+                      /* %13 */ "2" (vs2)
+            );
+
+            buf += k;
+            k -= k;
+        }
+
+        /* add horizontal */
+        vs1 = paddw_u(vs1, (uint32x2_t)pshufh_u((uint16x4_t)v0, (uint16x4_t)vs1, 0x4E));
+        vs2 = paddw_u(vs2, (uint32x2_t)pshufh_u((uint16x4_t)v0, (uint16x4_t)vs2, 0x4E));
+        /* shake and roll */
+        s1 = (unsigned int)(unsigned long long)vs1;
+        s2 = (unsigned int)(unsigned long long)vs2;
+        /* modulo again in scalar code */
+    }
+
+    /* handle a possible trailer */
+    if (unlikely(len)) do {
+        s1 += *buf++;
+        s2 += s1;
+    } while (--len);
+    MOD28(s1);
+    MOD28(s2);
+
+    /* return recombined sums */
+    return (s2 << 16) | s1;
+}
+
+#  elif defined(__mips_dsp)
+#    define HAVE_ADLER32_VEC
+#    define MIN_WORK 16
+#    define VNMAX ((5*NMAX)/2)
+
+typedef signed char v4i8 __attribute__((vector_size(4)));
+#    define SOV4 (sizeof(v4i8))
+
+/* ========================================================================= */
+local noinline uLong adler32_vec(adler, buf, len)
+    uLong adler;
+    const Bytef *buf;
+    uInt len;
+{
+    unsigned int s1, s2;
+
+    /* split Adler-32 into component sums */
+    s1 = adler & 0xffff;
+    s2 = (adler >> 16) & 0xffff;
+
+    if (likely(len >= 2*SOV4)) {
+        static const v4i8 v1 = {1,1,1,1};
+        v4i8 vord_lo, vord_hi, vord;
+        /*
+         * yes, our sums should stay within 32 bit, that accs are
+         * 64 bit is to no (real) use to us.
+         */
+        unsigned int vs1, vs2, vs1h, vs2h;
+        v4i8 in4;
+        unsigned f, n;
+        unsigned k;
+        unsigned int srl;
+
+        if(host_is_bigendian()) {
+            vord_lo = (v4i8){8,7,6,5};
+            vord_hi = (v4i8){4,3,2,1};
+        } else {
+            vord_lo = (v4i8){5,6,7,8};
+            vord_hi = (v4i8){1,2,3,4};
+        }
+        vord = vord_hi;
+        /*
+         * Add stuff to achieve alignment
+         */
+        /* align hard down */
+        f = (unsigned) ALIGN_DOWN_DIFF(buf, SOV4);
+        n = SOV4 - f;
+        buf = (const unsigned char *)ALIGN_DOWN(buf, SOV4);
+
+        /* add n times s1 to s2 for start round */
+        s2 += s1 * n;
+
+        k = len < VNMAX ? len : VNMAX;
+        len -= k;
+
+        /* insert scalar start somewhere */
+        vs1 = s1;
+        vs2 = s2;
+        vs1h = 0;
+        vs2h = 0;
+
+        /* get input data */
+        srl = *(const unsigned int *)buf;
+        if(host_is_bigendian()) {
+            srl <<= f * CHAR_BIT;
+            srl >>= f * CHAR_BIT;
+        } else {
+            srl >>= f * CHAR_BIT;
+            srl <<= f * CHAR_BIT;
+        }
+        in4 = (v4i8)srl;
+
+        /* add all byte horizontal and add to old dword */
+        vs1 = __builtin_mips_dpau_h_qbl(vs1, in4, v1);
+        vs1 = __builtin_mips_dpau_h_qbr(vs1, in4, v1);
+
+        /* apply order, add 4 byte horizontal and add to old dword */
+        vs2 = __builtin_mips_dpau_h_qbl(vs2, in4, vord);
+        vs2 = __builtin_mips_dpau_h_qbr(vs2, in4, vord);
+
+        buf += SOV4;
+        k -= n;
+
+        if (likely(k >= 2*SOV4)) do {
+            unsigned int vs1_r, vs1h_r, t1, t2;
+            v4i8 in4h;
+
+            /*
+             * gcc and special regs...
+             * Since the mips acc are special, gcc fears to treat.
+             * Esp. gcc thinks it's cool to have the result ready in
+             * the gpr at the end of this main loop, so he hoist a
+             * move out of the acc and back in into the acc into the
+             * loop. m(
+             *
+             * So do it by hand, unrolled two times...
+             */
+            __asm__ (
+                    ".set noreorder\n\t"
+                    "mflo	%10, %q0\n\t"
+                    "xor	%6, %6, %6\n\t"
+                    "xor	%7, %7, %7\n"
+                    "1:\n\t"
+                    "lw	%4,(%9)\n\t"
+                    "lw	%5,4(%9)\n\t"
+                    "addu	%6, %6, %10\n\t"
+                    "dpau.h.qbl	%q0, %4, %14\n\t"
+                    "addiu	%8, %8, -8\n\t"
+                    "mflo	%10, %q1\n\t"
+                    "dpau.h.qbr	%q0, %4, %14\n\t"
+                    "dpau.h.qbl	%q1, %5, %14\n\t"
+                    "sltu	%11, %8, 8\n\t"
+                    "dpau.h.qbl	%q3, %4, %13\n\t"
+                    "addu	%7, %7 ,%10\n\t"
+                    "dpau.h.qbl	%q2, %5, %12\n\t"
+                    SZPRFX"addiu	%9, %9, 8\n\t"
+                    "mflo	%10, %q0\n\t"
+                    "dpau.h.qbr	%q1, %5, %14\n\t"
+                    "dpau.h.qbr	%q3, %4, %13\n\t"
+                    "beqz	%11, 1b\n\t"
+                    "dpau.h.qbr	%q2, %5, %12\n\t"
+                    ".set reorder"
+                    : /* %0  */ "=a" (vs1),
+                      /* %1  */ "=A" (vs1h),
+                      /* %2  */ "=A" (vs2),
+                      /* %3  */ "=A" (vs2h),
+                      /* %4  */ "=&r" (in4),
+                      /* %5  */ "=&r" (in4h),
+                      /* %6  */ "=&r" (vs1_r),
+                      /* %7  */ "=&r" (vs1h_r),
+                      /* %8  */ "=r" (k),
+                      /* %9  */ "=d" (buf),
+                      /* %10 */ "=r" (t1),
+                      /* %11 */ "=r" (t2)
+                    : /* %12 */ "r" (vord_lo),
+                      /* %13 */ "r" (vord_hi),
+                      /* %14 */ "r" (v1),
+                      /* %  */ "0" (vs1),
+                      /* %  */ "1" (vs1h),
+                      /* %  */ "2" (vs2),
+                      /* %  */ "3" (vs2h),
+                      /* %  */ "8" (k),
+                      /* %  */ "9" (buf)
+            );
+
+            /* chop vs1 round sum before multiplying by 8 */
+            CHOP(vs1_r);
+            CHOP(vs1h_r);
+            /* add all vs1 for 8 times */
+            vs2  += vs1_r  * 8;
+            vs2h += vs1h_r * 8;
+            /* chop the vectors to something in the range of BASE */
+            CHOP(vs2);
+            CHOP(vs2h);
+            CHOP(vs1);
+            CHOP(vs1h);
+            len += k;
+            k = len < VNMAX ? len : VNMAX;
+            len -= k;
+        } while (likely(k >= 2*SOV4));
+        vs1 += vs1h;
+        vs2 += vs2h;
+        /* a complete piece left? */
+        if (likely(k >= SOV4)) {
+            /* get input data */
+            in4 = *(const v4i8 *)buf;
+            vs2 += vs1 * 4;
+
+            /* add all byte horizontal and add to old dword */
+            vs1 = __builtin_mips_dpau_h_qbl(vs1, in4, v1);
+            vs1 = __builtin_mips_dpau_h_qbr(vs1, in4, v1);
+
+            /* apply order, add 4 byte horizontal and add to old dword */
+            vs2 = __builtin_mips_dpau_h_qbl(vs2, in4, vord);
+            vs2 = __builtin_mips_dpau_h_qbr(vs2, in4, vord);
+
+            k -= SOV4;
+            buf += SOV4;
+        }
+
+        if (likely(k)) {
+            unsigned int vk;
+
+            /*
+             * handle trailer
+             */
+            f = SOV4 - k;
+
+            /* add k times vs1 for this trailer */
+            vk = vs1 * k;
+
+            /* get input data */
+            srl = *(const unsigned int *)buf;
+            if(host_is_bigendian()) {
+                srl >>= f * CHAR_BIT;
+                srl <<= f * CHAR_BIT;
+            } else {
+                srl <<= f * CHAR_BIT;
+                srl >>= f * CHAR_BIT;
+            }
+            in4 = (v4i8)srl;
+
+            /* add all byte horizontal and add to old dword */
+            vs1 = __builtin_mips_dpau_h_qbl(vs1, in4, v1);
+            vs1 = __builtin_mips_dpau_h_qbr(vs1, in4, v1);
+
+            /* apply order, add 4 byte horizontal and add to old dword */
+            vs2 = __builtin_mips_dpau_h_qbl(vs2, in4, vord);
+            vs2 = __builtin_mips_dpau_h_qbr(vs2, in4, vord);
+
+            vs2 += vk;
+
+            buf += k;
+            k -= k;
+        }
+
+        /* shake and roll */
+        s1 = (unsigned int)vs1;
+        s2 = (unsigned int)vs2;
+        /* modulo again in scalar code */
+    }
+
+    /* handle a possible trailer */
+    if (unlikely(len)) do {
+        s1 += *buf++;
+        s2 += s1;
+    } while (--len);
+    /* s2 should be small here */
+    MOD28(s1);
+    MOD28(s2);
+
+    /* return recombined sums */
+    return (s2 << 16) | s1;
+}
+#  endif
+#endif
diff -Naur a/mips/slhash.c b/mips/slhash.c
--- a/mips/slhash.c	1970-01-01 01:00:00.000000000 +0100
+++ b/mips/slhash.c	2015-08-16 05:57:02.000000000 +0200
@@ -0,0 +1,130 @@
+/* slhash.c -- slide the hash table during fill_window()
+ * Copyright (C) 1995-2010 Jean-loup Gailly and Mark Adler
+ * Copyright (C) 2011 Jan Seiffert
+ * For conditions of distribution and use, see copyright notice in zlib.h
+ */
+
+/* NOTE:
+ * We do not precheck the length or wsize for small values because
+ * we assume a minimum len of 256 (for MEM_LEVEL 1) and a minimum wsize
+ * of 256 for windowBits 8
+ */
+
+/* we use a bunch of inline asm and GCC vector internals, so GCC it is */
+#ifdef __GNUC__
+#  ifdef __mips_loongson_vector_rev
+#    define HAVE_SLHASH_VEC
+#    include <loongson.h>
+#    define SOV4 (sizeof(uint16x4_t))
+
+/* ========================================================================= */
+local inline void update_hoffset(Posf *p, uInt wsize, unsigned n)
+{
+    if (likely(wsize < (1<<16))) {
+        unsigned int i, f, wst;
+        uint16x4_t vwsize;
+
+        wst = (wsize  << 16) | (wsize  & 0x0000ffff);
+        vwsize = (uint16x4_t)(((unsigned long long)wst  << 32) | wst);
+        /* align */
+        f = (unsigned)ALIGN_DIFF(p, SOV4);
+        if (unlikely(f)) {
+            f /= sizeof(*p);
+            n -= f;
+            do {
+                register unsigned m = *p;
+                *p++ = (Pos)(m >= wsize ? m-wsize : NIL);
+            } while (--f);
+        }
+
+        /* do it */
+        i  = n / (SOV4/sizeof(*p));
+        n %= SOV4/sizeof(*p);
+        if (i & 1) {
+            uint16x4_t x = psubush(*(uint16x4_t *)p, vwsize);
+            *(uint16x4_t *)p = x;
+            p += SOV4/sizeof(*p);
+            i--;
+        }
+        i /= 2;
+        do {
+            uint16x4_t x1, x2;
+            x1 = ((uint16x4_t *)p)[0];
+            x2 = ((uint16x4_t *)p)[1];
+            x1 = psubush(x1, vwsize);
+            x2 = psubush(x2, vwsize);
+            ((uint16x4_t *)p)[0] = x1;
+            ((uint16x4_t *)p)[1] = x2;
+            p += 2*(SOV4/sizeof(*p));
+        } while (--i);
+
+        /* handle trailer */
+        if (unlikely(n)) {
+            do {
+                register unsigned m = *p;
+                *p++ = (Pos)(m >= wsize ? m-wsize : NIL);
+            } while (--n);
+        }
+    } else {
+        do {
+            register unsigned m = *p;
+            *p++ = (Pos)(m >= wsize ? m-wsize : NIL);
+        } while (--n);
+    }
+}
+#  elif defined(__mips_dspr2)
+#    define HAVE_SLHASH_VEC
+typedef short v2u16 __attribute__((vector_size(4)));
+#    define SOV2 (sizeof(v2u16))
+
+/* ========================================================================= */
+local inline void update_hoffset(Posf *p, uInt wsize, unsigned n)
+{
+    if (likely(wsize < (1<<16))) {
+        unsigned int i, f;
+        v2u16 vwsize;
+
+        vwsize = (v2u16)((wsize  << 16) | (wsize  & 0x0000ffff));
+        /* align */
+        f = (unsigned)ALIGN_DIFF(p, SOV2);
+        if (unlikely(f)) {
+            register unsigned m = *p;
+            *p++ = (Pos)(m >= wsize ? m-wsize : NIL);
+            n--;
+        }
+
+        /* do it */
+        i  = n / (SOV2/sizeof(*p));
+        n %= SOV2/sizeof(*p);
+        if (unlikely(i & 1)) {
+            v2u16 x = __builtin_mips_subu_s_ph(*(v2u16 *)p, vwsize);
+            *(v2u16 *)p = x;
+            p += SOV2/sizeof(*p);
+            i--;
+        }
+        i /= 2;
+        do {
+            v2u16 x1, x2;
+            x1 = ((v2u16 *)p)[0];
+            x2 = ((v2u16 *)p)[1];
+            x1 = __builtin_mips_subu_s_ph(x1, vwsize);
+            x2 = __builtin_mips_subu_s_ph(x2, vwsize);
+            ((v2u16 *)p)[0] = x1;
+            ((v2u16 *)p)[1] = x2;
+            p += 2*(SOV2/sizeof(*p));
+        } while (--i);
+
+        /* handle trailer */
+        if (unlikely(n)) {
+            register unsigned m = *p;
+            *p++ = (Pos)(m >= wsize ? m-wsize : NIL);
+        }
+    } else {
+        do {
+            register unsigned m = *p;
+            *p++ = (Pos)(m >= wsize ? m-wsize : NIL);
+        } while (--n);
+    }
+}
+#  endif
+#endif
diff -Naur a/parisc/slhash.c b/parisc/slhash.c
--- a/parisc/slhash.c	1970-01-01 01:00:00.000000000 +0100
+++ b/parisc/slhash.c	2015-08-16 05:57:02.000000000 +0200
@@ -0,0 +1,114 @@
+/* slhash.c -- slide the hash table during fill_window()
+ * Copyright (C) 1995-2010 Jean-loup Gailly and Mark Adler
+ * Copyright (C) 2011 Jan Seiffert
+ * For conditions of distribution and use, see copyright notice in zlib.h
+ */
+
+/* NOTE:
+ * We do not precheck the length or wsize for small values because
+ * we assume a minimum len of 256 (for MEM_LEVEL 1) and a minimum wsize
+ * of 256 for windowBits 8
+ */
+
+/* we use a bunch of inline asm, so GCC it is */
+#ifdef __GNUC__
+/*
+ * the 7100LC && 7300LC are parisc 1.1, but have MAX-1, which already has the
+ * hadd, but there is no preprocessor define to detect them.
+ */
+#  if defined(_PA_RISC2_0) || defined(HAVE_PA7x00LC)
+#    define HAVE_SLHASH_VEC
+#    define SOST (sizeof(size_t))
+
+/* ========================================================================= */
+local inline void update_hoffset(Posf *p, uInt wsize, unsigned n)
+{
+    if (likely(wsize <= (1<<15))) {
+        unsigned int i, f;
+        size_t vwsize;
+        size_t m1, m2;
+
+        /*
+         * We have unsigned saturation, but one operand to the
+         * instruction is always signed m(
+         * So we add the negated wsize...
+         */
+        vwsize = (unsigned short)((~wsize)+1);
+        vwsize = (vwsize  << 16) | (vwsize  & 0x0000ffff);
+        /*
+         * sigh...
+         * PARISC 2.0 is _always_ 64 bit. So the register are 64Bit
+         * and the instructions work on 64Bit, ldd/std works, even
+         * in a 32Bit executable.
+         * The sad part: Unfortunatly 32Bit processes do not seem
+         * to get their upper register half saved on context switch.
+         * So restrict the 64Bit-at-once goodness to hppa64.
+         * Without this braindamage, this could bring a nice 16.5%
+         * speedup even to 32Bit processes.
+         * Now it's just 12.7% for 32 Bit processes.
+         */
+        if (SOST > 4) /* silence warning about shift larger then type size */
+            vwsize = ((vwsize << 16) << 16) | (vwsize  & 0xffffffff);
+
+        /* align */
+        f = (unsigned)ALIGN_DIFF(p, SOST);
+        if (unlikely(f)) {
+            f /= sizeof(*p);
+            n -= f;
+            if (f & 1) {
+                register unsigned m = *p;
+                *p++ = (Pos)(m >= wsize ? m-wsize : NIL);
+                f--;
+            }
+            if (SOST > 4 && f >= 2) {
+                unsigned int m = *(unsigned int *)p;
+                asm ("hadd,us %1, %2, %0" : "=r" (m) : "r" (m), "r" (vwsize));
+                *(unsigned int *)p = m;
+                p += 2;
+            }
+        }
+
+        /* do it */
+        i  = n / (SOST/sizeof(*p));
+        n %= SOST/sizeof(*p);
+        if (i & 1) {
+                m1 = *(size_t *)p;
+                asm ("hadd,us %1, %2, %0" : "=r" (m1) : "r" (m1), "r" (vwsize));
+                *(size_t *)p = m1;
+                p += SOST/sizeof(*p);
+                i--;
+        }
+        i /= 2;
+        do {
+                m1 = ((size_t *)p)[0];
+                m2 = ((size_t *)p)[1];
+                asm ("hadd,us %1, %2, %0" : "=r" (m1) : "r" (m1), "r" (vwsize));
+                asm ("hadd,us %1, %2, %0" : "=r" (m2) : "r" (m2), "r" (vwsize));
+                ((size_t *)p)[0] = m1;
+                ((size_t *)p)[1] = m2;
+                p += 2*(SOST/sizeof(*p));
+        } while (--i);
+
+        /* handle trailer */
+        if (unlikely(n)) {
+            if (SOST > 4 && n >= 2) {
+                unsigned int m = *(unsigned int *)p;
+                asm ("hadd,us %1, %2, %0" : "=r" (m) : "r" (m), "r" (vwsize));
+                *(unsigned int *)p = m;
+                p += 2;
+                n -= 2;
+            }
+            if (n & 1) {
+                register unsigned m = *p;
+                *p++ = (Pos)(m >= wsize ? m-wsize : NIL);
+            }
+        }
+    } else {
+        do {
+            register unsigned m = *p;
+            *p++ = (Pos)(m >= wsize ? m-wsize : NIL);
+        } while (--n);
+    }
+}
+#  endif
+#endif
diff -Naur a/ppc/adler32.c b/ppc/adler32.c
--- a/ppc/adler32.c	1970-01-01 01:00:00.000000000 +0100
+++ b/ppc/adler32.c	2015-08-16 05:57:02.000000000 +0200
@@ -0,0 +1,404 @@
+/*
+ * adler32.c -- compute the Adler-32 checksum of a data stream
+ *   ppc implementation
+ * Copyright (C) 1995-2007 Mark Adler
+ * Copyright (C) 2009-2012 Jan Seiffert
+ * For conditions of distribution and use, see copyright notice in zlib.h
+ */
+
+/* @(#) $Id$ */
+
+/*
+ * We use the Altivec PIM vector stuff, but still, this is only
+ * tested with GCC, and prop. uses some GCC specifics (like GCC
+ * understands vector types and you can simply write a += b)
+ */
+#if defined(__ALTIVEC__) && defined(__GNUC__)
+# define HAVE_ADLER32_VEC
+/* it needs some bytes till the vec version gets up to speed... */
+local noinline uLong adler32_ge16(uLong adler, const Bytef *buf, uInt len);
+# define MIN_WORK 64
+# include <altivec.h>
+
+/*
+ * Depending on length, this can be slower (short length < 64 bytes),
+ * much faster (our beloved 128kb 22.2s generic to 3.4s vec, but cache
+ * is important...), to a little faster (very long length, 1.6MB, 47.6s
+ * to 36s), which is prop. only capped by memory bandwith.
+ * (The orig. 128k case was slower in AltiVec, because AltiVec loads
+ * are always uncached and trigger no HW prefetching, because that is
+ * what you often need with mass data manipulation (not poisen your
+ * cache, movntq), instead you have to do it for yourself (data stream
+ * touch). With 128k it could be cleanly seen: no prefetch, half as slow
+ * as generic, but comment out the memory load -> 3s. With proper prefetch
+ * we are at 3.4s. So AltiVec can execute these "expensive" FMA quite
+ * fast (even without fancy unrolling), only the data does not arrive
+ * fast enough. In cases where the working set does not fit into cache
+ * it simply cannot be delivered fast enough over the FSB/Mem).
+ * Still we have to prefetch, or we are slow as hell.
+ */
+/*
+ * Modern ultra SMT/SMP POWER processors (the BigIron ones) are better
+ * at mem bandwidth and do not need prefetching (those instructions are
+ * to be seen as nops, says the ISA doc).
+ * But they are _not_ OutOfOrder Execution CPUs (lots of small, high
+ * freq. "simple" cores). So unrolling with independent sums to prevent
+ * pipeline stalls is necessary.
+ * This also helps the OOOE CPUs a little bit, but was not crucial up
+ * till now.
+ *
+ * The result is a breath taking speedup of 11.5 on a POWER7@3.55GHz
+ * compared to the original adler32.
+ */
+# define SOVUC (sizeof(vector unsigned char))
+
+/* lot of independent sums, lots of bytes can be accumulated */
+# define VNMAX (28*NMAX)
+
+/* ========================================================================= */
+local inline vector unsigned char vec_identl(level)
+    unsigned int level;
+{
+    return vec_lvsl(level, (const unsigned char *)0);
+}
+
+/* ========================================================================= */
+local inline vector unsigned char vec_ident_rev(void)
+{
+    return vec_xor(vec_identl(0), vec_splat_u8(15));
+}
+
+/* ========================================================================= */
+/* multiply two 32 bit ints, return the low 32 bit */
+local inline vector unsigned int vec_mullw(vector unsigned int a, vector unsigned int b)
+{
+    vector unsigned int v16   = vec_splat_u32(-16);
+    vector unsigned int v0_32 = vec_splat_u32(0);
+    vector unsigned int swap, low, high;
+
+    swap = vec_rl(b, v16);
+     low = vec_mulo((vector unsigned short)a, (vector unsigned short)b);
+    high = vec_msum((vector unsigned short)a, (vector unsigned short)swap, v0_32);
+    high = vec_sl(high, v16);
+    return vec_add(low, high);
+}
+
+/* ========================================================================= */
+local inline vector unsigned int vector_chop(vector unsigned int x)
+{
+    vector unsigned int y;
+    vector unsigned int vsh;
+
+    vsh = vec_splat_u32(1);
+    vsh = vec_sl(vsh, vec_splat_u32(4));
+
+    y = vec_sl(x, vsh);
+    y = vec_sr(y, vsh);
+    x = vec_sr(x, vsh);
+    y = vec_sub(y, x);
+    x = vec_sl(x, vec_splat_u32(4));
+    x = vec_add(x, y);
+    return x;
+}
+
+/* ========================================================================= */
+local inline vector unsigned int vector_load_one_u32(unsigned int x)
+{
+    vector unsigned int val = vec_lde(0, &x);
+    vector unsigned char vperm, mask;
+
+     mask = (vector unsigned char)vec_cmpgt(vec_identl(0), vec_splat_u8(3));
+    vperm = (vector unsigned char)vec_splat((vector unsigned int)vec_lvsl(0, &x), 0);
+      val = vec_perm(val, val, vperm);
+      val = vec_sel(val, vec_splat_u32(0), (vector unsigned int)mask);
+    return val;
+}
+
+/* ========================================================================= */
+/*
+ * PPC hates conditional branches.
+ * Unfotunatly a cmov is not in the ISA, or brand new in POWER7
+ * (and GCC is not generating it...).
+ * So a little tricky bit magic is done.
+ */
+local inline unsigned int isel(int a, unsigned int x, unsigned int y)
+{
+    /*
+     * If the right shift of a signed number is an arithmetic
+     * shift in C is implementation defined.
+     * Since PPC is a 2-complement arch and has the right
+     * instructions to do arith. shifts, a compiler which
+     * does not generate an arithmetic shift in the following
+     * line can be considered ... obsolet.
+     * On the other hand with a little luck a compiler can
+     * change this back to isel or some subc pattern.
+     */
+    int mask = a >> 31; /* create mask of 0 or 0xffffffff */
+    /* if a >= 0 return x, else y */
+    return x + ((y - x) & mask);
+}
+
+/* ========================================================================= */
+local noinline uLong adler32_vec_real(adler, buf, len)
+    uLong adler;
+    const Bytef *buf;
+    uInt len;
+{
+    vector unsigned char vord[4];
+    vector unsigned int vs1, vs2;
+    unsigned int f, n;
+    unsigned int k;
+    unsigned int s1, s2;
+
+    s1 = adler & 0xffff;
+    s2 = (adler >> 16) & 0xffff;
+
+    /*
+     * start by firing of prefetches.
+     * Try to not prefetch to much, maybe it is not such
+     * a large len.
+     * prefetching will stop at page bounderies, because
+     * the prefetch engine does not know the virtual<>real
+     * address mapping of the next page.
+     * assume 4k pages
+     */
+    f = ALIGN_DIFF(buf, 4096);
+    if(len > f) {
+        unsigned int block_size = 1, block_num = DIV_ROUNDUP(f, 16);
+        /* prefetch rest of page */
+        while(block_num >= 256)
+            block_size *= 2, block_num /= 2;
+        f  = 8 << block_size; /* stride */
+        f |= block_num << 16;
+        f |= block_size << 24;
+        vec_dstt(buf, f, 2);
+        /* prefetch next page */
+        block_size = 1;
+        n = len - f > 4096 ? 4096 : len - f;
+        block_num = DIV_ROUNDUP(n, 16);
+        while(block_num >= 256)
+            block_size *= 2, block_num /= 2;
+        f  = 8 << block_size; /* stride */
+        f |= block_num << 16;
+        f |= block_size << 24;
+        vec_dstt(buf, f, 1);
+    } else {
+        unsigned int block_size = 1, block_num = DIV_ROUNDUP(len, 16);
+        while(block_num >= 256)
+            block_size *= 2, block_num /= 2;
+        f  = 8 << block_size; /* stride */
+        f |= block_num << 16;
+        f |= block_size << 24;
+        vec_dstt(buf, f, 1);
+    }
+    /*
+     * if i understand the Altivec PEM right, little
+     * endian impl. should have the data reversed on
+     * load, so the big endian vorder works.
+     */
+    {
+        vector unsigned char v1 = vec_splat_u8(1), vt;
+        vord[0] = vec_ident_rev() + v1;
+             vt = vec_sl(vec_splat_u8(8), v1);
+        vord[1] = vec_add(vord[0], vt);
+        vord[2] = vec_add(vord[1], vt);
+        vord[3] = vec_add(vord[2], vt);
+    }
+
+    /* align hard down */
+    f = (unsigned) ALIGN_DOWN_DIFF(buf, SOVUC);
+    n = SOVUC - f;
+    buf = (const unsigned char *)ALIGN_DOWN(buf, SOVUC);
+
+    /* add n times s1 to s2 for start round */
+    s2 += s1 * n;
+
+    k = len < VNMAX ? (unsigned)len : VNMAX;
+    len -= k;
+
+    /* insert scalar start somewhere */
+    vs1 = vector_load_one_u32(s1);
+    vs2 = vector_load_one_u32(s2);
+
+    {
+        vector unsigned char v0 = vec_splat_u8(0);
+        vector unsigned char in16, vperm;
+
+        /* get input data */
+         in16 = vec_ldl(0, buf);
+        /* Mask out exess data to achieve alignment */
+        vperm = vec_lvsl(f, buf);
+         in16 = vec_perm(in16, v0, vperm);
+        vperm = vec_lvsr(f, buf);
+         in16 = vec_perm(v0, in16, vperm);
+        /* add 4 byte horizontal and add to old dword */
+          vs1 = vec_sum4s(in16, vs1);
+        /* apply order, add 4 byte horizontal and add to old dword */
+          vs2 = vec_msum(in16, vord[0], vs2);
+    }
+
+    buf += SOVUC;
+      k -= n;
+    if (likely(k >= SOVUC)) do {
+        if (k >= 8*SOVUC) {
+            /* if length is larger, do it unrolled 4 times */
+            vector unsigned int vsh4;
+            vector unsigned int vs1_rm[4];
+            vector unsigned int vs1m[4];
+            vector unsigned int vs2m[4];
+            const unsigned char *t = (const unsigned char *) ALIGN_DOWN(buf + 4096, 4096);
+
+            vs1_rm[0] = vs1_rm[1] = vs1_rm[2] = vs1_rm[3] = vec_splat_u32(0);
+            vs1m[0] = vs1;
+            vs1m[1] = vs1m[2] = vs1m[3] = vs1_rm[0];
+            vs2m[0] = vs2m[1] = vs2m[2] = vs1_rm[0];
+            vs2m[3] = vs2;
+            do {
+                unsigned int i = isel(k - 8192, 8192, k);
+
+                f  = 512;
+                f |= 8 << 16;
+                /* prefetch the next two pages */
+#ifdef __GNUC__
+                /* gcc optimizes the dstt away, prop because he already sees
+                 * one above ... not good */
+                __asm__ __volatile__("dstt %0, %1, %2" : : "r" (t), "r" (f), "i" (2));
+                __asm__ __volatile__("dstt %0, %1, %2" : : "r" (t+4096), "r" (f), "i" (1));
+#else
+                vec_dstt(t, f, 2);
+                vec_dstt(t+4096, f, 1);
+#endif
+                t += 8192;
+                i /= 4*SOVUC;
+                k -= i * 4*SOVUC;
+                do {
+                    vector unsigned char in16m[4];
+                    /* get input data */
+                     in16m[0] = vec_ldl(0*SOVUC, buf);
+                     in16m[1] = vec_ldl(1*SOVUC, buf);
+                     in16m[2] = vec_ldl(2*SOVUC, buf);
+                     in16m[3] = vec_ldl(3*SOVUC, buf);
+
+                    /* add vs1 for this round */
+                    vs1_rm[0] = vec_add(vs1_rm[0], vs1m[0]);
+                    vs1_rm[1] = vec_add(vs1_rm[1], vs1m[1]);
+                    vs1_rm[2] = vec_add(vs1_rm[2], vs1m[2]);
+                    vs1_rm[3] = vec_add(vs1_rm[3], vs1m[3]);
+
+                    /* add 4 byte horizontal and add to old dword */
+                      vs1m[0] = vec_sum4s(in16m[0], vs1m[0]);
+                      vs1m[1] = vec_sum4s(in16m[1], vs1m[1]);
+                      vs1m[2] = vec_sum4s(in16m[2], vs1m[2]);
+                      vs1m[3] = vec_sum4s(in16m[3], vs1m[3]);
+                    /* apply order, add 4 byte horizontal and add to old dword */
+                      vs2m[0] = vec_msum(in16m[0], vord[3], vs2m[0]);
+                      vs2m[1] = vec_msum(in16m[1], vord[2], vs2m[1]);
+                      vs2m[2] = vec_msum(in16m[2], vord[1], vs2m[2]);
+                      vs2m[3] = vec_msum(in16m[3], vord[0], vs2m[3]);
+
+                    buf += 4*SOVUC;
+                } while (--i);
+            } while (k >= 4*SOVUC);
+            /* chop vs1 round sum before multiplying by 64 */
+            vs1_rm[0] = vector_chop(vs1_rm[0]);
+            vs1_rm[1] = vector_chop(vs1_rm[1]);
+            vs1_rm[2] = vector_chop(vs1_rm[2]);
+            vs1_rm[3] = vector_chop(vs1_rm[3]);
+            /* add all vs1 for 64 times */
+            vsh4 = vec_splat_u32(6);
+              vs2m[0] = vec_add(vs2m[0], vec_sl(vs1_rm[0], vsh4));
+              vs2m[1] = vec_add(vs2m[1], vec_sl(vs1_rm[1], vsh4));
+              vs2m[2] = vec_add(vs2m[2], vec_sl(vs1_rm[2], vsh4));
+              vs2m[3] = vec_add(vs2m[3], vec_sl(vs1_rm[3], vsh4));
+            /* chop the vectors to something in the range of BASE */
+                  vs2 = vector_chop(vs2m[0] + vs2m[1] + vs2m[2] + vs2m[3]);
+                  vs1 = vector_chop(vs1m[0] + vs1m[1] + vs1m[2] + vs1m[3]);
+        } else {
+            vector unsigned int vs1_r = vec_splat_u32(0), vsh;
+            do {
+                /* get input data */
+                vector unsigned char in16 = vec_ldl(0, buf);
+
+                /* add vs1 for this round */
+                vs1_r = vec_add(vs1_r, vs1);
+                /* add 4 byte horizontal and add to old dword */
+                  vs1 = vec_sum4s(in16, vs1);
+                /* apply order, add 4 byte horizontal and add to old dword */
+                  vs2 = vec_msum(in16, vord[0], vs2);
+
+                buf += SOVUC;
+                  k -= SOVUC;
+            } while (k >= SOVUC);
+            /* chop vs1 round sum before multiplying by 16 */
+            vs1_r = vector_chop(vs1_r);
+            /* add all vs1 for 16 times */
+              vsh = vec_splat_u32(4);
+              vs2 = vec_add(vs2, vec_sl(vs1_r, vsh));
+            /* chop the vectors to something in the range of BASE */
+              vs2 = vector_chop(vs2);
+              vs1 = vector_chop(vs1);
+        }
+        len += k;
+           k = len < VNMAX ? (unsigned)len : VNMAX;
+        len -= k;
+    } while (likely(k >= SOVUC));
+
+    vec_dss(1);
+    vec_dss(2);
+
+    if (likely(k)) {
+        /* handle trailer */
+        vector unsigned char in16, vperm;
+        vector unsigned char   v0 = vec_splat_u8(0);
+        /* transport k over to vec unit */
+        vector unsigned int vk = (vector unsigned int)vec_lvsl(0, (unsigned *)(intptr_t)k);
+        vk = (vector unsigned)vec_mergeh(v0, (vector unsigned char)vk);
+        vk = (vector unsigned)vec_mergeh((vector unsigned short)v0, (vector unsigned short)vk);
+        vk = vec_splat(vk, 0);
+        /* add k times vs1 for this trailer */
+        vs2 = vec_add(vs2, vec_mullw(vs1, vk));
+
+        /* get input data */
+         in16 = vec_ldl(0, buf);
+        /* mask out excess data */
+        vperm = vec_lvsr(0, (const unsigned char *)(SOVUC - k));
+         in16 = vec_perm(v0, in16, vperm);
+        /* add 4 byte horizontal and add to old dword */
+          vs1 = vec_sum4s(in16, vs1);
+        /* apply order, add 4 byte horizontal and add to old dword */
+          vs2 = vec_msum(in16, vord[0], vs2);
+
+        buf += k;
+          k -= k;
+    }
+
+    /* add horizontal */
+    /* stuff should be choped so no proplem with signed saturate */
+    vs1 = (vector unsigned int)vec_sums((vector int)vs1, vec_splat_s32(0));
+    vs2 = (vector unsigned int)vec_sums((vector int)vs2, vec_splat_s32(0));
+    /* shake and roll */
+    vs1 = vec_splat(vs1, 3);
+    vs2 = vec_splat(vs2, 3);
+    vec_ste(vs1, 0, &s1);
+    vec_ste(vs2, 0, &s2);
+
+    /* after horizontal add, modulo again in scalar code */
+    MOD28(s1);
+    MOD28(s2);
+
+    return (s2 << 16) | s1;
+}
+
+/* ========================================================================= */
+local noinline uLong adler32_vec(adler, buf, len)
+    uLong adler;
+    const Bytef *buf;
+    uInt len;
+{
+    /* Factor out short len handling, to get rid of one indent. While at it
+       force GCC to create a lean stackframe for to short len by tailcall */
+    if (unlikely(len < 2*SOVUC))
+        return adler32_ge16(adler, buf, len);
+    return adler32_vec_real(adler, buf, len);
+}
+#endif
diff -Naur a/ppc/slhash.c b/ppc/slhash.c
--- a/ppc/slhash.c	1970-01-01 01:00:00.000000000 +0100
+++ b/ppc/slhash.c	2015-08-16 05:57:02.000000000 +0200
@@ -0,0 +1,247 @@
+/* slhash.c -- slide the hash table during fill_window()
+ * Copyright (C) 1995-2010 Jean-loup Gailly and Mark Adler
+ * Copyright (C) 2011 Jan Seiffert
+ * For conditions of distribution and use, see copyright notice in zlib.h
+ */
+
+/* NOTE:
+ * We do not precheck the length or wsize for small values because
+ * we assume a minimum len of 256 (for MEM_LEVEL 1) and a minimum wsize
+ * of 256 for windowBits 8
+ */
+
+#if defined(__ALTIVEC__) && defined(__GNUC__)
+#  define HAVE_SLHASH_VEC
+#  include <altivec.h>
+#  define SOVUS (sizeof(vector unsigned short))
+
+local inline vector int vector_splat_scalar_s32(int x)
+{
+    vector int val = vec_lde(0, &x);
+    vector unsigned char vperm;
+
+    vperm = (vector unsigned char)vec_splat((vector int)vec_lvsl(0, &x), 0);
+    return vec_perm(val, val, vperm);
+}
+
+local inline vector unsigned short vector_splat_scalar_u16(unsigned short x)
+{
+    vector unsigned short val = vec_lde(0, &x);
+    vector unsigned char vperm;
+
+    vperm = (vector unsigned char)vec_splat((vector unsigned short)vec_lvsl(0, &x), 0);
+    return vec_perm(val, val, vperm);
+}
+
+local void update_hoffset_l(Posf *p, uInt wsize, unsigned n)
+{
+    vector unsigned short vin;
+    vector unsigned short v0;
+    vector int vwsize, vinl, vinh;
+    unsigned int f, i;
+
+    v0 = vec_splat_u16(0);
+    vwsize = vector_splat_scalar_s32((int)wsize);
+    /* achive alignment */
+    f = (unsigned) ALIGN_DOWN_DIFF(p, SOVUS);
+    if (f) {
+        vector unsigned char vperml, vpermr;
+        vector unsigned short vin_o;
+            f  = (SOVUS - f)/sizeof(*p);
+            n -= f;
+        vperml = vec_lvsl(0, p);
+        vpermr = vec_lvsr(0, p);
+        /* align hard down */
+             p = (unsigned short *)ALIGN_DOWN(p, SOVUS);
+           vin = vec_ldl(0, p);
+         vin_o = vec_perm(vin, vin, vperml);
+           vin = vec_perm(vin, v0, vperml);
+          vinl = (vector int)vec_mergel(v0, vin);
+          vinh = (vector int)vec_mergeh(v0, vin);
+          vinl = vec_sub(vinl, vwsize);
+          vinh = vec_sub(vinh, vwsize);
+           vin = vec_packsu(vinh, vinl);
+           vin = vec_perm(vin_o, vin, vpermr);
+        /*
+         * We write it out again element by element
+         * because writing out the whole vector
+         * may race against the owner of the other
+         * data.
+         * The proper fix for this is aligning the
+         * arrays at malloc time.
+         */
+        do {
+            vec_ste(vin, SOVUS-(f*sizeof(*p)), p);
+        } while (--f);
+        p += SOVUS/sizeof(*p);
+    }
+
+    i  = n / (SOVUS/sizeof(*p));
+    n %= SOVUS/sizeof(*p);
+
+    do {
+         vin = vec_ldl(0, p);
+        vinl = (vector int)vec_mergel(v0, vin);
+        vinh = (vector int)vec_mergeh(v0, vin);
+        vinl = vec_sub(vinl, vwsize);
+        vinh = vec_sub(vinh, vwsize);
+         vin = vec_packsu(vinh, vinl);
+        vec_stl(vin, 0, p);
+          p += SOVUS/sizeof(*p);
+    } while (--i);
+
+    if (n) {
+        vector unsigned char vperml, vpermr;
+        vector unsigned short vin_o;
+           vin = vec_ldl(0, p);
+        vperml = vec_lvsl(n*sizeof(*p), p);
+        vpermr = vec_lvsr(n*sizeof(*p), p);
+         vin_o = vec_perm(vin, vin, vperml);
+           vin = vec_perm(v0, vin, vperml);
+          vinl = (vector int)vec_mergel(v0, vin);
+          vinh = (vector int)vec_mergeh(v0, vin);
+          vinl = vec_sub(vinl, vwsize);
+          vinh = vec_sub(vinh, vwsize);
+           vin = vec_packsu(vinh, vinl);
+           vin = vec_perm(vin, vin_o, vpermr);
+             i = 0;
+        do {
+            vec_ste(vin, i*sizeof(*p), p);
+            i++;
+        } while (--n);
+    }
+}
+
+local void update_hoffset(Posf *p, uInt wsize, unsigned n)
+{
+    vector unsigned short vin;
+    vector unsigned short v0;
+    vector unsigned short vwsize;
+    unsigned int f, i;
+    unsigned int block_num;
+
+    block_num = DIV_ROUNDUP(n, 512); /* 32 block size * 16 */
+    f  = 512;
+    f |= block_num >= 256 ? 0 : block_num << 16;
+    vec_dst(p, f, 2);
+
+    if (unlikely(wsize > (1<<16)-1)) {
+        update_hoffset_l(p, wsize, n);
+        return;
+    }
+
+        v0 = vec_splat_u16(0);
+    vwsize = vector_splat_scalar_u16((unsigned short)wsize);
+    /* achive alignment */
+    f = (unsigned) ALIGN_DOWN_DIFF(p, SOVUS);
+    if (f) {
+        vector unsigned char vperml, vpermr;
+        vector unsigned short vin_o;
+            f  = (SOVUS-f)/sizeof(*p);
+            n -= f;
+        vperml = vec_lvsl(0, p);
+        vpermr = vec_lvsr(0, p);
+        /* align hard down */
+             p = (unsigned short *)ALIGN_DOWN(p, SOVUS);
+           vin = vec_ldl(0, p);
+         vin_o = vec_perm(vin, vin, vperml);
+           vin = vec_perm(vin, v0, vperml);
+           vin = vec_subs(vin, vwsize);
+           vin = vec_perm(vin_o, vin, vpermr);
+        do {
+            vec_ste(vin, SOVUS-(f*sizeof(*p)), p);
+        } while (--f);
+        p += SOVUS/sizeof(*p);
+    }
+
+    i  = n / (SOVUS/sizeof(*p));
+    n %= SOVUS/sizeof(*p);
+
+    do {
+        vin = vec_ldl(0, p);
+        vin = vec_subs(vin, vwsize);
+        vec_stl(vin, 0, p);
+         p += SOVUS/sizeof(*p);
+    } while (--i);
+
+    if (n) {
+        vector unsigned char vperml, vpermr;
+        vector unsigned short vin_o;
+           vin = vec_ldl(0, p);
+        vperml = vec_lvsl(n*sizeof(*p), p);
+        vpermr = vec_lvsr(n*sizeof(*p), p);
+         vin_o = vec_perm(vin, vin, vperml);
+           vin = vec_perm(v0, vin, vperml);
+           vin = vec_subs(vin, vwsize);
+           vin = vec_perm(vin, vin_o, vpermr);
+             i = 0;
+        do {
+            vec_ste(vin, i*sizeof(*p), p);
+            i++;
+        } while (--n);
+    }
+}
+#else
+#  define HAVE_SLHASH_VEC
+#  include <limits.h>
+/*
+ * PowerPC is a complicated beast...
+ *
+ * Most PPC pipelines do not cope well with
+ * conditional branches. The simple ones (embedded
+ * & synthesized) because they are simple, the big
+ * ones because they are ... big.
+ *
+ * And here comes the kicker: PPC does not have a
+ * conditional move. (Or: The isel instruction is
+ * "brand new" for the Power ISA v2.06, a.k.a the
+ * POWER7)
+ * So GCC generates a little branch over a "move
+ * zero". Which is data dependent. Which plays
+ * havok with the branch prediction if the CPU has
+ * one, or simply with the instruciton flow in the
+ * pipeline if not.
+ *
+ * So rewrite this a little to get some kind of
+ * conditional move.
+ * This gives an ~5.6% overall speedup on a G5 with
+ * minigzip to test deflate.
+ *
+ * (going over the flags would be a little faster,
+ * but expressing carry arithmetic in C is cumbersome
+ * and esp. with GCC generates ugly code, which
+ * would bring us to inline asm, and if one PPC does
+ * not like it...)
+ */
+local inline unsigned isel(int a, unsigned x, unsigned y)
+{
+    /*
+     * If the right shift of a signed number is an arithmetic
+     * shift in C is implementation defined.
+     * Since PPC is a 2-complement arch and has the right
+     * instructions to do arith. shifts, a compiler which
+     * does not generate an arithmetic shift in the following
+     * line can be considered ... obsolet.
+     * On the other hand with a little luck a compiler can
+     * change this back to isel or some subc pattern.
+     */
+    int mask = a >> 31; /* create mask of 0 or 0xffffffff */
+    /* if a >= 0 return x, else y */
+    return x + ((y - x) & mask);
+}
+
+local void update_hoffset(Posf *p, uInt wsize, unsigned n)
+{
+    if (sizeof(*p)*CHAR_BIT < 32) {
+        do {
+            register unsigned m = *p;
+            *p++ = (Pos)isel(m-wsize, m-wsize, NIL);
+        } while (--n);
+    } else {
+        do {
+            register unsigned m = *p;
+            *p++ = (Pos)(m >= wsize ? m-wsize : NIL);
+        } while (--n);
+    }
+}
+#endif
diff -Naur a/slhash.c b/slhash.c
--- a/slhash.c	1970-01-01 01:00:00.000000000 +0100
+++ b/slhash.c	2015-08-16 05:57:02.000000000 +0200
@@ -0,0 +1,56 @@
+/* slhash.c -- slide the hash table during fill_window()
+ * Copyright (C) 1995-2010 Jean-loup Gailly and Mark Adler
+ * For conditions of distribution and use, see copyright notice in zlib.h
+ */
+#include "deflate.h"
+#define NIL 0
+
+#ifndef NO_SLHASH_VEC
+#  if defined(__arm__)
+#    include "arm/slhash.c"
+#  elif defined(__alpha__)
+#    include "alpha/slhash.c"
+#  elif defined(__bfin__)
+#    include "bfin/slhash.c"
+#  elif defined(__ia64__)
+#    include "ia64/slhash.c"
+#  elif defined(__mips__)
+#    include "mips/slhash.c"
+#  elif defined(__hppa__) || defined(__hppa64__)
+#    include "parisc/slhash.c"
+#  elif defined(__powerpc__) || defined(__powerpc64__)
+#    include "ppc/slhash.c"
+#  elif defined(__tile__)
+#    include "tile/slhash.c"
+#  elif defined(__i386__) || defined(__x86_64__)
+#    include "x86/slhash.c"
+#  endif
+#endif
+
+#ifndef HAVE_SLHASH_COMPLETE
+#  ifndef HAVE_SLHASH_VEC
+local void update_hoffset(Posf *p, uInt wsize, unsigned n)
+{
+    register unsigned m;
+    do {
+        m = *p;
+        *p++ = (Pos)(m >= wsize ? m-wsize : NIL);
+    } while (--n);
+}
+#  endif
+
+void ZLIB_INTERNAL _sh_slide (p, q, wsize, n)
+    Posf *p;
+    Posf *q;
+    uInt wsize;
+    unsigned n;
+{
+    update_hoffset(p, wsize, n);
+#  ifndef FASTEST
+    /* If n is not on any hash chain, prev[n] is garbage but
+     * its value will never be used.
+     */
+    update_hoffset(q, wsize, wsize);
+#  endif
+}
+#endif
diff -Naur a/sparc/adler32.c b/sparc/adler32.c
--- a/sparc/adler32.c	1970-01-01 01:00:00.000000000 +0100
+++ b/sparc/adler32.c	2015-08-16 05:57:02.000000000 +0200
@@ -0,0 +1,354 @@
+/*
+ * adler32.c -- compute the Adler-32 checksum of a data stream
+ *   sparc implementation
+ * Copyright (C) 1995-2007 Mark Adler
+ * Copyright (C) 2009-2011 Jan Seiffert
+ * For conditions of distribution and use, see copyright notice in zlib.h
+ */
+
+/* @(#) $Id$ */
+
+/* we use inline asm, so GCC it is */
+#if defined(HAVE_VIS) && defined(__GNUC__)
+#  define HAVE_ADLER32_VEC
+#  define MIN_WORK 512
+#  define VNMAX ((5*NMAX)/2)
+#  define SOVV (sizeof(unsigned long long))
+
+/*
+ * SPARC CPUs gained a SIMD extention with the UltraSPARC I, called
+ * VIS, implemented in their FPU. We can build adler32 with it.
+ *
+ * But Note:
+ * - Do not use it with Niagara or other CPUs like it. They have a
+ *   shared FPU (T1: 1 FPU for all up to 8 cores, T2: 1 FPU for 8 threads)
+ *   and to make matters worse the code does not seem to work there
+ *   (binary which creates correct results on other SPARC creates wrong
+ *   result on T1)
+ * - There is no clear preprocesor define which tells us if we have VIS.
+ *   Often the tool chain even disguises a sparcv9 as a sparcv8
+ *   (pre-UltraSPARC) to not confuse the userspace.
+ * - The code has a high startup cost
+ * - The code only handles big endian
+ *
+ * For these reasons this code is not automatically enabled and you have
+ * to define HAVE_VIS as a switch to enable it. We can not easily provide a
+ * dynamic runtime switch. The CPU has make and model encoded in the Processor
+ * Status Word (PSW), but reading the PSW is a privilidged instruction (same
+ * as PowerPC...).
+ */
+
+/* ========================================================================= */
+local inline unsigned long long fzero(void)
+{
+    unsigned long long t;
+    asm ("fzero %0" : "=e" (t));
+    return t;
+}
+
+/* ========================================================================= */
+local inline unsigned long long fpmerge_lo(unsigned long long a, unsigned long long b)
+{
+    unsigned long long t;
+    asm ("fpmerge	%L1, %L2, %0" : "=e" (t) : "f" (a), "f" (b));
+    return t;
+}
+
+/* ========================================================================= */
+local inline unsigned long long fpmerge_hi(unsigned long long a, unsigned long long b)
+{
+    unsigned long long t;
+    asm ("fpmerge	%H1, %H2, %0" : "=e" (t) : "f" (a), "f" (b));
+    return t;
+}
+#define pextlbh(x) fpmerge_lo(fzero(), x)
+#define pexthbh(x) fpmerge_hi(fzero(), x)
+
+/* ========================================================================= */
+local inline unsigned long long fpadd32(unsigned long long a, unsigned long long b)
+{
+    unsigned long long t;
+    asm ("fpadd32	%1, %2, %0" : "=e" (t) : "%e" (a), "e" (b));
+    return t;
+}
+
+/* ========================================================================= */
+local inline unsigned long long fpadd16(unsigned long long a, unsigned long long b)
+{
+    unsigned long long t;
+    asm ("fpadd16	%1, %2, %0" : "=e" (t) : "%e" (a), "e" (b));
+    return t;
+}
+
+/* ========================================================================= */
+local inline unsigned long long fpsub32(unsigned long long a, unsigned long long b)
+{
+    unsigned long long t;
+    asm ("fpsub32	%1, %2, %0" : "=e" (t) : "e" (a), "e" (b));
+    return t;
+}
+
+/* ========================================================================= */
+local inline unsigned long long pdist(unsigned long long a, unsigned long long b, unsigned long long acc)
+{
+    asm ("pdist	%1, %2, %0" : "=e" (acc) : "e" (a), "e" (b), "0" (acc));
+    return acc;
+}
+
+/* ========================================================================= */
+local inline unsigned long long fmuld8ulx16_lo(unsigned long long a, unsigned long long b)
+{
+    unsigned long long t;
+    asm ("fmuld8ulx16	%L1, %L2, %0" : "=e" (t) : "f" (a), "f" (b));
+    return t;
+}
+
+/* ========================================================================= */
+local inline unsigned long long fmuld8ulx16_hi(unsigned long long a, unsigned long long b)
+{
+    unsigned long long t;
+    asm ("fmuld8ulx16	%H1, %H2, %0" : "=e" (t) : "f" (a), "f" (b));
+    return t;
+}
+
+/* ========================================================================= */
+local inline unsigned long long fmuld8sux16_lo(unsigned long long a, unsigned long long b)
+{
+    unsigned long long t;
+    asm ("fmuld8sux16	%L1, %L2, %0" : "=e" (t) : "f" (a), "f" (b));
+    return t;
+}
+
+/* ========================================================================= */
+local inline unsigned long long fmuld8sux16_hi(unsigned long long a, unsigned long long b)
+{
+    unsigned long long t;
+    asm ("fmuld8sux16	%H1, %H2, %0" : "=e" (t) : "f" (a), "f" (b));
+    return t;
+}
+
+/* ========================================================================= */
+local inline unsigned long long fpmul16x16x32_lo(unsigned long long a, unsigned long long b)
+{
+    unsigned long long r_l = fmuld8ulx16_lo(a, b);
+    unsigned long long r_h = fmuld8sux16_lo(a, b);
+    return fpadd32(r_l, r_h);
+}
+
+/* ========================================================================= */
+local inline unsigned long long fpmul16x16x32_hi(unsigned long long a, unsigned long long b)
+{
+    unsigned long long r_l = fmuld8ulx16_hi(a, b);
+    unsigned long long r_h = fmuld8sux16_hi(a, b);
+    return fpadd32(r_l, r_h);
+}
+
+/* ========================================================================= */
+local inline unsigned long long fpand(unsigned long long a, unsigned long long b)
+{
+    unsigned long long t;
+    asm ("fand	%1, %2, %0" : "=e" (t) : "e" (a), "e" (b));
+    return t;
+}
+
+/* ========================================================================= */
+local inline unsigned long long fpandnot2(unsigned long long a, unsigned long long b)
+{
+    unsigned long long t;
+    asm ("fandnot2	%1, %2, %0" : "=e" (t) : "e" (a), "e" (b));
+    return t;
+}
+
+/* ========================================================================= */
+local inline void *alignaddr(const void *ptr, int off)
+{
+    void *t;
+    asm volatile ("alignaddr	%1, %2, %0" : "=r" (t) : "r" (ptr), "r" (off));
+    return t;
+}
+
+/* ========================================================================= */
+local inline void *alignaddrl(const void *ptr, int off)
+{
+    void *t;
+    asm volatile ("alignaddrl	%1, %2, %0" : "=r" (t) : "r" (ptr), "r" (off));
+    return t;
+}
+
+/* ========================================================================= */
+local inline unsigned long long faligndata(unsigned long long a, unsigned long long b)
+{
+    unsigned long long ret;
+    asm volatile ("faligndata	%1, %2, %0" : "=e" (ret) : "e" (a), "e" (b));
+    return ret;
+}
+
+/* ========================================================================= */
+local inline unsigned long long vector_chop(unsigned long long x)
+{
+    unsigned long long mask = 0x0000ffff0000ffffull;
+    unsigned long long y = fpand(x, mask);
+    x = fpandnot2(x, mask);
+    x >>= 16;
+    y = fpsub32(y, x);
+    x = fpadd32(x, x); /* << 1 */
+    x = fpadd32(x, x); /* << 1 */
+    x = fpadd32(x, x); /* << 1 */
+    x = fpadd32(x, x); /* << 1 */
+    return fpadd32(x, y);
+}
+
+/* ========================================================================= */
+local noinline uLong adler32_vec(adler, buf, len)
+    uLong adler;
+    const Bytef *buf;
+    uInt len;
+{
+    unsigned int s1, s2;
+
+    s1 = adler & 0xffff;
+    s2 = (adler >> 16) & 0xffff;
+
+    if (likely(len >= 2 * SOVV)) {
+        unsigned long long vorder_lo = 0x0004000300020001ULL;
+        unsigned long long vorder_hi = 0x0008000700060005ULL;
+        unsigned long long vs1, vs2;
+        unsigned long long v0 = fzero();
+        unsigned long long in;
+        const unsigned char *o_buf;
+        unsigned int k, f, n;
+
+        /* align hard down */
+        f = (unsigned int)ALIGN_DOWN_DIFF(buf, SOVV);
+        n = SOVV - f;
+
+        /* add n times s1 to s2 for start round */
+        s2 += s1 * n;
+
+        /* insert scalar start somehwere */
+        vs1 = s1;
+        vs2 = s2;
+
+        k = len < VNMAX ? len : VNMAX;
+        len -= k;
+
+        /* get input data */
+        o_buf = buf;
+        buf = alignaddr(buf, 0);
+        in = *(const unsigned long long *)buf;
+        {
+            unsigned long long vs2t;
+            /* swizzle data in place */
+            in = faligndata(in, v0);
+            if(f) {
+                alignaddrl(o_buf, 0);
+                in = faligndata(v0, in);
+            }
+            /* add horizontal and acc */
+            vs1 = pdist(in, v0, vs1);
+            /* extract, apply order and acc */
+            vs2t = pextlbh(in);
+            vs2 = fpadd32(vs2, fpmul16x16x32_lo(vs2t, vorder_lo));
+            vs2 = fpadd32(vs2, fpmul16x16x32_hi(vs2t, vorder_lo));
+            vs2t = pexthbh(in);
+            vs2 = fpadd32(vs2, fpmul16x16x32_lo(vs2t, vorder_hi));
+            vs2 = fpadd32(vs2, fpmul16x16x32_hi(vs2t, vorder_hi));
+        }
+        buf += SOVV;
+        k   -= n;
+
+        if (likely(k >= SOVV)) do {
+            unsigned long long vs1_r = fzero();
+            do {
+                unsigned long long vs2l = fzero(), vs2h = fzero();
+                unsigned j;
+
+                j = (k/SOVV) > 127 ? 127 : k/SOVV;
+                k -= j * SOVV;
+                do {
+                    /* get input data */
+                    in = *(const unsigned long long *)buf;
+                    buf += SOVV;
+                    /* add vs1 for this round */
+                    vs1_r = fpadd32(vs1_r, vs1);
+                    /* add horizontal and acc */
+                    vs1 = pdist(in, v0, vs1);
+                    /* extract */
+                    vs2l = fpadd16(vs2l, pextlbh(in));
+                    vs2h = fpadd16(vs2h, pexthbh(in));
+                } while (--j);
+                vs2 = fpadd32(vs2, fpmul16x16x32_lo(vs2l, vorder_lo));
+                vs2 = fpadd32(vs2, fpmul16x16x32_hi(vs2l, vorder_lo));
+                vs2 = fpadd32(vs2, fpmul16x16x32_lo(vs2h, vorder_hi));
+                vs2 = fpadd32(vs2, fpmul16x16x32_hi(vs2h, vorder_hi));
+            } while (k >= SOVV);
+            /* chop vs1 round sum before multiplying by 8 */
+            vs1_r = vector_chop(vs1_r);
+            /* add vs1 for this round (8 times) */
+            vs1_r = fpadd32(vs1_r, vs1_r); /* *2 */
+            vs1_r = fpadd32(vs1_r, vs1_r); /* *4 */
+            vs1_r = fpadd32(vs1_r, vs1_r); /* *8 */
+            vs2   = fpadd32(vs2, vs1_r);
+            /* chop both sums */
+            vs2 = vector_chop(vs2);
+            vs1 = vector_chop(vs1);
+            len += k;
+            k = len < VNMAX ? len : VNMAX;
+            len -= k;
+        } while (likely(k >= SOVV));
+
+        if (likely(k)) {
+            /* handle trailer */
+            unsigned long long t, r, vs2t;
+            unsigned int k_m;
+
+            /* get input data */
+            in = *(const unsigned long long *)buf;
+
+            /* swizzle data in place */
+            alignaddr(buf, k);
+            in = faligndata(v0, in);
+
+            /* add k times vs1 for this trailer */
+            /* since VIS muls are painfull, use the russian peasant method, k is small */
+            t = 0;
+            r = vs1;
+            k_m = k;
+            do {
+                if(k_m & 1)
+                    t = fpadd32(t, r); /* add to result if odd */
+                r = fpadd32(r, r); /* *2 */
+                k_m >>= 1; /* /2 */
+            } while (k_m);
+            vs2 = fpadd32(vs2, t);
+
+            /* add horizontal and acc */
+            vs1 = pdist(in, v0, vs1);
+            /* extract, apply order and acc */
+            vs2t = pextlbh(in);
+            vs2  = fpadd32(vs2, fpmul16x16x32_lo(vs2t, vorder_lo));
+            vs2  = fpadd32(vs2, fpmul16x16x32_hi(vs2t, vorder_lo));
+            vs2t = pexthbh(in);
+            vs2  = fpadd32(vs2, fpmul16x16x32_lo(vs2t, vorder_hi));
+            vs2  = fpadd32(vs2, fpmul16x16x32_hi(vs2t, vorder_hi));
+
+            buf += k;
+            k   -= k;
+        }
+        /* vs1 is one giant 64 bit sum, but we only calc to 32 bit */
+        s1 = vs1;
+        /* add both vs2 sums */
+        s2 = (vs2 & 0xFFFFFFFFUL) + (vs2 >> 32);
+    }
+
+    if (unlikely(len)) do {
+        s1 += *buf++;
+        s2 += s1;
+    } while (--len);
+    /* at this point we should not have so big s1 & s2 */
+    MOD28(s1);
+    MOD28(s2);
+    /* return recombined sums */
+    return (s2 << 16) | s1;
+}
+#endif
diff -Naur a/tile/adler32.c b/tile/adler32.c
--- a/tile/adler32.c	1970-01-01 01:00:00.000000000 +0100
+++ b/tile/adler32.c	2015-08-16 05:57:02.000000000 +0200
@@ -0,0 +1,255 @@
+/*
+ * adler32.c -- compute the Adler-32 checksum of a data stream
+ *   Tile implementation
+ * Copyright (C) 1995-2007 Mark Adler
+ * Copyright (C) 2011 Jan Seiffert
+ * For conditions of distribution and use, see copyright notice in zlib.h
+ */
+
+/* @(#) $Id$ */
+
+#if defined(__GNUC__)
+#  if defined(__tilegx__)
+#    define HAVE_ADLER32_VEC
+#    define MIN_WORK 32
+// TODO: VNMAX could prop. be a little higher?
+#    define VNMAX (2*NMAX+((9*NMAX)/10))
+
+#    define SOUL (sizeof(unsigned long))
+
+/* ========================================================================= */
+local inline unsigned long v1ddotpua(unsigned long d, unsigned long a, unsigned long b)
+{
+    return __insn_v1ddotpua(d, a, b);
+}
+
+/* ========================================================================= */
+local inline unsigned long v4shl(unsigned long a, unsigned long b)
+{
+    return __insn_v4shl(a, b);
+}
+
+/* ========================================================================= */
+local inline unsigned long v4shru(unsigned long a, unsigned long b)
+{
+    return __insn_v4shru(a, b);
+}
+
+/* ========================================================================= */
+local inline unsigned long v4sub(unsigned long a, unsigned long b)
+{
+    return __insn_v4sub(a, b);
+}
+
+/* ========================================================================= */
+local inline unsigned long v4add(unsigned long a, unsigned long b)
+{
+    return __insn_v4add(a, b);
+}
+
+/* ========================================================================= */
+local inline unsigned long vector_chop(unsigned long x)
+{
+    unsigned long y;
+
+    y = v4shl(x, 16);
+    y = v4shru(y, 16);
+    x = v4shru(x, 16);
+    y = v4sub(y, x);
+    x = v4shl(x, 4);
+    x = v4add(x, y);
+    return x;
+}
+
+/* ========================================================================= */
+local noinline uLong adler32_vec(adler, buf, len)
+    uLong adler;
+    const Bytef *buf;
+    uInt len;
+{
+    unsigned int s1, s2;
+    unsigned k;
+
+    /* split Adler-32 into component sums */
+    s1 = adler & 0xffff;
+    s2 = (adler >> 16) & 0xffff;
+
+    /* align input */
+    k    = ALIGN_DIFF(buf, SOUL);
+    len -= k;
+    if (k) do {
+        s1 += *buf++;
+        s2 += s1;
+    } while (--k);
+
+    k = len < VNMAX ? len : VNMAX;
+    len -= k;
+    if (likely(k >= 2 * SOUL)) {
+        const unsigned long v1 = 0x0101010101010101ul;
+        unsigned long vs1 = s1, vs2 = s2;
+        unsigned long vorder;
+
+        if(host_is_bigendian())
+            vorder = 0x0807060504030201;
+        else
+            vorder = 0x0102030405060708;
+
+        do {
+            unsigned long vs1_r = 0;
+            do {
+                /* get input data */
+                unsigned long in = *(const unsigned long *)buf;
+                /* add vs1 for this round */
+                vs1_r += vs1; /* we don't overflow, so normal add */
+                /* add horizontal & acc vs1 */
+                vs1 = v1ddotpua(vs1, in, v1); /* only into x0 pipeline */
+                /* mul, add & acc vs2 */
+                vs2 = v1ddotpua(vs2, in, vorder); /* only in x0 pipe */
+                buf += SOUL;
+                k -= SOUL;
+            } while (k >= SOUL);
+            /* chop vs1 round sum before multiplying by 8 */
+            vs1_r = vector_chop(vs1_r);
+            /* add vs1 for this round (8 times) */
+            vs1_r = v4shl(vs1_r, 3);
+            vs2 += vs1_r;
+            /* chop both sums */
+            vs2 = vector_chop(vs2);
+            vs1 = vector_chop(vs1);
+            len += k;
+            k = len < VNMAX ? len : VNMAX;
+            len -= k;
+        } while (likely(k >= SOUL));
+        s1 = (vs1 & 0xffffffff) + (vs1 >> 32);
+        s2 = (vs2 & 0xffffffff) + (vs2 >> 32);
+    }
+
+    /* handle trailer */
+    if (unlikely(k)) do {
+        s1 += *buf++;
+        s2 += s1;
+    } while (--k);
+    /* at this point s1 & s2 should be in range */
+    MOD28(s1);
+    MOD28(s2);
+
+    /* return recombined sums */
+    return (s2 << 16) | s1;
+}
+#  else
+#    define HAVE_ADLER32_VEC
+#    define MIN_WORK 32
+// TODO: VNMAX could be higher
+#    define VNMAX NMAX
+
+#    define SOUL (sizeof(unsigned long))
+
+/* ========================================================================= */
+local inline unsigned long sadab_u(unsigned long d, unsigned long a, unsigned long b)
+{
+    return __insn_sadab_u(d, a, b);
+}
+
+/* ========================================================================= */
+local inline unsigned long inthb(unsigned long a, unsigned long b)
+{
+    return __insn_inthb(a, b);
+}
+
+/* ========================================================================= */
+local inline unsigned long intlb(unsigned long a, unsigned long b)
+{
+    return __insn_intlb(a, b);
+}
+
+/* ========================================================================= */
+local noinline uLong adler32_vec(adler, buf, len)
+    uLong adler;
+    const Bytef *buf;
+    uInt len;
+{
+    unsigned int s1, s2;
+    unsigned k;
+
+    /* split Adler-32 into component sums */
+    s1 = adler & 0xffff;
+    s2 = (adler >> 16) & 0xffff;
+
+    /* align input */
+    k    = ALIGN_DIFF(buf, SOUL);
+    len -= k;
+    if (k) do {
+        s1 += *buf++;
+        s2 += s1;
+    } while (--k);
+
+    k = len < VNMAX ? len : VNMAX;
+    len -= k;
+    if (likely(k >= 2 * SOUL)) {
+        unsigned long vs1 = s1, vs2 = s2;
+
+        do {
+            unsigned long vs1_r = 0;
+            do {
+                unsigned long a, b, c, d;
+                unsigned long vs2l = 0, vs2h = 0;
+                unsigned j;
+
+                j = k > 257 * SOUL ? 257 : k/SOUL;
+                k -= j * SOUL;
+                do {
+                    /* get input data */
+                    unsigned long in = *(const unsigned long *)buf;
+                    /* add vs1 for this round */
+                    vs1_r += vs1;
+                    /* add horizontal */
+                    vs1 = sadab_u(vs1, in, 0);
+                    /* extract */
+                    vs2l += intlb(0, in);
+                    vs2h += inthb(0, in);
+                    buf += SOUL;
+                } while (--j);
+                /* split vs2 */
+                if(host_is_bigendian()) {
+                    a = (vs2h >> 16) & 0x0000ffff;
+                    b = (vs2h      ) & 0x0000ffff;
+                    c = (vs2l >> 16) & 0x0000ffff;
+                    d = (vs2l      ) & 0x0000ffff;
+                } else {
+                    a = (vs2l      ) & 0x0000ffff;
+                    b = (vs2l >> 16) & 0x0000ffff;
+                    c = (vs2h      ) & 0x0000ffff;
+                    d = (vs2h >> 16) & 0x0000ffff;
+                }
+                /* mull&add vs2 horiz. */
+                vs2 += 4*a + 3*b + 2*c + 1*d;
+            } while (k >= SOUL);
+            /* reduce vs1 round sum before multiplying by 4 */
+            CHOP(vs1_r);
+            /* add vs1 for this round (4 times) */
+            vs2 += vs1_r * 4;
+            /* reduce both sums */
+            CHOP(vs2);
+            CHOP(vs1);
+            len += k;
+            k = len < VNMAX ? len : VNMAX;
+            len -= k;
+        } while (likely(k >= SOUL));
+        s1 = vs1;
+        s2 = vs2;
+    }
+
+    /* handle trailer */
+    if (unlikely(k)) do {
+        s1 += *buf++;
+        s2 += s1;
+    } while (--k);
+    /* at this point we should not have so big s1 & s2 */
+    MOD28(s1);
+    MOD28(s2);
+
+    /* return recombined sums */
+    return (s2 << 16) | s1;
+}
+#  endif
+#endif
diff -Naur a/tile/slhash.c b/tile/slhash.c
--- a/tile/slhash.c	1970-01-01 01:00:00.000000000 +0100
+++ b/tile/slhash.c	2015-08-16 05:57:02.000000000 +0200
@@ -0,0 +1,146 @@
+/* slhash.c -- slide the hash table during fill_window()
+ * Copyright (C) 1995-2010 Jean-loup Gailly and Mark Adler
+ * Copyright (C) 2011 Jan Seiffert
+ * For conditions of distribution and use, see copyright notice in zlib.h
+ */
+
+/* NOTE:
+ * We do not precheck the length or wsize for small values because
+ * we assume a minimum len of 256 (for MEM_LEVEL 1) and a minimum wsize
+ * of 256 for windowBits 8
+ */
+
+/* we use a bunch of inline asm, so GCC it is */
+#ifdef __GNUC__
+#  define HAVE_SLHASH_VEC
+#  define SOUL (sizeof(unsigned long))
+
+/* ========================================================================= */
+local inline unsigned long v2sub(unsigned long a, unsigned long b)
+{
+    unsigned long r;
+#  ifdef __tilegx__
+    r = __insn_v2sub(a, b);
+#  else
+    r = __insn_subh(a, b);
+#  endif
+    return r;
+}
+
+/* ========================================================================= */
+local inline unsigned long v2cmpltu(unsigned long a, unsigned long b)
+{
+    unsigned long r;
+#  ifdef __tilegx__
+    r =  __insn_v2cmpltu(a, b);
+#  else
+    r = __insn_slth_u(a, b);
+#  endif
+    return r;
+}
+
+/* ========================================================================= */
+local inline unsigned long v2mz(unsigned long a, unsigned long b)
+{
+    unsigned long r;
+#  ifdef __tilegx__
+    r = __insn_v2mz(a, b);
+#  else
+    r = __insn_mzh(a, b);
+#  endif
+    return r;
+}
+
+/* ========================================================================= */
+local void update_hoffset_m(Posf *p, uInt wsize, unsigned n)
+{
+    unsigned long vwsize;
+    unsigned int i, f;
+
+    vwsize = (unsigned short)wsize;
+    vwsize = (vwsize << 16) | (vwsize & 0x0000ffff);
+    if (SOUL > 4)
+        vwsize = ((vwsize << 16) << 16) | (vwsize & 0xffffffff);
+
+    /* align */
+    f = (unsigned)ALIGN_DIFF(p, SOUL);
+    if (unlikely(f)) {
+        f /= sizeof(*p);
+        n -= f;
+        if (f & 1) {
+            register unsigned m = *p;
+            *p++ = (Pos)(m >= wsize ? m-wsize : NIL);
+            f--;
+        }
+        if (SOUL > 4 && f >= 2) {
+            unsigned int m = *(unsigned int *)p;
+            m = v2mz(v2cmpltu(m, vwsize), v2sub(m, vwsize));
+            *(unsigned int *)p = m;
+            p += 2;
+        }
+    }
+
+    /* do it */
+    i  = n / (SOUL/sizeof(*p));
+    n %= SOUL/sizeof(*p);
+    if (i & 1) {
+            unsigned long m = *(unsigned long *)p;
+            m = v2mz(v2cmpltu(m, vwsize), v2sub(m, vwsize));
+            *(unsigned long *)p = m;
+            p += SOUL/sizeof(*p);
+            i--;
+    }
+    i /= 2;
+    do {
+            unsigned long m1 = ((unsigned long *)p)[0];
+            unsigned long m2 = ((unsigned long *)p)[1];
+            unsigned long mask1, mask2;
+            mask1 = v2sub(m1, vwsize);
+            m1    = v2cmpltu(m1, vwsize);
+            mask2 = v2sub(m2, vwsize);
+            m2    = v2cmpltu(m2, vwsize);
+            m1    = v2mz(m1, mask1);
+            m2    = v2mz(m2, mask2);
+            ((unsigned long *)p)[0] = m1;
+            ((unsigned long *)p)[1] = m2;
+            p += 2*(SOUL/sizeof(*p));
+    } while (--i);
+
+    /* handle trailer */
+    if (unlikely(n)) {
+        if (SOUL > 4 && n >= 2) {
+            unsigned int m = *(unsigned int *)p;
+            m = v2mz(v2cmpltu(m, vwsize), v2sub(m, vwsize));
+            *(unsigned int *)p = m;
+            p += 2;
+            n -= 2;
+        }
+        if (n & 1) {
+            register unsigned m = *p;
+            *p++ = (Pos)(m >= wsize ? m-wsize : NIL);
+        }
+    }
+}
+
+/* ========================================================================= */
+local void update_hoffset_l(Posf *p, uInt wsize, unsigned n)
+{
+    do {
+        register unsigned m = *p;
+        *p++ = (Pos)(m >= wsize ? m-wsize : NIL);
+    } while (--n);
+}
+
+/* ========================================================================= */
+local inline void update_hoffset(Posf *p, uInt wsize, unsigned n)
+{
+    /*
+     * Unfortunatly most chip designer prefer signed saturation...
+     * So we are better off with a parralel compare and move/mask
+     */
+    if (likely(2 == sizeof(*p) && wsize < (1<<16)))
+        update_hoffset_m(p, wsize, n);
+    else
+        update_hoffset_l(p, wsize, n);
+}
+#endif
diff -Naur a/x86/adler32.c b/x86/adler32.c
--- a/x86/adler32.c	1970-01-01 01:00:00.000000000 +0100
+++ b/x86/adler32.c	2015-08-16 05:57:02.000000000 +0200
@@ -0,0 +1,1186 @@
+/*
+ * adler32.c -- compute the Adler-32 checksum of a data stream
+ *   x86 implementation
+ * Copyright (C) 1995-2007 Mark Adler
+ * Copyright (C) 2009-2012 Jan Seiffert
+ * For conditions of distribution and use, see copyright notice in zlib.h
+ */
+
+/* @(#) $Id$ */
+
+#include "x86.h"
+
+#if GCC_VERSION_GE(203)
+#  define GCC_ATTR_ALIGNED(x) __attribute__((__aligned__(x)))
+#else
+#  define VEC_NO_GO
+#endif
+
+/* inline asm, so only on GCC (or compatible) */
+#if defined(__GNUC__) && !defined(VEC_NO_GO)
+#  define HAVE_ADLER32_VEC
+#  define MIN_WORK 64
+
+/* ========================================================================= */
+local const struct { short d[24]; } vord GCC_ATTR_ALIGNED(16) = {
+    {1,1,1,1,1,1,1,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1}
+};
+
+/* ========================================================================= */
+local const struct { char d[16]; } vord_b GCC_ATTR_ALIGNED(16) = {
+    {16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1}
+};
+
+/* ========================================================================= */
+local noinline const Bytef *adler32_jumped(buf, s1, s2, k)
+    const Bytef *buf;
+    unsigned int *s1;
+    unsigned int *s2;
+    unsigned int k;
+{
+    unsigned int t;
+    unsigned n = k % 16;
+    buf += n;
+    k = (k / 16) + 1;
+
+    __asm__ __volatile__ (
+#  ifdef __x86_64__
+#    define CLOB "&"
+            "lea	1f(%%rip), %q4\n\t"
+            "lea	(%q4,%q5,8), %q4\n\t"
+            "jmp	*%q4\n\t"
+#  else
+#    ifndef __PIC__
+#      define CLOB
+            "lea	1f(,%5,8), %4\n\t"
+#    else
+#      define CLOB
+            "lea	1f-3f(,%5,8), %4\n\t"
+            "call	9f\n"
+            "3:\n\t"
+#    endif
+            "jmp	*%4\n\t"
+#    ifdef __PIC__
+            ".p2align 1\n"
+            "9:\n\t"
+            "addl	(%%esp), %4\n\t"
+            "ret\n\t"
+#    endif
+#  endif
+            ".p2align 1\n"
+            "2:\n\t"
+#  ifdef __i386
+            ".byte 0x3e\n\t"
+#  endif
+            "add	$0x10, %2\n\t"
+            ".p2align 1\n"
+            "1:\n\t"
+            /* 128 */
+            "movzbl	-16(%2), %4\n\t"	/* 4 */
+            "add	%4, %0\n\t"	/* 2 */
+            "add	%0, %1\n\t"	/* 2 */
+            /* 120 */
+            "movzbl	-15(%2), %4\n\t"	/* 4 */
+            "add	%4, %0\n\t"	/* 2 */
+            "add	%0, %1\n\t"	/* 2 */
+            /* 112 */
+            "movzbl	-14(%2), %4\n\t"	/* 4 */
+            "add	%4, %0\n\t"	/* 2 */
+            "add	%0, %1\n\t"	/* 2 */
+            /* 104 */
+            "movzbl	-13(%2), %4\n\t"	/* 4 */
+            "add	%4, %0\n\t"	/* 2 */
+            "add	%0, %1\n\t"	/* 2 */
+            /*  96 */
+            "movzbl	-12(%2), %4\n\t"	/* 4 */
+            "add	%4, %0\n\t"	/* 2 */
+            "add	%0, %1\n\t"	/* 2 */
+            /*  88 */
+            "movzbl	-11(%2), %4\n\t"	/* 4 */
+            "add	%4, %0\n\t"	/* 2 */
+            "add	%0, %1\n\t"	/* 2 */
+            /*  80 */
+            "movzbl	-10(%2), %4\n\t"	/* 4 */
+            "add	%4, %0\n\t"	/* 2 */
+            "add	%0, %1\n\t"	/* 2 */
+            /*  72 */
+            "movzbl	 -9(%2), %4\n\t"	/* 4 */
+            "add	%4, %0\n\t"	/* 2 */
+            "add	%0, %1\n\t"	/* 2 */
+            /*  64 */
+            "movzbl	 -8(%2), %4\n\t"	/* 4 */
+            "add	%4, %0\n\t"	/* 2 */
+            "add	%0, %1\n\t"	/* 2 */
+            /*  56 */
+            "movzbl	 -7(%2), %4\n\t"	/* 4 */
+            "add	%4, %0\n\t"	/* 2 */
+            "add	%0, %1\n\t"	/* 2 */
+            /*  48 */
+            "movzbl	 -6(%2), %4\n\t"	/* 4 */
+            "add	%4, %0\n\t"	/* 2 */
+            "add	%0, %1\n\t"	/* 2 */
+            /*  40 */
+            "movzbl	 -5(%2), %4\n\t"	/* 4 */
+            "add	%4, %0\n\t"	/* 2 */
+            "add	%0, %1\n\t"	/* 2 */
+            /*  32 */
+            "movzbl	 -4(%2), %4\n\t"	/* 4 */
+            "add	%4, %0\n\t"	/* 2 */
+            "add	%0, %1\n\t"	/* 2 */
+            /*  24 */
+            "movzbl	 -3(%2), %4\n\t"	/* 4 */
+            "add	%4, %0\n\t"	/* 2 */
+            "add	%0, %1\n\t"	/* 2 */
+            /*  16 */
+            "movzbl	 -2(%2), %4\n\t"	/* 4 */
+            "add	%4, %0\n\t"	/* 2 */
+            "add	%0, %1\n\t"	/* 2 */
+            /*   8 */
+            "movzbl	 -1(%2), %4\n\t"	/* 4 */
+            "add	%4, %0\n\t"	/* 2 */
+            "add	%0, %1\n\t"	/* 2 */
+            /*   0 */
+            "dec	%3\n\t"
+            "jnz	2b"
+        : /* %0 */ "=R" (*s1),
+          /* %1 */ "=R" (*s2),
+          /* %2 */ "=abdSD" (buf),
+          /* %3 */ "=c" (k),
+          /* %4 */ "="CLOB"R" (t)
+        : /* %5 */ "r" (16 - n),
+          /*    */ "0" (*s1),
+          /*    */ "1" (*s2),
+          /*    */ "2" (buf),
+          /*    */ "3" (k)
+        : "cc", "memory"
+    );
+
+    return buf;
+}
+
+
+
+#  if 0 && (HAVE_BINUTILS-0) >= 222
+        /*
+         * 2013 Intel will hopefully bring the Haswell CPUs,
+         * which hopefully will have AVX2, which brings integer
+         * ops to the full width AVX regs.
+         */
+        "2:\n\t"
+        "mov	$256, %1\n\t"
+        "cmp	%1, %3\n\t"
+        "cmovb	%3, %1\n\t"
+        "and	$-32, %1\n\t"
+        "sub	%1, %3\n\t"
+        "shr	$5, %1\n\t"
+        "vpxor	%%xmm6, %%xmm6\n\t"
+        ".p2align 4,,7\n"
+        ".p2align 3\n"
+        "1:\n\t"
+        "vmovdqa	(%0), %%ymm0\n\t"
+        "prefetchnta	0x70(%0)\n\t"
+        "vpaddd	%%ymm3, %%ymm7, %%ymm7\n\t"
+        "add	$32, %0\n\t"
+        "dec	%1\n\t"
+        "vpsadbw	%%ymm4, %%ymm0, %%ymm1\n\t"
+        "vpmaddubsw	%%ymm5, %%ymm0, %%ymm0\n\t"
+        "vpaddd	%%ymm1, %%ymm3, %%ymm3\n\t"
+        "vpaddw	%%ymm0, %%ymm6, %%ymm6\n\t"
+        "jnz	1b\n\t"
+        "vpunpckhwd	%%ymm4, %%ymm6, %%xmm0\n\t"
+        "vpunpcklwd	%%ymm4, %%ymm6, %%ymm6\n\t"
+        "vpaddd	%%ymm0, %%ymm2, %%ymm2\n\t"
+        "vpaddd	%%ymm6, %%ymm2, %%ymm2\n\t"
+        "cmp	$32, %3\n\t"
+        "jg	2b\n\t"
+        avx2_chop
+        ...
+#  endif
+
+#  if 0
+        /*
+         * Will XOP processors have SSSE3/AVX??
+         * And what is the unaligned load performance?
+         */
+        "prefetchnta	0x70(%0)\n\t"
+        "lddqu	(%0), %%xmm0\n\t"
+        "vpaddd	%%xmm3, %%xmm5, %%xmm5\n\t"
+        "sub	$16, %3\n\t"
+        "add	$16, %0\n\t"
+        "cmp	$15, %3\n\t"
+        "vphaddubd	%%xmm0, %%xmm1\n\t" /* A */
+        "vpmaddubsw %%xmm4, %%xmm0, %%xmm0\n\t"/* AVX! */ /* 1 */
+        "vphadduwd	%%xmm0, %%xmm0\n\t" /* 2 */
+        "vpaddd	%%xmm1, %%xmm3, %%xmm3\n\t" /* B: A+B => hadd+acc or vpmadcubd w. mul = 1 */
+        "vpaddd	%%xmm0, %%xmm2, %%xmm2\n\t" /* 3: 1+2+3 => vpmadcubd w. mul = 16,15,14... */
+        "jg	1b\n\t"
+        xop_chop
+        xop_chop
+        xop_chop
+        setup
+        "jg	1b\n\t"
+        "vphaddudq	%%xmm2, %%xmm0\n\t"
+        "vphaddudq	%%xmm3, %%xmm1\n\t"
+        "pshufd	$0xE6, %%xmm0, %%xmm2\n\t"
+        "pshufd	$0xE6, %%xmm1, %%xmm3\n\t"
+        "paddd	%%xmm0, %%xmm2\n\t"
+        "paddd	%%xmm1, %%xmm3\n\t"
+        "movd	%%xmm2, %2\n\t"
+        "movd	%%xmm3, %1\n\t"
+#  endif
+
+/* ========================================================================= */
+#  define TO_STR2(x) #x
+#  define TO_STR(x) TO_STR2(x)
+#  if (HAVE_BINUTILS-0) >= 217
+#   define PMADDUBSW_XMM5_XMM0 "pmaddubsw	%%xmm5, %%xmm0\n\t"
+#   define PALIGNR_XMM0_XMM0(o) "palignr	$("TO_STR(o)"), %%xmm0, %%xmm0\n\t"
+#   define PALIGNR_XMM1_XMM0(o) "palignr	$("TO_STR(o)"), %%xmm1, %%xmm0\n\t"
+#  else
+#   define PMADDUBSW_XMM5_XMM0 ".byte 0x66, 0x0f, 0x38, 0x04, 0xc5\n\t"
+#   define PALIGNR_XMM0_XMM0(o) ".byte 0x66, 0x0f, 0x3a, 0x0f, 0xc0, "TO_STR(o)"\n\t"
+#   define PALIGNR_XMM1_XMM0(o) ".byte 0x66, 0x0f, 0x3a, 0x0f, 0xc1, "TO_STR(o)"\n\t"
+#  endif
+#  ifdef __x86_64__
+#   define NREG "q"
+#  else
+#   define NREG ""
+#  endif
+local uLong adler32_SSSE3(adler, buf, len)
+    uLong adler;
+    const Bytef *buf;
+    uInt len;
+{
+    unsigned int s1 = adler & 0xffff;
+    unsigned int s2 = (adler >> 16) & 0xffff;
+    unsigned int k,t;
+
+    __asm__ __volatile__ (
+            "jb	9f\n\t"				/* if(k < 32) goto OUT */
+            "prefetchnta	0x70(%0)\n\t"
+            "movdqa	%6, %%xmm5\n\t"		/* get vord_b */
+            "movd	%k2, %%xmm2\n\t"	/* init vector sum vs2 with s2 */
+            "movd	%k1, %%xmm3\n\t"	/* init vector sum vs1 with s1 */
+            "pxor	%%xmm4, %%xmm4\n"	/* zero */
+            "mov	%k0, %k3\n\t"		/* copy buf pointer */
+            "and	$-16, %0\n\t"		/* align buf pointer down */
+            "xor	%k0, %k3\n\t"		/* extract num_misaligned */
+            "jz	4f\n\t"				/* no misalignment? goto start of loop */
+            "movdqa	(%0), %%xmm0\n\t"	/* fetch input data as first arg */
+            "pxor	%%xmm1, %%xmm1\n\t"	/* clear second arg */
+            "mov	%3, %5\n\t"		/* copy num_misaligned as last arg */
+            "call	ssse3_align_l\n"	/* call ssse3 left alignment helper */
+            "sub	$16, %k3\n\t"		/* build valid bytes ... */
+            "add	$16, %0\n\t"		/* advance input data pointer */
+            "neg	%k3\n\t"		/* ... out of num_misaligned */
+            "sub	%k3, %k4\n\t"		/* len -= valid bytes */
+            "imul	%k1, %k3\n\t"		/* valid_bytes *= s1 */
+            "movd	%k3, %%xmm1\n\t"	/* transfer to sse */
+            "pslldq	$12, %%xmm1\n\t"	/* mov into high dword */
+            "por	%%xmm1, %%xmm2\n\t"	/* stash s1 times valid_bytes in vs2 */
+            "movdqa	%%xmm0, %%xmm1\n\t"	/* make a copy of the input data */
+            PMADDUBSW_XMM5_XMM0		/* multiply all input bytes by vord_b bytes, add adjecent results to words */
+            "psadbw	%%xmm4, %%xmm1\n\t"	/* subtract zero from every byte, add 8 bytes to a sum */
+            "movdqa	%%xmm0, %%xmm6\n\t"	/* copy word mul result */
+            "paddd	%%xmm1, %%xmm3\n\t"	/* vs1 += psadbw */
+            "punpckhwd	%%xmm4, %%xmm0\n\t"	/* zero extent upper words to dwords */
+            "punpcklwd	%%xmm4, %%xmm6\n\t"	/* zero extent lower words to dwords */
+            "paddd	%%xmm0, %%xmm2\n\t"	/* vs2 += vs2_i.upper */
+            "paddd	%%xmm6, %%xmm2\n\t"	/* vs2 += vs2_i.lower */
+            "jmp	4f\n\t"
+#  if defined(__ELF__) && !defined(__clang__)
+            ".subsection 2\n\t"
+#  endif
+            ".p2align 2\n"
+            /*
+             * helper function to align data in sse reg
+             * will keep %5 bytes in xmm0 on the left
+             * and fill the rest with bytes from xmm1
+             * (note: little-endian -> left == higher addr)
+             */
+            "ssse3_align_l:\n\t"
+            "sub	$1, %5\n\t"	/* k -= 1 */
+            "shl	$4, %5\n\t"	/* k *= 16 */
+#  ifdef __x86_64__
+            "lea	7f(%%rip), %%r11\n\t"
+            "lea	(%%r11,%q5, 1), %q5\n\t"
+            "jmp	*%q5\n\t"
+#  else
+#   ifndef __PIC__
+            "lea	7f(%5), %5\n\t"
+#   else
+            "lea	7f-1f(%5), %5\n\t"
+            "call	2f\n\t"
+            "1:\n\t"
+#   endif
+            "jmp	*%5\n\t"
+#  endif
+            ".p2align 2\n"
+            /*
+             * helper function to align data in sse reg
+             * will push %5 bytes in xmm0 to the left
+             * and fill the rest with bytes from xmm1
+             * (note: little-endian -> left == higher addr)
+             */
+            "ssse3_align_r:\n\t"
+            "sub	$15, %5\n\t"
+            "neg	%5\n\t"		/* k = 16 - (k - 1) */
+            "shl	$4, %5\n\t"	/* k *= 16 */
+#  ifdef __x86_64__
+            "lea	6f(%%rip), %%r11\n\t"
+            "lea	(%%r11,%q5, 1), %q5\n\t"
+            "jmp	*%q5\n\t"
+#  else
+#   ifndef __PIC__
+            "lea	6f(%5), %5\n\t"
+#   else
+            "lea	6f-1f(%5), %5\n\t"
+            "call	2f\n\t"
+            "1:\n\t"
+#   endif
+            "jmp	*%5\n\t"
+#   ifdef __PIC__
+            "2:\n\t"
+            "addl	(%%esp), %5\n\t"
+            "ret\n\t"
+#   endif
+#  endif
+
+#  define ALIGNSTEP(o) \
+    PALIGNR_XMM0_XMM0(o) \
+    ".p2align 3\n\t" \
+    PALIGNR_XMM1_XMM0(16-o) \
+    "ret\n\t" \
+    ".p2align 4\n\t"
+
+            ".p2align 4\n"
+            "7:\n\t"		/* 0 */
+            PALIGNR_XMM0_XMM0(1) /* 6 */
+            ".p2align 3\n\t" /* 2 */
+            "6:\n\t"
+            PALIGNR_XMM1_XMM0(16-1) /* 6 */
+            "ret\n\t" /* 1 */
+            ".p2align 4\n\t"	/* 16 */
+            ALIGNSTEP(2)	/* 32 */
+            ALIGNSTEP(3)	/* 48 */
+            ALIGNSTEP(4)	/* 64 */
+            ALIGNSTEP(5)	/* 80 */
+            ALIGNSTEP(6)	/* 96 */
+            ALIGNSTEP(7)	/* 112 */
+            ALIGNSTEP(8)	/* 128 */
+            ALIGNSTEP(9)	/* 144 */
+            ALIGNSTEP(10)	/* 160 */
+            ALIGNSTEP(11)	/* 176 */
+            ALIGNSTEP(12)	/* 192 */
+            ALIGNSTEP(13)	/* 208 */
+            ALIGNSTEP(14)	/* 224 */
+            ALIGNSTEP(15)	/* 256 */
+#  undef ALIGNSTEP
+            ".p2align 2\n"
+            /*
+             * reduction function to bring a vector sum within the range of BASE
+             * This does no full reduction! When the sum is large, a number > BASE
+             * is the result. To do a full reduction call multiple times.
+             */
+            "sse2_chop:\n\t"
+            "movdqa	%%xmm0, %%xmm1\n\t"	/* y = x */
+            "pslld	$16, %%xmm1\n\t"	/* y <<= 16 */
+            "psrld	$16, %%xmm0\n\t"	/* x >>= 16 */
+            "psrld	$16, %%xmm1\n\t"	/* y >>= 16 */
+            "psubd	%%xmm0, %%xmm1\n\t"	/* y -= x */
+            "pslld	$4, %%xmm0\n\t"		/* x <<= 4 */
+            "paddd	%%xmm1, %%xmm0\n\t"	/* x += y */
+            "ret\n\t"
+#  if defined(__ELF__) && !defined(__clang__)
+            ".previous\n\t"
+#  endif
+            ".p2align 2\n"
+            "6:\n\t"
+            "mov	$128, %1\n\t"		/* inner_k = 128 bytes till vs2_i overflows */
+            "cmp	%1, %3\n\t"
+            "cmovb	%3, %1\n\t"		/* inner_k = k >= inner_k ? inner_k : k */
+            "and	$-16, %1\n\t"		/* inner_k = ROUND_TO(inner_k, 16) */
+            "sub	%1, %3\n\t"		/* k -= inner_k */
+            "shr	$4, %1\n\t"		/* inner_k /= 16 */
+            "mov	$1, %5\n\t"		/* outer_k = 1 */
+            "jmp	5f\n"			/* goto loop start */
+            "3:\n\t"
+            "pxor	%%xmm7, %%xmm7\n\t"	/* zero vs1_round_sum */
+            "mov	%3, %5\n\t"		/* determine full inner_k (==8) rounds from k */
+            "and	$-128, %5\n\t"		/* outer_k = ROUND_TO(outer_k, 128) */
+            "sub	%5, %3\n\t"		/* k -= outer_k */
+            "shr	$7, %5\n\t"		/* outer_k /= 128 */
+            "jz         6b\n\t"			/* if outer_k == 0 handle trailer */
+            ".p2align 3,,3\n\t"
+            ".p2align 2\n"
+            "2:\n\t"
+            "mov	$8, %1\n"		/* inner_k = 8 */
+            "5:\n\t"
+            "pxor	%%xmm6, %%xmm6\n\t"	/* zero vs2_i */
+            ".p2align 4,,7\n"
+            ".p2align 3\n"
+            "1:\n\t"
+            "movdqa	(%0), %%xmm0\n\t"	/* fetch input data */
+            "prefetchnta	0x70(%0)\n\t"
+            "paddd	%%xmm3, %%xmm7\n\t"	/* vs1_round_sum += vs1 */
+            "add	$16, %0\n\t"		/* advance input data pointer */
+            "dec	%1\n\t"			/* decrement inner_k */
+            "movdqa	%%xmm0, %%xmm1\n\t"	/* make a copy of the input data */
+            PMADDUBSW_XMM5_XMM0		/* multiply all input bytes by vord_b bytes, add adjecent results to words */
+            "psadbw	%%xmm4, %%xmm1\n\t"	/* subtract zero from every byte, add 8 bytes to a sum */
+            "paddw	%%xmm0, %%xmm6\n\t"	/* vs2_i += in * vorder_b */
+            "paddd	%%xmm1, %%xmm3\n\t"	/* vs1 += psadbw */
+            "jnz	1b\n\t"			/* repeat if inner_k != 0 */
+            "movdqa	%%xmm6, %%xmm0\n\t"	/* copy vs2_i */
+            "punpckhwd	%%xmm4, %%xmm0\n\t"	/* zero extent vs2_i upper words to dwords */
+            "punpcklwd	%%xmm4, %%xmm6\n\t"	/* zero extent vs2_i lower words to dwords */
+            "paddd	%%xmm0, %%xmm2\n\t"	/* vs2 += vs2_i.upper */
+            "paddd	%%xmm6, %%xmm2\n\t"	/* vs2 += vs2_i.lower */
+            "dec	%5\n\t"			/* decrement outer_k  */
+            "jnz	2b\n\t"			/* repeat with inner_k = 8 if outer_k != 0 */
+            "cmp	$15, %3\n\t"
+            "ja	6b\n\t"				/* if(k > 15) repeat */
+            "movdqa	%%xmm7, %%xmm0\n\t"	/* move vs1_round_sum */
+            "call	sse2_chop\n\t"		/* chop vs1_round_sum */
+            "pslld	$4, %%xmm0\n\t"		/* vs1_round_sum *= 16 */
+            "paddd	%%xmm2, %%xmm0\n\t"	/* vs2 += vs1_round_sum */
+            "call	sse2_chop\n\t"		/* chop again */
+            "movdqa	%%xmm0, %%xmm2\n\t"	/* move vs2 back in place */
+            "movdqa	%%xmm3, %%xmm0\n\t"	/* move vs1 */
+            "call	sse2_chop\n\t"		/* chop */
+            "movdqa	%%xmm0, %%xmm3\n\t"	/* move vs1 back in place */
+            "add	%3, %4\n"		/* len += k */
+            "4:\n\t"	/* top of loop */
+            "mov	%7, %3\n\t"		/* get max. byte count VNMAX till v1_round_sum overflows */
+            "cmp	%3, %4\n\t"
+            "cmovb	%4, %3\n\t"		/* k = len >= VNMAX ? k : len */
+            "sub	%3, %4\n\t"		/* len -= k */
+            "cmp	$15, %3\n\t"
+            "ja	3b\n\t"				/* if(k > 15) repeat */
+            "test	%k3, %k3\n\t"		/* test for 0 */
+            "jz	5f\n\t"				/* if (k == 0) goto OUT */
+            "movdqa	(%0), %%xmm0\n\t"	/* fetch remaining input data as first arg */
+            "add	%"NREG"3, %"NREG"0\n\t"		/* add remainder to to input data pointer */
+            "movdqa	%%xmm4, %%xmm1\n\t"	/* set second arg 0 */
+            "mov	%k3, %k5\n\t"		/* copy remainder as last arg */
+            "call	ssse3_align_r\n\t"	/* call ssse3 right alignment helper */
+            "movdqa	%%xmm4, %%xmm7\n\t"	/* sum = 0 */
+            "movdqa	%%xmm3, %%xmm1\n\t"	/* t = vs1 */
+            /* russian peasant mul, k is small */
+            ".p2align 2\n"
+            "6:\n\t"
+            "shr	$1, %k3\n\t"		/* k >>= 1 */
+            "jnc	7f\n\t"
+            "paddd	%%xmm1, %%xmm7\n\t"	/* add t to sum if 1 bit shifted out of k */
+            "7:\n\t"
+            "paddd	%%xmm1, %%xmm1\n\t"	/* t *= 2 */
+            "jnz	6b\n\t"			/* while(k != 0) */
+            "paddd	%%xmm7, %%xmm2\n\t"	/* vs2 += k * vs1 */
+            "movdqa	%%xmm0, %%xmm1\n\t"	/* make a copy of the input data */
+            PMADDUBSW_XMM5_XMM0		/* multiply all input bytes by vord_b bytes, add adjecent results to words */
+            "psadbw	%%xmm4, %%xmm1\n\t"	/* subtract zero from every byte, add 8 bytes to a sum */
+            "movdqa	%%xmm0, %%xmm6\n\t"	/* copy word mul result */
+            "paddd	%%xmm1, %%xmm3\n\t"	/* vs1 += psadbw */
+            "punpckhwd	%%xmm4, %%xmm0\n\t"	/* zero extent upper words to dwords */
+            "punpcklwd	%%xmm4, %%xmm6\n\t"	/* zero extent lower words to dwords */
+            "paddd	%%xmm0, %%xmm2\n\t"	/* vs2 += vs2_i.upper */
+            "paddd	%%xmm6, %%xmm2\n\t"	/* vs2 += vs2_i.lower */
+            "5:\n\t"	/* OUT */
+            "pshufd	$0xEE, %%xmm3, %%xmm1\n\t"	/* collect vs1 & vs2 in lowest vector member */
+            "pshufd	$0xEE, %%xmm2, %%xmm0\n\t"
+            "paddd	%%xmm3, %%xmm1\n\t"
+            "paddd	%%xmm2, %%xmm0\n\t"
+            "pshufd	$0xE5, %%xmm0, %%xmm2\n\t"
+            "paddd	%%xmm0, %%xmm2\n\t"
+            "movd	%%xmm1, %1\n\t"		/* mov vs1 to s1 */
+            "movd	%%xmm2, %2\n"		/* mov vs2 to s2 */
+            "9:"
+        : /* %0 */ "=r" (buf),
+          /* %1 */ "=r" (s1),
+          /* %2 */ "=r" (s2),
+          /* %3 */ "=r" (k),
+          /* %4 */ "=r" (len),
+          /* %5 */ "=r" (t)
+        : /* %6 */ "m" (vord_b),
+          /*
+           * somewhere between 5 & 6, psadbw 64 bit sums ruin the party
+           * spreading the sums with palignr only brings it to 7 (?),
+           * while introducing an op into the main loop (2800 ms -> 3200 ms)
+           */
+          /* %7 */ "i" (5*NMAX),
+          /*    */ "0" (buf),
+          /*    */ "1" (s1),
+          /*    */ "2" (s2),
+          /*    */ "4" (len)
+        : "cc", "memory"
+#  ifdef __x86_64__
+          , "r11"
+#  endif
+#  ifdef __SSE__
+          , "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7"
+#  endif
+    );
+
+    if (unlikely(k))
+        buf = adler32_jumped(buf, &s1, &s2, k);
+    MOD28(s1);
+    MOD28(s2);
+    return (s2 << 16) | s1;
+}
+#  undef PMADDUBSW_XMM5_XMM0
+#  undef PALIGNR_XMM0_XMM0
+#  undef PALIGNR_XMM1_XMM0
+#  undef NREG
+
+/* ========================================================================= */
+local uLong adler32_SSE2(adler, buf, len)
+    uLong adler;
+    const Bytef *buf;
+    uInt len;
+{
+    unsigned int s1 = adler & 0xffff;
+    unsigned int s2 = (adler >> 16) & 0xffff;
+    unsigned int k;
+
+    k    = ALIGN_DIFF(buf, 16);
+    len -= k;
+    if (k)
+        buf = adler32_jumped(buf, &s1, &s2, k);
+
+    __asm__ __volatile__ (
+            "mov	%6, %3\n\t"
+            "cmp	%3, %4\n\t"
+            "cmovb	%4, %3\n\t"
+            "sub	%3, %4\n\t"
+            "cmp	$16, %3\n\t"
+            "jb	8f\n\t"
+            "prefetchnta	0x70(%0)\n\t"
+            "movd	%1, %%xmm4\n\t"
+            "movd	%2, %%xmm3\n\t"
+            "pxor	%%xmm2, %%xmm2\n\t"
+            "pxor	%%xmm5, %%xmm5\n\t"
+            ".p2align 2\n"
+            "3:\n\t"
+            "pxor	%%xmm6, %%xmm6\n\t"
+            "pxor	%%xmm7, %%xmm7\n\t"
+            "mov	$2048, %1\n\t"		/* get byte count till vs2_{l|h}_word overflows */
+            "cmp	%1, %3\n\t"
+            "cmovb	%3, %1\n"
+            "and	$-16, %1\n\t"
+            "sub	%1, %3\n\t"
+            "shr	$4, %1\n\t"
+            ".p2align 4,,7\n"
+            ".p2align 3\n"
+            "1:\n\t"
+            "prefetchnta	0x70(%0)\n\t"
+            "movdqa	(%0), %%xmm0\n\t"	/* fetch input data */
+            "paddd	%%xmm4, %%xmm5\n\t"	/* vs1_round_sum += vs1 */
+            "add	$16, %0\n\t"
+            "dec	%1\n\t"
+            "movdqa	%%xmm0, %%xmm1\n\t"	/* copy input data */
+            "psadbw	%%xmm2, %%xmm0\n\t"	/* add all bytes horiz. */
+            "paddd	%%xmm0, %%xmm4\n\t"	/* add that to vs1 */
+            "movdqa	%%xmm1, %%xmm0\n\t"	/* copy input data */
+            "punpckhbw	%%xmm2, %%xmm1\n\t"	/* zero extent input upper bytes to words */
+            "punpcklbw	%%xmm2, %%xmm0\n\t"	/* zero extent input lower bytes to words */
+            "paddw	%%xmm1, %%xmm7\n\t"	/* vs2_h_words += in_high_words */
+            "paddw	%%xmm0, %%xmm6\n\t"	/* vs2_l_words += in_low_words */
+            "jnz	1b\n\t"
+            "cmp	$15, %3\n\t"
+            "pmaddwd	32+%5, %%xmm7\n\t"	/* multiply vs2_h_words with order, add adjecend results */
+            "pmaddwd	16+%5, %%xmm6\n\t"	/* multiply vs2_l_words with order, add adjecend results */
+            "paddd	%%xmm7, %%xmm3\n\t"	/* add to vs2 */
+            "paddd	%%xmm6, %%xmm3\n\t"	/* add to vs2 */
+            "jg	3b\n\t"
+            "movdqa	%%xmm5, %%xmm0\n\t"
+            "pxor	%%xmm5, %%xmm5\n\t"
+            "call	sse2_chop\n\t"
+            "pslld	$4, %%xmm0\n\t"
+            "paddd	%%xmm3, %%xmm0\n\t"
+            "call	sse2_chop\n\t"
+            "movdqa	%%xmm0, %%xmm3\n\t"
+            "movdqa	%%xmm4, %%xmm0\n\t"
+            "call	sse2_chop\n\t"
+            "movdqa	%%xmm0, %%xmm4\n\t"
+            "add	%3, %4\n\t"
+            "mov	%6, %3\n\t"
+            "cmp	%3, %4\n\t"
+            "cmovb	%4, %3\n"
+            "sub	%3, %4\n\t"
+            "cmp	$15, %3\n\t"
+            "jg	3b\n\t"
+            "pshufd	$0xEE, %%xmm4, %%xmm1\n\t"
+            "pshufd	$0xEE, %%xmm3, %%xmm0\n\t"
+            "paddd	%%xmm4, %%xmm1\n\t"
+            "paddd	%%xmm3, %%xmm0\n\t"
+            "pshufd	$0xE5, %%xmm0, %%xmm3\n\t"
+            "paddd	%%xmm0, %%xmm3\n\t"
+            "movd	%%xmm1, %1\n\t"
+            "movd	%%xmm3, %2\n"
+            "8:\n\t"
+        : /* %0 */ "=r" (buf),
+          /* %1 */ "=r" (s1),
+          /* %2 */ "=r" (s2),
+          /* %3 */ "=r" (k),
+          /* %4 */ "=r" (len)
+        : /* %5 */ "m" (vord),
+          /* %6 */ "i" (5*NMAX),
+          /*    */ "0" (buf),
+          /*    */ "1" (s1),
+          /*    */ "2" (s2),
+          /*    */ "3" (k),
+          /*    */ "4" (len)
+        : "cc", "memory"
+#  ifdef __SSE__
+          , "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7"
+#  endif
+    );
+
+    if (unlikely(k))
+        buf = adler32_jumped(buf, &s1, &s2, k);
+    MOD28(s1);
+    MOD28(s2);
+    return (s2 << 16) | s1;
+}
+
+#  if 0
+/* ========================================================================= */
+/*
+ * The SSE2 version above is faster on my CPUs (Athlon64, Core2,
+ * P4 Xeon, K10 Sempron), but has instruction stalls only a
+ * Out-Of-Order-Execution CPU can solve.
+ * So this Version _may_ be better for the new old thing, Atom.
+ */
+local noinline uLong adler32_SSE2_no_oooe(adler, buf, len)
+    uLong adler;
+    const Bytef *buf;
+    uInt len;
+{
+    unsigned int s1 = adler & 0xffff;
+    unsigned int s2 = (adler >> 16) & 0xffff;
+    unsigned int k;
+
+    k    = ALIGN_DIFF(buf, 16);
+    len -= k;
+    if (k)
+        buf = adler32_jumped(buf, &s1, &s2, k);
+
+    __asm__ __volatile__ (
+            "mov	%6, %3\n\t"
+            "cmp	%3, %4\n\t"
+            "cmovb	%4, %3\n\t"
+            "sub	%3, %4\n\t"
+            "cmp	$16, %3\n\t"
+            "jb	8f\n\t"
+            "movdqa	16+%5, %%xmm6\n\t"
+            "movdqa	32+%5, %%xmm5\n\t"
+            "prefetchnta	16(%0)\n\t"
+            "pxor	%%xmm7, %%xmm7\n\t"
+            "movd	%1, %%xmm4\n\t"
+            "movd	%2, %%xmm3\n\t"
+            ".p2align 3,,3\n\t"
+            ".p2align 2\n"
+            "1:\n\t"
+            "prefetchnta	32(%0)\n\t"
+            "movdqa	(%0), %%xmm1\n\t"
+            "sub	$16, %3\n\t"
+            "movdqa	%%xmm4, %%xmm2\n\t"
+            "add	$16, %0\n\t"
+            "movdqa	%%xmm1, %%xmm0\n\t"
+            "cmp	$15, %3\n\t"
+            "pslld	$4, %%xmm2\n\t"
+            "paddd	%%xmm3, %%xmm2\n\t"
+            "psadbw	%%xmm7, %%xmm0\n\t"
+            "paddd	%%xmm0, %%xmm4\n\t"
+            "movdqa	%%xmm1, %%xmm0\n\t"
+            "punpckhbw	%%xmm7, %%xmm1\n\t"
+            "punpcklbw	%%xmm7, %%xmm0\n\t"
+            "movdqa	%%xmm1, %%xmm3\n\t"
+            "pmaddwd	%%xmm6, %%xmm0\n\t"
+            "paddd	%%xmm2, %%xmm0\n\t"
+            "pmaddwd	%%xmm5, %%xmm3\n\t"
+            "paddd	%%xmm0, %%xmm3\n\t"
+            "jg	1b\n\t"
+            "movdqa	%%xmm3, %%xmm0\n\t"
+            "call	sse2_chop\n\t"
+            "call	sse2_chop\n\t"
+            "movdqa	%%xmm0, %%xmm3\n\t"
+            "movdqa	%%xmm4, %%xmm0\n\t"
+            "call	sse2_chop\n\t"
+            "movdqa	%%xmm0, %%xmm4\n\t"
+            "add	%3, %4\n\t"
+            "mov	%6, %3\n\t"
+            "cmp	%3, %4\n\t"
+            "cmovb	%4, %3\n\t"
+            "sub	%3, %4\n\t"
+            "cmp	$15, %3\n\t"
+            "jg	1b\n\t"
+            "pshufd	$0xEE, %%xmm3, %%xmm0\n\t"
+            "pshufd	$0xEE, %%xmm4, %%xmm1\n\t"
+            "paddd	%%xmm3, %%xmm0\n\t"
+            "pshufd	$0xE5, %%xmm0, %%xmm2\n\t"
+            "paddd	%%xmm4, %%xmm1\n\t"
+            "movd	%%xmm1, %1\n\t"
+            "paddd	%%xmm0, %%xmm2\n\t"
+            "movd	%%xmm2, %2\n"
+            "8:"
+        : /* %0 */ "=r" (buf),
+          /* %1 */ "=r" (s1),
+          /* %2 */ "=r" (s2),
+          /* %3 */ "=r" (k),
+          /* %4 */ "=r" (len)
+        : /* %5 */ "m" (vord),
+          /* %6 */ "i" (NMAX + NMAX/3),
+          /*    */ "0" (buf),
+          /*    */ "1" (s1),
+          /*    */ "2" (s2),
+          /*    */ "4" (len)
+        : "cc", "memory"
+#  ifdef __SSE__
+          , "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7"
+#  endif
+    );
+
+    if (unlikely(k))
+        buf = adler32_jumped(buf, &s1, &s2, k);
+    MOD28(s1);
+    MOD28(s2);
+    return (s2 << 16) | s1;
+}
+#  endif
+
+#  ifndef __x86_64__
+/* ========================================================================= */
+/*
+ * SSE version to help VIA-C3_2, P2 & P3
+ */
+local uLong adler32_SSE(adler, buf, len)
+    uLong adler;
+    const Bytef *buf;
+    uInt len;
+{
+    unsigned int s1 = adler & 0xffff;
+    unsigned int s2 = (adler >> 16) & 0xffff;
+    unsigned int k;
+
+    k    = ALIGN_DIFF(buf, 8);
+    len -= k;
+    if (k)
+        buf = adler32_jumped(buf, &s1, &s2, k);
+
+    __asm__ __volatile__ (
+            "mov	%6, %3\n\t"
+            "cmp	%3, %4\n\t"
+            "cmovb	%4, %3\n\t"
+            "sub	%3, %4\n\t"
+            "cmp	$8, %3\n\t"
+            "jb	8f\n\t"
+            "movd	%1, %%mm4\n\t"
+            "movd	%2, %%mm3\n\t"
+            "pxor	%%mm2, %%mm2\n\t"
+            "pxor	%%mm5, %%mm5\n\t"
+#    ifdef __ELF__
+            ".subsection 2\n\t"
+#    else
+            "jmp	7f\n\t"
+#    endif
+            ".p2align 2\n"
+            "mmx_chop:\n\t"
+            "movq	%%mm0, %%mm1\n\t"
+            "pslld	$16, %%mm1\n\t"
+            "psrld	$16, %%mm0\n\t"
+            "psrld	$16, %%mm1\n\t"
+            "psubd	%%mm0, %%mm1\n\t"
+            "pslld	$4, %%mm0\n\t"
+            "paddd	%%mm1, %%mm0\n\t"
+            "ret\n\t"
+#    ifdef __ELF__
+            ".previous\n\t"
+#    else
+            "7:\n\t"
+#    endif
+            ".p2align	2\n"
+            "3:\n\t"
+            "pxor	%%mm6, %%mm6\n\t"
+            "pxor	%%mm7, %%mm7\n\t"
+            "mov	$1024, %1\n\t"
+            "cmp	%1, %3\n\t"
+            "cmovb	%3, %1\n"
+            "and	$-8, %1\n\t"
+            "sub	%1, %3\n\t"
+            "shr	$3, %1\n\t"
+            ".p2align 4,,7\n"
+            ".p2align 3\n"
+            "1:\n\t"
+            "movq	(%0), %%mm0\n\t"
+            "paddd	%%mm4, %%mm5\n\t"
+            "add	$8, %0\n\t"
+            "dec	%1\n\t"
+            "movq	%%mm0, %%mm1\n\t"
+            "psadbw	%%mm2, %%mm0\n\t"
+            "paddd	%%mm0, %%mm4\n\t"
+            "movq	%%mm1, %%mm0\n\t"
+            "punpckhbw	%%mm2, %%mm1\n\t"
+            "punpcklbw	%%mm2, %%mm0\n\t"
+            "paddw	%%mm1, %%mm7\n\t"
+            "paddw	%%mm0, %%mm6\n\t"
+            "jnz	1b\n\t"
+            "cmp	$7, %3\n\t"
+            "pmaddwd	40+%5, %%mm7\n\t"
+            "pmaddwd	32+%5, %%mm6\n\t"
+            "paddd	%%mm7, %%mm3\n\t"
+            "paddd	%%mm6, %%mm3\n\t"
+            "jg	3b\n\t"
+            "movq	%%mm5, %%mm0\n\t"
+            "pxor	%%mm5, %%mm5\n\t"
+            "call	mmx_chop\n\t"
+            "pslld	$3, %%mm0\n\t"
+            "paddd	%%mm3, %%mm0\n\t"
+            "call	mmx_chop\n\t"
+            "movq	%%mm0, %%mm3\n\t"
+            "movq	%%mm4, %%mm0\n\t"
+            "call	mmx_chop\n\t"
+            "movq	%%mm0, %%mm4\n\t"
+            "add	%3, %4\n\t"
+            "mov	%6, %3\n\t"
+            "cmp	%3, %4\n\t"
+            "cmovb	%4, %3\n"
+            "sub	%3, %4\n\t"
+            "cmp	$7, %3\n\t"
+            "jg	3b\n\t"
+            "movd	%%mm4, %1\n\t"
+            "psrlq	$32, %%mm4\n\t"
+            "movd	%%mm3, %2\n\t"
+            "psrlq	$32, %%mm3\n\t"
+            "movd	%%mm4, %4\n\t"
+            "add	%4, %1\n\t"
+            "movd	%%mm3, %4\n\t"
+            "add	%4, %2\n\t"
+            "emms\n"
+            "8:\n\t"
+        : /* %0 */ "=r" (buf),
+          /* %1 */ "=r" (s1),
+          /* %2 */ "=r" (s2),
+          /* %3 */ "=r" (k),
+          /* %4 */ "=r" (len)
+        : /* %5 */ "m" (vord),
+          /* %6 */ "i" ((5*NMAX)/2),
+          /*    */ "0" (buf),
+          /*    */ "1" (s1),
+          /*    */ "2" (s2),
+          /*    */ "3" (k),
+          /*    */ "4" (len)
+        : "cc", "memory"
+#    ifdef __MMX__
+          , "mm0", "mm1", "mm2", "mm3", "mm4", "mm5", "mm6", "mm7"
+#    endif
+                               );
+
+    if (unlikely(k))
+        buf = adler32_jumped(buf, &s1, &s2, k);
+    MOD28(s1);
+    MOD28(s2);
+    return (s2 << 16) | s1;
+}
+
+/* ========================================================================= */
+/*
+ * Processors which only have MMX will prop. not like this
+ * code, they are so old, they are not Out-Of-Order
+ * (maybe except AMD K6, Cyrix, Winchip/VIA).
+ * I did my best to get at least 1 instruction between result -> use
+ */
+local uLong adler32_MMX(adler, buf, len)
+    uLong adler;
+    const Bytef *buf;
+    uInt len;
+{
+    unsigned int s1 = adler & 0xffff;
+    unsigned int s2 = (adler >> 16) & 0xffff;
+    unsigned int k;
+
+    k    = ALIGN_DIFF(buf, 8);
+    len -= k;
+    if (k)
+        buf = adler32_jumped(buf, &s1, &s2, k);
+
+    __asm__ __volatile__ (
+            "mov	%6, %3\n\t"
+            "cmp	%3, %4\n\t"
+            "jae	6f\n\t"
+            "mov	%4, %3\n"
+            "6:\n\t"
+            "sub	%3, %4\n\t"
+            "cmp	$8, %3\n\t"
+            "jb	8f\n\t"
+            "sub	$8, %%esp\n\t"
+            "movd	%1, %%mm4\n\t"
+            "movd	%2, %%mm2\n\t"
+            "movq	%5, %%mm3\n"
+            "5:\n\t"
+            "movq	%%mm2, %%mm0\n\t"
+            "pxor	%%mm2, %%mm2\n\t"
+            "pxor	%%mm5, %%mm5\n\t"
+            ".p2align 2\n"
+            "3:\n\t"
+            "movq	%%mm0, (%%esp)\n\t"
+            "pxor	%%mm6, %%mm6\n\t"
+            "pxor	%%mm7, %%mm7\n\t"
+            "mov	$1024, %1\n\t"
+            "cmp	%1, %3\n\t"
+            "jae	4f\n\t"
+            "mov	%3, %1\n"
+            "4:\n\t"
+            "and	$-8, %1\n\t"
+            "sub	%1, %3\n\t"
+            "shr	$3, %1\n\t"
+            ".p2align 4,,7\n\t"
+            ".p2align 3\n"
+            "1:\n\t"
+            "movq	(%0), %%mm0\n\t"
+            "paddd	%%mm4, %%mm5\n\t"
+            "add	$8, %0\n\t"
+            "dec	%1\n\t"
+            "movq	%%mm0, %%mm1\n\t"
+            "punpcklbw	%%mm2, %%mm0\n\t"
+            "punpckhbw	%%mm2, %%mm1\n\t"
+            "paddw	%%mm0, %%mm6\n\t"
+            "paddw	%%mm1, %%mm0\n\t"
+            "paddw	%%mm1, %%mm7\n\t"
+            "pmaddwd	%%mm3, %%mm0\n\t"
+            "paddd	%%mm0, %%mm4\n\t"
+            "jnz	1b\n\t"
+            "movq	(%%esp), %%mm0\n\t"
+            "cmp	$7, %3\n\t"
+            "pmaddwd	32+%5, %%mm6\n\t"
+            "pmaddwd	40+%5, %%mm7\n\t"
+            "paddd	%%mm6, %%mm0\n\t"
+            "paddd	%%mm7, %%mm0\n\t"
+            "jg	3b\n\t"
+            "movq	%%mm0, %%mm2\n\t"
+            "movq	%%mm5, %%mm0\n\t"
+            "call	mmx_chop\n\t"
+            "pslld	$3, %%mm0\n\t"
+            "paddd	%%mm2, %%mm0\n\t"
+            "call	mmx_chop\n\t"
+            "movq	%%mm0, %%mm2\n\t"
+            "movq	%%mm4, %%mm0\n\t"
+            "call	mmx_chop\n\t"
+            "movq	%%mm0, %%mm4\n\t"
+            "add	%3, %4\n\t"
+            "mov	%6, %3\n\t"
+            "cmp	%3, %4\n\t"
+            "jae	2f\n\t"
+            "mov	%4, %3\n"
+            "2:\n\t"
+            "sub	%3, %4\n\t"
+            "cmp	$7, %3\n\t"
+            "jg	5b\n\t"
+            "add	$8, %%esp\n\t"
+            "movd	%%mm4, %1\n\t"
+            "psrlq	$32, %%mm4\n\t"
+            "movd	%%mm2, %2\n\t"
+            "psrlq	$32, %%mm2\n\t"
+            "movd	%%mm4, %4\n\t"
+            "add	%4, %1\n\t"
+            "movd	%%mm2, %4\n\t"
+            "add	%4, %2\n\t"
+            "emms\n"
+            "8:\n\t"
+        : /* %0 */ "=r" (buf),
+          /* %1 */ "=r" (s1),
+          /* %2 */ "=r" (s2),
+          /* %3 */ "=r" (k),
+          /* %4 */ "=r" (len)
+        : /* %5 */ "m" (vord),
+          /* %6 */ "i" (4*NMAX),
+          /*    */ "0" (buf),
+          /*    */ "1" (s1),
+          /*    */ "2" (s2),
+          /*    */ "3" (k),
+          /*    */ "4" (len)
+        : "cc", "memory"
+#    ifdef __MMX__
+          , "mm0", "mm1", "mm2", "mm3", "mm4", "mm5", "mm6", "mm7"
+#    endif
+    );
+
+    if (unlikely(k))
+        buf = adler32_jumped(buf, &s1, &s2, k);
+    MOD28(s1);
+    MOD28(s2);
+    return (s2 << 16) | s1;
+}
+#  endif
+
+/* ========================================================================= */
+local uLong adler32_x86(adler, buf, len)
+    uLong adler;
+    const Bytef *buf;
+    uInt len;
+{
+    /* split Adler-32 into component sums */
+    unsigned int s1 = adler & 0xffff;
+    unsigned int s2 = (adler >> 16) & 0xffff;
+    unsigned int n;
+
+    do {
+        /* find maximum, len or NMAX */
+        n = len < NMAX ? len : NMAX;
+        len -= n;
+
+        /* do it */
+        buf = adler32_jumped(buf, &s1, &s2, n);
+        /* modulo */
+        MOD(s1);
+        MOD(s2);
+    } while (likely(len));
+
+    /* return recombined sums */
+    return (s2 << 16) | s1;
+}
+
+/* ========================================================================= */
+#define NO_ADLER32_GE16
+local noinline uLong adler32_ge16(adler, buf, len)
+    uLong adler;
+    const Bytef *buf;
+    uInt len;
+{
+    /* split Adler-32 into component sums */
+    unsigned int s1 = adler & 0xffff;
+    unsigned int s2 = (adler >> 16) & 0xffff;
+
+    /* simply do it, we do not expect more then NMAX as len */
+    adler32_jumped(buf, &s1, &s2, len);
+    /* actually we expect much less, MOD28 it */
+    MOD28(s1);
+    MOD28(s2);
+
+    /* return recombined sums */
+    return (s2 << 16) | s1;
+}
+
+/* ========================================================================= */
+/*
+ * Knot it all together with a runtime switch
+ */
+/* ========================================================================= */
+/* function enum */
+enum adler32_types
+{
+    T_ADLER32_RTSWITCH = 0,
+    T_ADLER32_X86,
+#  ifndef __x86_64__
+    T_ADLER32_MMX,
+    T_ADLER32_SSE,
+#  endif
+    T_ADLER32_SSE2,
+    T_ADLER32_SSSE3,
+    T_ADLER32_MAX
+};
+
+/* ========================================================================= */
+/* Decision table */
+local const struct test_cpu_feature tfeat_adler32_vec[] =
+{
+    /* func                             flags   features       */
+    {T_ADLER32_SSSE3,         0, {CFB(CFEATURE_CMOV), CFB(CFEATURE_SSSE3)}},
+    {T_ADLER32_SSE2,          0, {CFB(CFEATURE_SSE2)|CFB(CFEATURE_CMOV), 0}},
+#  ifndef __x86_64__
+    {T_ADLER32_SSE,           0, {CFB(CFEATURE_SSE)|CFB(CFEATURE_CMOV), 0}},
+    {T_ADLER32_MMX,           0, {CFB(CFEATURE_MMX), 0}},
+#  endif
+    {T_ADLER32_X86, CFF_DEFAULT, { 0, 0}},
+};
+
+/* ========================================================================= */
+/* Prototypes */
+local uLong adler32_vec_runtimesw(uLong adler, const Bytef *buf, uInt len);
+
+/* ========================================================================= */
+/* Function pointer table */
+local uLong (*const adler32_ptr_tab[])(uLong adler, const Bytef *buf, uInt len) =
+{
+    adler32_vec_runtimesw,
+    adler32_x86,
+#  ifndef __x86_64__
+    adler32_MMX,
+    adler32_SSE,
+#  endif
+    adler32_SSE2,
+    adler32_SSSE3,
+};
+
+/* ========================================================================= */
+#  if _FORTIFY_SOURCE-0 > 0
+/* Runtime decide var */
+local enum adler32_types adler32_f_type = T_ADLER32_RTSWITCH;
+#  else
+/* Runtime Function pointer */
+local uLong (*adler32_vec_ptr)(uLong adler, const Bytef *buf, uInt len) = adler32_vec_runtimesw;
+#  endif
+
+/* ========================================================================= */
+/* Constructor to init the decide var early */
+local GCC_ATTR_CONSTRUCTOR void adler32_vec_select(void)
+{
+    enum adler32_types lf_type =
+        _test_cpu_feature(tfeat_adler32_vec, sizeof (tfeat_adler32_vec)/sizeof (tfeat_adler32_vec[0]));
+#  if _FORTIFY_SOURCE-0 > 0
+    adler32_f_type = lf_type;
+#  else
+    adler32_vec_ptr = adler32_ptr_tab[lf_type];
+#  endif
+}
+
+/* ========================================================================= */
+/* Jump function */
+local noinline uLong adler32_vec(adler, buf, len)
+    uLong adler;
+    const Bytef *buf;
+    uInt len;
+{
+    /*
+     * Protect us from memory corruption. As long as the function pointer table
+     * resides in rodata, with a little bounding we can prevent arb. code
+     * execution (overwritten vtable pointer). We still may crash if the corruption
+     * is within bounds (or the cpudata gets corrupted too) and we jump into an
+     * function with unsupported instr., but this should mitigate the worst case
+     * scenario.
+     * But it's more expensive than a simple function pointer, so only when more
+     * security is wanted.
+     */
+#  if _FORTIFY_SOURCE-0 > 0
+    enum adler32_types lf_type = adler32_f_type;
+    /*
+     * If the compiler is smart he creates a cmp + sbb + and, cmov have a high
+     * latency and are not always avail.
+     * Otherwise compiler logic is advanced enough to see what's happening here,
+     * so there maybe is a reason why he changes this to a cmov...
+     * (or he simply does not see he can create a conditional -1/0 the cheap way)
+     *
+     * Maybe change it to an unlikely() cbranch? Which still leaves the question
+     * what's the mispredition propability, esp. with lots of different x86
+     * microarchs and not always perfect CFLAGS (-march/-mtune) to arrange the
+     * code to the processors liking.
+     */
+    lf_type &= likely((unsigned)lf_type < (unsigned)T_ADLER32_MAX) ? -1 : 0;
+    return adler32_ptr_tab[lf_type](adler, buf, len);
+#  else
+    return adler32_vec_ptr(adler, buf, len);
+#  endif
+}
+
+/* ========================================================================= */
+/*
+ * the runtime switcher is a little racy, but this is OK,
+ * it should normaly not run if the constructor works, and
+ * we are on x86, which isn't that picky about ordering
+ */
+local uLong adler32_vec_runtimesw(uLong adler, const Bytef *buf, uInt len)
+{
+    adler32_vec_select();
+    return adler32_vec(adler, buf, len);
+}
+#endif
diff -Naur a/x86/cpudet.c b/x86/cpudet.c
--- a/x86/cpudet.c	1970-01-01 01:00:00.000000000 +0100
+++ b/x86/cpudet.c	2015-08-16 05:57:02.000000000 +0200
@@ -0,0 +1,154 @@
+/* cpudet.c -- runtime cpu detection, x86 part
+ * Copyright (C) 2009-2011 Jan Seiffert
+ * For conditions of distribution and use, see copyright notice in zlib.h
+ */
+
+#include "x86.h"
+
+/* ========================================================================= */
+/* Internal data types */
+struct cpuid_regs
+{
+    unsigned long eax, ebx, ecx, edx;
+};
+
+local struct
+{
+    unsigned int max_basic;
+    unsigned int features[FEATURE_WORDS];
+    int init_done;
+} our_cpu;
+
+/* ========================================================================= */
+local inline unsigned long read_flags(void)
+{
+    unsigned long f;
+    __asm__ __volatile__ (
+            "pushf\n\t"
+            "pop %0\n\t"
+        : "=r" (f)
+    );
+    return f;
+}
+
+/* ========================================================================= */
+local inline void write_flags(unsigned long f)
+{
+    __asm__ __volatile__ (
+            "push %0\n\t"
+            "popf\n\t"
+        : : "ri" (f) : "cc"
+    );
+}
+
+/* ========================================================================= */
+local inline void cpuid(struct cpuid_regs *regs, unsigned long func)
+{
+    /* save ebx around cpuid call, PIC code needs it */
+    __asm__ __volatile__ (
+            "xchg	%1, " PICREG "\n\t"
+            "cpuid\n\t"
+            "xchg	%1, " PICREG "\n"
+        : /* %0 */ "=a" (regs->eax),
+          /* %1 */ "=r" (regs->ebx),
+          /* %2 */ "=c" (regs->ecx),
+          /* %4 */ "=d" (regs->edx)
+        : /* %5 */ "0" (func),
+          /* %6 */ "2" (regs->ecx)
+        : "cc"
+    );
+}
+
+/* ========================================================================= */
+local inline void cpuids(struct cpuid_regs *regs, unsigned long func)
+{
+    regs->ecx = 0;
+    cpuid(regs, func);
+}
+
+/* ========================================================================= */
+local inline int toggle_eflags_test(const unsigned long mask)
+{
+    unsigned long f;
+    int result;
+
+    f = read_flags();
+    write_flags(f ^ mask);
+    result = !!((f ^ read_flags()) & mask);
+    /*
+     * restore the old flags, the test for i486 tests the alignment
+     * check bit, and left set will confuse the x86 software world.
+     */
+    write_flags(f);
+    return result;
+}
+
+/* ========================================================================= */
+local inline int is_486(void)
+{
+    return toggle_eflags_test(1 << 18);
+}
+
+/* ========================================================================= */
+local inline int has_cpuid(void)
+{
+    return toggle_eflags_test(1 << 21);
+}
+
+/* ========================================================================= */
+local void identify_cpu(void)
+{
+    struct cpuid_regs a;
+
+    if (our_cpu.init_done)
+        return;
+
+    our_cpu.init_done = -1;
+    /* force a write out to memory */
+    __asm__ __volatile__ ("" : : "m" (our_cpu.init_done));
+
+    if (!is_486())
+        return;
+
+    if (!has_cpuid())
+        return;
+
+    /* get the maximum basic leaf number */
+    cpuids(&a, 0x00000000);
+    our_cpu.max_basic = (unsigned int)a.eax;
+    /* we could get the vendor string from ebx, edx, ecx */
+
+    /* get the first basic leaf, if it is avail. */
+    if (our_cpu.max_basic >= 0x00000001)
+        cpuids(&a, 0x00000001);
+    else
+        a.eax = a.ebx = a.ecx = a.edx = 0;
+
+    /* we could extract family, model, stepping from eax */
+
+    /* there is the first set of features */
+    our_cpu.features[0] = a.edx;
+    our_cpu.features[1] = a.ecx;
+
+    /* now we could test the extended features, but is not needed, for now */
+}
+
+/* ========================================================================= */
+int ZLIB_INTERNAL _test_cpu_feature (t, l)
+    const struct test_cpu_feature *t;
+    unsigned int l;
+{
+    unsigned int i, j, f;
+    identify_cpu();
+
+    for (i = 0; i < l; i++) {
+        if (t[i].flags & CFF_DEFAULT)
+            return t[i].f_type;
+        for (f = 0, j = 0; j < FEATURE_WORDS; j++)
+            f |= (our_cpu.features[j] & t[i].features[j]) ^ t[i].features[j];
+        if (f)
+            continue;
+        return t[i].f_type;
+    }
+    return 1; /* default */
+}
diff -Naur a/x86/slhash.c b/x86/slhash.c
--- a/x86/slhash.c	1970-01-01 01:00:00.000000000 +0100
+++ b/x86/slhash.c	2015-08-16 05:57:02.000000000 +0200
@@ -0,0 +1,406 @@
+/* slhash.c -- slide the hash table during fill_window()
+ * Copyright (C) 1995-2010 Jean-loup Gailly and Mark Adler
+ * Copyright (C) 2011 Jan Seiffert
+ * For conditions of distribution and use, see copyright notice in zlib.h
+ */
+
+#include "x86.h"
+
+/* inline asm, so only on GCC (or compatible) */
+#if defined(__GNUC__) && !defined(VEC_NO_GO)
+#  define HAVE_SLHASH_VEC
+#  define HAVE_SLHASH_COMPLETE
+
+local noinline void update_hoffset_x86(Posf *p, uInt wsize, unsigned n);
+local noinline void slhash_x86(Posf *p, Posf *q, uInt wsize, unsigned n);
+local noinline void slhash_SSE2(Posf *p, Posf *q, uInt wsize, unsigned n);
+
+/* NOTE:
+ * We do not precheck the length or wsize for small values because
+ * we assume a minimum len of 256 (for MEM_LEVEL 1) and a minimum wsize
+ * of 256 for windowBits 8
+ */
+
+/* ========================================================================= */
+/* This is totally bogus, because the Pos type is only 16 bit, and as soon as
+ * wsize > 65534, we can not hold the distances in a Pos. All this is a
+ * kind of complicated memset 0.
+ */
+local void update_hoffset_SSE4_1(Posf *p, uInt wsize, unsigned n)
+{
+    register unsigned m;
+    unsigned int i, j;
+
+    i  = ALIGN_DIFF(p, 8)/sizeof(Pos);
+    n -= i;
+    if (unlikely(i)) do {
+        m = *p;
+        *p++ = (Pos)(m >= wsize ? m-wsize : NIL);
+    } while (--i);
+    i = n / 4;
+    n %= 4;
+    asm (
+        "pxor %%xmm6, %%xmm6\n\t"
+        "movd %k3, %%xmm7\n\t"
+        "pshufd $0, %%xmm7, %%xmm7\n\t"
+        "test $8, %0\n\t"
+        "jz 2f\n\t"
+        "movq (%0), %%xmm0\n\t"
+        "add $8, %0\n\t"
+        "dec %1\n\t"
+        "punpcklwd %%xmm6, %%xmm0\n\t"
+        "psubd %%xmm7, %%xmm0\n\t"
+        "packusdw %%xmm6, %%xmm0\n\t"
+        "movq %%xmm0, -8(%0)\n"
+        "2:\n\t"
+        "mov %1, %2\n\t"
+        "shr $1, %1\n\t"
+        "and $1, %2\n\t"
+        ".p2align 3\n"
+        "1:\n\t"
+        "movdqa (%0), %%xmm0\n\t"
+        "add $16, %0\n\t"
+        "movdqa %%xmm0, %%xmm1\n\t"
+        "punpcklwd %%xmm6, %%xmm0\n\t"
+        "punpckhwd %%xmm6, %%xmm1\n\t"
+        "psubd %%xmm7, %%xmm0\n\t"
+        "psubd %%xmm7, %%xmm1\n\t"
+        "packusdw %%xmm1, %%xmm0\n\t"
+        "movdqa %%xmm0, -16(%0)\n\t"
+        "dec %1\n\t"
+        "jnz 1b\n\t"
+        "test %2, %2\n\t"
+        "jz 3f\n\t"
+        "movq (%0), %%xmm0\n\t"
+        "add $8, %0\n\t"
+        "punpcklwd %%xmm6, %%xmm0\n\t"
+        "psubd %%xmm7, %%xmm0\n\t"
+        "packusdw %%xmm6, %%xmm0\n\t"
+        "movq %%xmm0, -8(%0)\n"
+        "3:"
+        : /* %0 */ "=r" (p),
+          /* %1 */ "=r" (i),
+          /* %2 */ "=r" (j)
+        : /* %3 */ "r" (wsize),
+          /*  */ "0" (p),
+          /*  */ "1" (i)
+#  ifdef __SSE2__
+        : "xmm0", "xmm7"
+#  endif
+    );
+    if (unlikely(n))
+        update_hoffset_x86(p, wsize, n);
+}
+
+/* ========================================================================= */
+local void slhash_SSE4_1(Posf *p, Posf *q, uInt wsize, unsigned n)
+{
+    if (likely(wsize <= (1<<16)-1)) {
+        slhash_SSE2(p, q, wsize, n);
+        return;
+    }
+
+    update_hoffset_SSE4_1(p, wsize, n);
+#  ifndef FASTEST
+    /* If n is not on any hash chain, prev[n] is garbage but
+     * its value will never be used.
+     */
+    update_hoffset_SSE4_1(q, wsize, wsize);
+#  endif
+}
+
+/* ========================================================================= */
+local void update_hoffset_SSE2(Posf *p, uInt wsize, unsigned n)
+{
+    register unsigned m;
+    unsigned int i, j;
+
+    i  = ALIGN_DIFF(p, 8)/sizeof(Pos);
+    n -= i;
+    if (unlikely(i)) do {
+        m = *p;
+        *p++ = (Pos)(m >= wsize ? m-wsize : NIL);
+    } while (--i);
+    i = n / 4;
+    n %= 4;
+    asm (
+        "movd %k3, %%xmm7\n\t"
+        "pshuflw $0, %%xmm7, %%xmm7\n\t"
+        "pshufd $0, %%xmm7, %%xmm7\n\t"
+        "test $8, %0\n\t"
+        "jz 2f\n\t"
+        "movq (%0), %%xmm0\n\t"
+        "add $8, %0\n\t"
+        "dec %1\n\t"
+        "psubusw %%xmm7, %%xmm0\n\t"
+        "movq %%xmm0, -8(%0)\n\t"
+        "2:\n\t"
+        "mov %1, %2\n\t"
+        "shr $1, %1\n\t"
+        "and $1, %2\n\t"
+        ".p2align 3\n"
+        "1:\n\t"
+        "movdqa (%0), %%xmm0\n\t"
+        "add $16, %0\n\t"
+        "psubusw %%xmm7, %%xmm0\n\t"
+        "movdqa %%xmm0, -16(%0)\n\t"
+        "dec %1\n\t"
+        "jnz 1b\n\t"
+        "test %2, %2\n\t"
+        "jz 3f\n\t"
+        "movq (%0), %%xmm0\n\t"
+        "add $8, %0\n\t"
+        "psubusw %%xmm7, %%xmm0\n\t"
+        "movq %%xmm0, -8(%0)\n\t"
+        "3:"
+        : /* %0 */ "=r" (p),
+          /* %1 */ "=r" (i),
+          /* %2 */ "=r" (j)
+        : /* %3 */ "r" (wsize),
+          /*  */ "0" (p),
+          /*  */ "1" (i)
+#  ifdef __SSE2__
+        : "xmm0", "xmm7"
+#  endif
+    );
+    if (unlikely(n))
+        update_hoffset_x86(p, wsize, n);
+}
+
+/* ========================================================================= */
+local noinline void slhash_SSE2(Posf *p, Posf *q, uInt wsize, unsigned n)
+{
+    if (unlikely(wsize > (1 << 16)-1)) {
+        slhash_x86(p, q, wsize, n);
+        return;
+    }
+
+    update_hoffset_SSE2(p, wsize, n);
+#  ifndef FASTEST
+    /* If n is not on any hash chain, prev[n] is garbage but
+     * its value will never be used.
+     */
+    update_hoffset_SSE2(q, wsize, wsize);
+#  endif
+}
+
+#  ifndef __x86_64__
+/* ========================================================================= */
+local void update_hoffset_MMX(Posf *p, uInt wsize, unsigned n)
+{
+    register unsigned m;
+    unsigned int i;
+
+    i  = ALIGN_DIFF(p, 8)/sizeof(Pos);
+    n -= i;
+    if (unlikely(i)) do {
+        m = *p;
+        *p++ = (Pos)(m >= wsize ? m-wsize : NIL);
+    } while (--i);
+    i = n / 4;
+    n %= 4;
+    asm (
+        "movd %k2, %%mm7\n\t"
+        "pshufw $0, %%mm7, %%mm7\n\t"
+        ".p2align 2\n"
+        "1:\n\t"
+        "movq (%0), %%mm0\n\t"
+        "add $8, %0\n\t"
+        "psubusw %%mm7, %%mm0\n\t"
+        "movq %%xmm0, -8(%0)\n\t"
+        "dec %1\n\t"
+        "jnz 1b"
+        : /* %0 */ "=r" (p),
+          /* %1 */ "=r" (i)
+        : /* %2 */ "r" (wsize),
+          /*  */ "0" (p),
+          /*  */ "1" (i)
+#    ifdef __MMX__
+        : "mm0", "mm7"
+#    endif
+    );
+    if (unlikely(n))
+        update_hoffset_x86(p, wsize, n);
+}
+
+/* ========================================================================= */
+local noinline void slhash_MMX(Posf *p, Posf *q, uInt wsize, unsigned n)
+{
+    if (unlikely(wsize > (1 << 16)-1)) {
+        slhash_x86(p, q, wsize, n);
+        return;
+    }
+
+    update_hoffset_MMX(p, wsize, n);
+#    ifndef FASTEST
+    /* If n is not on any hash chain, prev[n] is garbage but
+     * its value will never be used.
+     */
+    update_hoffset_MMX(q, wsize, wsize);
+#    endif
+    asm volatile ("emms");
+}
+#  endif
+
+/* ========================================================================= */
+local noinline void update_hoffset_x86(Posf *p, uInt wsize, unsigned n)
+{
+    /*
+     * This code is cheaper then a cmov, measuring whole loops with
+     * rdtsc:
+     * This code:  593216
+     * compiler:  1019864
+     * (and 1000 runs show the same trend)
+     * Old CPUs without cmov will also love it, better then jumps.
+     *
+     * GCC does not manage to create it, x86 is a cc_mode target,
+     * and prop. will stay forever.
+     */
+    do {
+        register unsigned m = *p;
+        unsigned t;
+        asm (
+            "sub	%2, %0\n\t"
+            "sbb	$0, %1\n\t"
+            : "=r" (m),
+              "=r" (t)
+            : "r" (wsize),
+              "0" (m),
+              "1" (0)
+        );
+        *p++ = (Pos)(m & ~t);
+    } while (--n);
+}
+
+/* ========================================================================= */
+local noinline void slhash_x86(Posf *p, Posf *q, uInt wsize, unsigned n)
+{
+    update_hoffset_x86(p, wsize, n);
+#  ifndef FASTEST
+    /* If n is not on any hash chain, prev[n] is garbage but
+     * its value will never be used.
+     */
+    update_hoffset_x86(q, wsize, wsize);
+#  endif
+}
+
+/*
+ * Knot it all together with a runtime switch
+ */
+/* ========================================================================= */
+/* function enum */
+enum slhash_types
+{
+    T_SLHASH_RTSWITCH = 0,
+    T_SLHASH_X86,
+#  ifndef __x86_64__
+    T_SLHASH_MMX,
+#  endif
+    T_SLHASH_SSE2,
+    T_SLHASH_SSE4_1,
+    T_SLHASH_MAX
+};
+
+/* ========================================================================= */
+/* Decision table */
+local const struct test_cpu_feature tfeat_slhash_vec[] =
+{
+    /* func               flags  features       */
+    {T_SLHASH_SSE4_1,        0, {0,                  CFB(CFEATURE_SSE4_1)}},
+    {T_SLHASH_SSE2,          0, {CFB(CFEATURE_SSE2),                    0}},
+#  ifndef __x86_64__
+    {T_SLHASH_MMX,           0, {CFB(CFEATURE_MMX),                     0}},
+#  endif
+    {T_SLHASH_X86, CFF_DEFAULT, { 0, 0}},
+};
+
+/* ========================================================================= */
+/* Prototypes */
+local void slhash_vec_runtimesw(Posf *p, Posf *q, uInt wsize, unsigned n);
+
+/* ========================================================================= */
+/* Function pointer table */
+local void (*const slhash_ptr_tab[])(Posf *p, Posf *q, uInt wsize, unsigned n) =
+{
+    slhash_vec_runtimesw,
+    slhash_x86,
+#  ifndef __x86_64__
+    slhash_MMX,
+#  endif
+    slhash_SSE2,
+    slhash_SSE4_1,
+};
+
+/* ========================================================================= */
+#  if _FORTIFY_SOURCE-0 > 0
+/* Runtime decide var */
+local enum slhash_types slhash_f_type = T_SLHASH_RTSWITCH;
+#  else
+/* Runtime Function pointer */
+local void (*slhash_vec_ptr)(Posf *p, Posf *q, uInt wsize, unsigned n) = slhash_vec_runtimesw;
+#  endif
+
+/* ========================================================================= */
+/* Constructor to init the decide var early */
+local GCC_ATTR_CONSTRUCTOR void slhash_vec_select(void)
+{
+    enum slhash_types lf_type =
+        _test_cpu_feature(tfeat_slhash_vec, sizeof (tfeat_slhash_vec)/sizeof (tfeat_slhash_vec[0]));
+#  if _FORTIFY_SOURCE-0 > 0
+    slhash_f_type = lf_type;
+#  else
+    slhash_vec_ptr = slhash_ptr_tab[lf_type];
+#  endif
+}
+
+/* ========================================================================= */
+/* Jump function */
+void ZLIB_INTERNAL _sh_slide (p, q, wsize, n)
+    Posf *p;
+    Posf *q;
+    uInt wsize;
+    unsigned n;
+{
+    /*
+     * Protect us from memory corruption. As long as the function pointer table
+     * resides in rodata, with a little bounding we can prevent arb. code
+     * execution (overwriten vtable pointer). We still may crash if the corruption
+     * is within bounds (or the cpudata gets corrupted too) and we jump into an
+     * function with unsupported instr., but this should mitigate the worst case
+     * scenario.
+     * But it's more expensive than a simple function pointer, so only when more
+     * security is wanted.
+     */
+#  if _FORTIFY_SOURCE-0 > 0
+    enum slhash_types lf_type = slhash_f_type;
+    /*
+     * If the compiler is smart he creates a cmp + sbb + and, cmov have a high
+     * latency and are not always avail.
+     * Otherwise compiler logic is advanced enough to see what's happening here,
+     * so there maybe is a reason why he changes this to a cmov...
+     * (or he simply does not see he can create a conditional -1/0 the cheap way)
+     *
+     * Maybe change it to an unlikely() cbranch? Which still leaves the question
+     * what's the mispredition propability, esp. with lots of different x86
+     * microarchs and not always perfect CFLAGS (-march/-mtune) to arrange the
+     * code to the processors liking.
+     */
+    lf_type &= likely((unsigned)lf_type < (unsigned)T_SLHASH_MAX) ? -1 : 0;
+    return slhash_ptr_tab[lf_type](p, q, wsize, n);
+#  else
+    return slhash_vec_ptr(p, q, wsize, n);
+#  endif
+}
+
+/* ========================================================================= */
+/*
+ * the runtime switcher is a little racy, but this is OK,
+ * it should normaly not run if the constructor works, and
+ * we are on x86, which isn't that picky about ordering
+ */
+local void slhash_vec_runtimesw(Posf *p, Posf *q, uInt wsize, unsigned n)
+{
+    slhash_vec_select();
+    return _sh_slide(p, q, wsize, n);
+}
+#endif
diff -Naur a/x86/x86.h b/x86/x86.h
--- a/x86/x86.h	1970-01-01 01:00:00.000000000 +0100
+++ b/x86/x86.h	2015-08-16 05:57:02.000000000 +0200
@@ -0,0 +1,45 @@
+/* x86.h -- x86 cpu magic
+ * Copyright (C) 2009-2011 Jan Seiffert
+ * For conditions of distribution and use, see copyright notice in zlib.h
+ */
+
+#ifndef X86_H
+#define X86_H
+
+#if GCC_VERSION_GE(207)
+#  define GCC_ATTR_CONSTRUCTOR __attribute__((__constructor__))
+#else
+#  define VEC_NO_GO
+#endif
+
+#ifdef __x86_64__
+#  define PICREG "%%rbx"
+#else
+#  define PICREG "%%ebx"
+#endif
+
+/* Flags */
+#define CFF_DEFAULT (1 << 0)
+/* Processor features */
+#define CFEATURE_CMOV   (15 +   0)
+#define CFEATURE_MMX    (23 +   0)
+#define CFEATURE_SSE    (25 +   0)
+#define CFEATURE_SSE2   (26 +   0)
+#define CFEATURE_SSSE3  ( 9 +  32)
+#define CFEATURE_SSE4_1 (19 +  32)
+
+#define CFB(x) (1 << ((x)%32))
+
+#define FEATURE_WORDS 2
+
+/* ========================================================================= */
+/* data structure */
+struct test_cpu_feature
+{
+    int f_type;
+    int flags;
+    unsigned int features[FEATURE_WORDS];
+};
+
+int ZLIB_INTERNAL _test_cpu_feature OF((const struct test_cpu_feature *t, unsigned int l));
+#endif
diff -Naur a/zutil.c b/zutil.c
--- a/zutil.c	2012-08-13 09:02:40.000000000 +0200
+++ b/zutil.c	2015-08-16 05:57:02.000000000 +0200
@@ -322,3 +322,7 @@
 #endif /* MY_ZCALLOC */
 
 #endif /* !Z_SOLO */
+
+#if defined(__i386__) || defined(__x86_64__)
+#  include "x86/cpudet.c"
+#endif
diff -Naur a/zutil.h b/zutil.h
--- a/zutil.h	2013-03-25 06:47:59.000000000 +0100
+++ b/zutil.h	2015-08-16 05:57:59.000000000 +0200
@@ -13,6 +13,8 @@
 #ifndef ZUTIL_H
 #define ZUTIL_H
 
+#define GCC_VERSION_GE(x) ((__GNUC__-0) * 100 + __GNUC_MINOR__-0 >= x)
+
 #ifdef HAVE_HIDDEN
 #  define ZLIB_INTERNAL __attribute__((visibility ("hidden")))
 #else
@@ -21,6 +23,36 @@
 
 #include "zlib.h"
 
+#if GCC_VERSION_GE(301)
+/* sometimes leaks out of old kernel header */
+#  undef noinline
+#  define noinline __attribute__((__noinline__))
+#else
+#  ifndef noinline
+#    define noinline
+#  endif
+#endif
+
+#if GCC_VERSION_GE(301)
+#  define GCC_ATTR_UNUSED_PARAM __attribute__((__unused__))
+#else
+#  define GCC_ATTR_UNUSED_PARAM
+#endif
+
+#if GCC_VERSION_GE(296)
+#  undef likely
+#  undef unlikely
+#  define likely(x)   __builtin_expect(!!(x), 1)
+#  define unlikely(x) __builtin_expect(!!(x), 0)
+#else
+#  ifndef likely
+#    define likely(x)   (x)
+#  endif
+#  ifndef unlikely
+#    define unlikely(x) (x)
+#  endif
+#endif
+
 #if defined(STDC) && !defined(Z_SOLO)
 #  if !(defined(_WIN32_WCE) && defined(_MSC_VER))
 #    include <stddef.h>
@@ -33,6 +65,12 @@
    typedef long ptrdiff_t;  /* guess -- will be caught if guess is wrong */
 #endif
 
+#define ROUND_TO(x , n) ((x) & ~((n) - 1L))
+#define DIV_ROUNDUP(a, b) (((a) + (b) - 1) / (b))
+#define ALIGN_DIFF(x, n) ((((intptr_t)((x)+(n) - 1L) & ~((intptr_t)(n) - 1L))) - (intptr_t)(x))
+#define ALIGN_DOWN(x, n) (((intptr_t)(x)) & ~((intptr_t)(n) - 1L))
+#define ALIGN_DOWN_DIFF(x, n) (((intptr_t)(x)) & ((intptr_t)(n) - 1L))
+
 #ifndef local
 #  define local static
 #endif
@@ -161,6 +199,14 @@
 #  endif
 #endif
 
+#ifndef UINT64_C
+#  if defined(_MSC_VER) || defined(__BORLANDC__)
+#    define UINT64_C(c)    (c ## ui64)
+#  else
+#    define UINT64_C(c)    (c ## ULL)
+#  endif
+#endif
+
 #if defined(__BORLANDC__) && !defined(MSDOS)
   #pragma warn -8004
   #pragma warn -8008
